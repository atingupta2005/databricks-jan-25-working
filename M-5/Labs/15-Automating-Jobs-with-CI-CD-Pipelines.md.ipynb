{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Automating Jobs with CI/CD Pipelines - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This hands-on lab document provides **detailed, step-by-step exercises**\n",
    "for implementing **CI/CD pipelines** in **data engineering and machine\n",
    "learning workflows** using **Databricks, Terraform, GitHub Actions, and\n",
    "Azure DevOps**.\n",
    "\n",
    "These labs will cover: - **Setting up version control for Databricks\n",
    "notebooks and data workflows**. - **Automating testing and validation of\n",
    "Spark jobs, Delta Lake pipelines, and ML models**. - **Deploying\n",
    "Databricks jobs using GitHub Actions and Azure DevOps**. -\n",
    "**Infrastructure as Code (IaC) for managing Databricks clusters and\n",
    "resources**. - **Monitoring, error handling, and security best practices\n",
    "in CI/CD pipelines**. - **Using existing datasets (Banks Data, Loan\n",
    "Foreclosure Data, Flights Data) from previous notebooks to ensure\n",
    "real-world relevance.**\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step\n",
    "instructions**, and **sample dataset usage** to ensure **scalability and\n",
    "reliability**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Setting Up Version Control for Databricks Notebooks**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Learn how to integrate **Databricks Workflows** with **GitHub/Azure\n",
    "    DevOps**.\n",
    "\n",
    "### **Step 1: Enable Git Integration in Databricks**\n",
    "\n",
    "1.  Open **Databricks Workspace** → Navigate to **Repos**.\n",
    "2.  Click **Add Repo** → Select **GitHub/Azure DevOps**.\n",
    "3.  Connect to a GitHub repository with your **notebooks and scripts**.\n",
    "\n",
    "### **Step 2: Clone a Repository into Databricks**\n",
    "\n",
    "``` bash\n",
    "databricks repos create --url https://github.com/my-org/databricks-repo --path /Repos/my-repo\n",
    "```\n",
    "\n",
    "### **Step 3: Commit and Push Changes from Databricks to Git**\n",
    "\n",
    "``` bash\n",
    "git add .\n",
    "git commit -m \"Updated ETL notebooks\"\n",
    "git push origin main\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Databricks notebooks are **version-controlled in\n",
    "GitHub/Azure DevOps**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Automating Testing and Validation in CI/CD Pipelines**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Implement **automated testing** for **Spark jobs and Delta Lake\n",
    "    ingestion pipelines**.\n",
    "\n",
    "### **Step 1: Install Great Expectations for Data Validation**\n",
    "\n",
    "``` python\n",
    "pip install great_expectations\n",
    "```\n",
    "\n",
    "### **Step 2: Validate a Delta Lake Table Schema (Using Flights Data)**\n",
    "\n",
    "``` python\n",
    "import great_expectations as ge\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataValidation\").getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")\n",
    "\n",
    "df_expectations = ge.dataset.SparkDFDataset(df)\n",
    "assert df_expectations.expect_column_values_to_not_be_null(\"flight_id\").success\n",
    "assert df_expectations.expect_column_to_exist(\"departure_time\").success\n",
    "```\n",
    "\n",
    "### **Step 3: Automate Testing in CI/CD Pipeline**\n",
    "\n",
    "Modify **GitHub Actions YAML** to run tests on every push:\n",
    "\n",
    "``` yaml\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v2\n",
    "      - name: Run Data Validation\n",
    "        run: python validate_data.py\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - The test will **fail if missing values** exist\n",
    "in `flight_id` or `departure_time`. - CI/CD pipeline ensures **data\n",
    "integrity before processing**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Deploying Databricks Jobs Using GitHub Actions**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Automate **Databricks job deployments** using **GitHub Actions**.\n",
    "\n",
    "### **Step 1: Create a Job JSON Config**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"name\": \"Loan Foreclosure ETL\",\n",
    "  \"new_cluster\": {\n",
    "    \"spark_version\": \"11.3.x-scala2.12\",\n",
    "    \"num_workers\": 3\n",
    "  },\n",
    "  \"notebook_task\": {\n",
    "    \"notebook_path\": \"/Repos/databricks-repo/loan_foreclosure_etl\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Deploy the Job Using GitHub Actions**\n",
    "\n",
    "Modify the CI/CD pipeline to deploy the job:\n",
    "\n",
    "``` yaml\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout repository\n",
    "        uses: actions/checkout@v2\n",
    "      - name: Deploy Databricks Job\n",
    "        run: databricks jobs create --json-file job_config.json\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Databricks **automatically executes the ETL\n",
    "pipeline** when new data is available.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Infrastructure as Code (IaC) for Databricks Using Terraform**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Provision Databricks **clusters and job workflows** using Terraform.\n",
    "\n",
    "### **Step 1: Create a Terraform Configuration for a Cluster (Banks Data Processing)**\n",
    "\n",
    "``` hcl\n",
    "resource \"databricks_cluster\" \"banks_cluster\" {\n",
    "  cluster_name = \"Banks Data Processing\"\n",
    "  spark_version = \"11.3.x-scala2.12\"\n",
    "  num_workers = 4\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Terraform Configuration**\n",
    "\n",
    "``` bash\n",
    "terraform init\n",
    "terraform apply -auto-approve\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - A **Databricks cluster** is automatically\n",
    "provisioned to handle **Banks Data Processing**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Implementing Monitoring and Error Handling in CI/CD**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Enable **logging, monitoring, and failure handling** in CI/CD\n",
    "    pipelines.\n",
    "\n",
    "### **Step 1: Configure Logging in Databricks Jobs**\n",
    "\n",
    "``` python\n",
    "import logging\n",
    "logger = logging.getLogger(\"ci-cd-pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Processing started for Loan Foreclosure Data\")\n",
    "```\n",
    "\n",
    "### **Step 2: Implement Retry Logic in CI/CD Workflows**\n",
    "\n",
    "Modify **GitHub Actions YAML** to include **automatic retries**:\n",
    "\n",
    "``` yaml\n",
    "jobs:\n",
    "  deploy:\n",
    "    steps:\n",
    "      - name: Retry Databricks Job Deployment\n",
    "        run: |\n",
    "          for i in {1..3}; do\n",
    "            databricks jobs run-now --job-id 7890 && break || sleep 60;\n",
    "          done\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Job failures **trigger alerts**, and **retry\n",
    "logic** ensures automatic reattempts.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have learned how to: -\n",
    "**Automate Databricks workflows using GitHub Actions and Azure\n",
    "DevOps**. - **Implement automated testing for data pipelines using\n",
    "real-world datasets**. - **Provision infrastructure using Terraform**. -\n",
    "**Enable monitoring, alerts, and retry logic in CI/CD workflows**.\n",
    "\n",
    "These labs provide **real-world experience** in **deploying\n",
    "production-ready data engineering pipelines** using **CI/CD best\n",
    "practices**."
   ],
   "id": "376cfed5-2f87-4c1c-876f-6a3bffce29b6"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
