{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e08f00ad",
   "metadata": {},
   "source": [
    "# **Integration with Airflow and Step Functions - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** for building, managing, and optimizing **workflow orchestration** using **Apache Airflow and AWS Step Functions**. These labs cover:\n",
    "- **Creating Apache Airflow DAGs for Databricks job orchestration**\n",
    "- **Triggering workflows dynamically using event-based execution**\n",
    "- **Building AWS Step Functions state machines to automate workflows**\n",
    "- **Error handling, retries, and monitoring execution logs**\n",
    "- **Real-world integration with Databricks, Azure, and AWS services**\n",
    "- **Using existing datasets such as Banks Data, Loan Foreclosure Data, and Flights Data from previous notebooks**\n",
    "\n",
    "Each lab includes **real-world examples, step-by-step instructions, sample dataset usage, and advanced workflow orchestration techniques** to build **scalable and production-ready workflows**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Orchestrating a Databricks Job using Apache Airflow**\n",
    "### **Objective:**\n",
    "- Define an **Airflow DAG** to run a **Databricks job** in **Azure Databricks**.\n",
    "\n",
    "### **Step 1: Install Required Packages in Airflow**\n",
    "```bash\n",
    "pip install apache-airflow-providers-databricks\n",
    "```\n",
    "\n",
    "### **Step 2: Create an Airflow DAG to Trigger a Databricks Job Using Bank Transactions Data**\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    \"owner\": \"airflow\",\n",
    "    \"start_date\": datetime(2024, 1, 1),\n",
    "    \"retries\": 3,\n",
    "    \"retry_delay\": timedelta(minutes=5)\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    \"bank_transactions_databricks_orchestration\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"@daily\",\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "run_databricks_job = DatabricksRunNowOperator(\n",
    "    task_id=\"trigger_bank_transactions_processing\",\n",
    "    job_id=\"5678\",\n",
    "    databricks_conn_id=\"databricks_default\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "run_databricks_job\n",
    "```\n",
    "\n",
    "### **Step 3: Deploy and Monitor the DAG in Airflow**\n",
    "1. Save the DAG as `bank_transactions_airflow_dag.py` in the Airflow DAGs folder.\n",
    "2. Restart the Airflow scheduler to detect the new DAG.\n",
    "3. Open the **Airflow UI**, enable the DAG, and trigger a manual run.\n",
    "4. Check the **Logs & Execution Status**.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Airflow DAG successfully triggers a **Databricks job processing bank transactions data** and completes execution.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Automating a Loan Foreclosure Data Pipeline with Apache Airflow**\n",
    "### **Objective:**\n",
    "- Automate ETL workflows for **Loan Foreclosure Data** with **Databricks** and **Airflow**.\n",
    "\n",
    "### **Step 1: Define a DAG for Loan Data Processing**\n",
    "```python\n",
    "from airflow.operators.dummy import DummyOperator\n",
    "from airflow.providers.databricks.operators.databricks import DatabricksRunNowOperator\n",
    "\n",
    "dag = DAG(\n",
    "    \"loan_foreclosure_processing\",\n",
    "    default_args=default_args,\n",
    "    schedule_interval=\"0 2 * * *\",  # Runs daily at 2 AM UTC\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "start = DummyOperator(task_id=\"start_task\", dag=dag)\n",
    "process_loan_data = DatabricksRunNowOperator(\n",
    "    task_id=\"run_loan_data_etl\",\n",
    "    job_id=\"9102\",\n",
    "    databricks_conn_id=\"databricks_default\",\n",
    "    dag=dag,\n",
    ")\n",
    "start >> process_loan_data\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The workflow **extracts, transforms, and loads loan foreclosure data into Databricks Delta Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Automating Workflows using AWS Step Functions**\n",
    "### **Objective:**\n",
    "- Create an **AWS Step Functions** state machine that processes **Flights Data** and sends alerts for delayed flights.\n",
    "\n",
    "### **Step 1: Define the AWS Step Functions State Machine**\n",
    "```json\n",
    "{\n",
    "  \"Comment\": \"A Step Function to process flight delay data\",\n",
    "  \"StartAt\": \"TriggerFlightProcessing\",\n",
    "  \"States\": {\n",
    "    \"TriggerFlightProcessing\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:lambda:us-east-1:123456789012:function:triggerFlightsProcessing\",\n",
    "      \"Next\": \"CheckForDelays\"\n",
    "    },\n",
    "    \"CheckForDelays\": {\n",
    "      \"Type\": \"Choice\",\n",
    "      \"Choices\": [\n",
    "        {\n",
    "          \"Variable\": \"$.delay_status\",\n",
    "          \"StringEquals\": \"Delayed\",\n",
    "          \"Next\": \"SendAlert\"\n",
    "        }\n",
    "      ],\n",
    "      \"Default\": \"EndState\"\n",
    "    },\n",
    "    \"SendAlert\": {\n",
    "      \"Type\": \"Task\",\n",
    "      \"Resource\": \"arn:aws:sns:us-east-1:123456789012:SendDelayAlert\",\n",
    "      \"End\": true\n",
    "    },\n",
    "    \"EndState\": {\n",
    "      \"Type\": \"Pass\",\n",
    "      \"End\": true\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The **Step Function processes flight delays** and **triggers an alert if a flight is delayed**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Implementing Advanced Error Handling and Logging in Workflows**\n",
    "### **Objective:**\n",
    "- Implement **robust retry logic and logging mechanisms** in **Airflow DAGs and AWS Step Functions**.\n",
    "\n",
    "### **Step 1: Enable Logging and Retry Logic in Airflow DAG**\n",
    "```python\n",
    "from airflow.exceptions import AirflowFailException\n",
    "from airflow.decorators import task\n",
    "\n",
    "@task(retries=3, retry_delay=timedelta(minutes=10))\n",
    "def unreliable_task():\n",
    "    import random\n",
    "    if random.choice([True, False]):\n",
    "        raise AirflowFailException(\"Simulated failure\")\n",
    "    return \"Task Succeeded\"\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Logging in AWS Step Functions**\n",
    "1. Open **AWS Step Functions Console** â†’ Select a State Machine Execution.\n",
    "2. Click on **Execution History** to track each step.\n",
    "3. Enable **CloudWatch Logs** for real-time monitoring.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Workflows retry failed tasks up to 3 times before marking them as failed**.\n",
    "- **CloudWatch and Airflow logs provide detailed insights for debugging**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Orchestrating Databricks workflows with Apache Airflow.**\n",
    "- **Automating Loan Foreclosure and Flight Data Pipelines.**\n",
    "- **Implementing error handling, retries, and logging in workflows.**\n",
    "- **Using AWS Step Functions for event-driven automation.**\n",
    "\n",
    "These labs provide **real-world experience** in **building robust, scalable workflow automation solutions** for **enterprise-scale data pipelines**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}