{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d46e1bb9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Automating Jobs with CI/CD Pipelines - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** for implementing **CI/CD pipelines** in **data engineering and machine learning workflows** using **Databricks, Terraform, GitHub Actions, and Azure DevOps**.\n",
    "\n",
    "These labs will cover:\n",
    "- **Setting up version control for Databricks notebooks and data workflows**.\n",
    "- **Automating testing and validation of Spark jobs, Delta Lake pipelines, and ML models**.\n",
    "- **Deploying Databricks jobs using GitHub Actions and Azure DevOps**.\n",
    "- **Infrastructure as Code (IaC) for managing Databricks clusters and resources**.\n",
    "- **Monitoring, error handling, and security best practices in CI/CD pipelines**.\n",
    "- **Using existing datasets (Banks Data, Loan Foreclosure Data, Flights Data) from previous notebooks to ensure real-world relevance.**\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step instructions**, and **sample dataset usage** to ensure **scalability and reliability**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Setting Up Version Control for Databricks Notebooks**\n",
    "### **Objective:**\n",
    "- Learn how to integrate **Databricks Workflows** with **GitHub/Azure DevOps**.\n",
    "\n",
    "### **Step 1: Enable Git Integration in Databricks**\n",
    "1. Open **Databricks Workspace** → Navigate to **Repos**.\n",
    "2. Click **Add Repo** → Select **GitHub/Azure DevOps**.\n",
    "3. Connect to a GitHub repository with your **notebooks and scripts**.\n",
    "\n",
    "### **Step 2: Clone a Repository into Databricks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd44a6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "databricks repos create --url https://github.com/my-org/databricks-repo --path /Repos/my-repo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1835d2a1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Commit and Push Changes from Databricks to Git**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f285d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "git add .\n",
    "git commit -m \"Updated ETL notebooks\"\n",
    "git push origin main"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f951f5a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Databricks notebooks are **version-controlled in GitHub/Azure DevOps**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Automating Testing and Validation in CI/CD Pipelines**\n",
    "### **Objective:**\n",
    "- Implement **automated testing** for **Spark jobs and Delta Lake ingestion pipelines**.\n",
    "\n",
    "### **Step 1: Install Great Expectations for Data Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5aab92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install great_expectations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f188bd46",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Validate a Delta Lake Table Schema (Using Flights Data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a179ce01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import great_expectations as ge\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DataValidation\").getOrCreate()\n",
    "df = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")\n",
    "\n",
    "df_expectations = ge.dataset.SparkDFDataset(df)\n",
    "assert df_expectations.expect_column_values_to_not_be_null(\"flight_id\").success\n",
    "assert df_expectations.expect_column_to_exist(\"departure_time\").success"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d15e239",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Automate Testing in CI/CD Pipeline**\n",
    "Modify **GitHub Actions YAML** to run tests on every push:\n",
    "```yaml\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout Repository\n",
    "        uses: actions/checkout@v2\n",
    "      - name: Run Data Validation\n",
    "        run: python validate_data.py\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The test will **fail if missing values** exist in `flight_id` or `departure_time`.\n",
    "- CI/CD pipeline ensures **data integrity before processing**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Deploying Databricks Jobs Using GitHub Actions**\n",
    "### **Objective:**\n",
    "- Automate **Databricks job deployments** using **GitHub Actions**.\n",
    "\n",
    "### **Step 1: Create a Job JSON Config**\n",
    "```json\n",
    "{\n",
    "  \"name\": \"Loan Foreclosure ETL\",\n",
    "  \"new_cluster\": {\n",
    "    \"spark_version\": \"11.3.x-scala2.12\",\n",
    "    \"num_workers\": 3\n",
    "  },\n",
    "  \"notebook_task\": {\n",
    "    \"notebook_path\": \"/Repos/databricks-repo/loan_foreclosure_etl\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Deploy the Job Using GitHub Actions**\n",
    "Modify the CI/CD pipeline to deploy the job:\n",
    "```yaml\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Checkout repository\n",
    "        uses: actions/checkout@v2\n",
    "      - name: Deploy Databricks Job\n",
    "        run: databricks jobs create --json-file job_config.json\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Databricks **automatically executes the ETL pipeline** when new data is available.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Infrastructure as Code (IaC) for Databricks Using Terraform**\n",
    "### **Objective:**\n",
    "- Provision Databricks **clusters and job workflows** using Terraform.\n",
    "\n",
    "### **Step 1: Create a Terraform Configuration for a Cluster (Banks Data Processing)**\n",
    "```hcl\n",
    "resource \"databricks_cluster\" \"banks_cluster\" {\n",
    "  cluster_name = \"Banks Data Processing\"\n",
    "  spark_version = \"11.3.x-scala2.12\"\n",
    "  num_workers = 4\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Terraform Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560903f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "terraform init\n",
    "terraform apply -auto-approve"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "096ebf39",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- A **Databricks cluster** is automatically provisioned to handle **Banks Data Processing**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Implementing Monitoring and Error Handling in CI/CD**\n",
    "### **Objective:**\n",
    "- Enable **logging, monitoring, and failure handling** in CI/CD pipelines.\n",
    "\n",
    "### **Step 1: Configure Logging in Databricks Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30b1f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logger = logging.getLogger(\"ci-cd-pipeline\")\n",
    "logger.setLevel(logging.INFO)\n",
    "logger.info(\"Processing started for Loan Foreclosure Data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff362c91",
   "metadata": {},
   "source": [
    "### **Step 2: Implement Retry Logic in CI/CD Workflows**\n",
    "Modify **GitHub Actions YAML** to include **automatic retries**:\n",
    "```yaml\n",
    "jobs:\n",
    "  deploy:\n",
    "    steps:\n",
    "      - name: Retry Databricks Job Deployment\n",
    "        run: |\n",
    "          for i in {1..3}; do\n",
    "            databricks jobs run-now --job-id 7890 && break || sleep 60;\n",
    "          done\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Job failures **trigger alerts**, and **retry logic** ensures automatic reattempts.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have learned how to:\n",
    "- **Automate Databricks workflows using GitHub Actions and Azure DevOps**.\n",
    "- **Implement automated testing for data pipelines using real-world datasets**.\n",
    "- **Provision infrastructure using Terraform**.\n",
    "- **Enable monitoring, alerts, and retry logic in CI/CD workflows**.\n",
    "\n",
    "These labs provide **real-world experience** in **deploying production-ready data engineering pipelines** using **CI/CD best practices**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
