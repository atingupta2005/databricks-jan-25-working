{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building and Managing Workflows with Databricks Workflows - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This hands-on lab document provides **detailed, step-by-step exercises**\n",
    "for building, managing, and optimizing **Databricks Workflows**. These\n",
    "labs will cover: - **Creating Databricks Jobs and Tasks** - **Defining\n",
    "task dependencies and execution order** - **Automating ETL and ML\n",
    "pipelines using Databricks Workflows** - **Scheduling workflows using\n",
    "triggers and event-based execution** - **Monitoring and handling\n",
    "workflow errors** - **Optimizing execution time and cost efficiency** -\n",
    "**Using sample data from Bank Transactions, Loan Foreclosure Data, and\n",
    "Flights Data**\n",
    "\n",
    "Each lab provides **real-world scenarios**, **detailed execution\n",
    "steps**, **sample data usage**, and **expected outcomes** to build\n",
    "**scalable and production-ready workflows**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Creating a Simple Databricks Workflow**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Learn how to create a **Databricks Job** and define **multiple\n",
    "    tasks**.\n",
    "\n",
    "### **Step 1: Create a New Job**\n",
    "\n",
    "1.  Navigate to **Workflows \\> Jobs** in Databricks.\n",
    "2.  Click **Create Job**.\n",
    "3.  Name the job: `ETL_Pipeline_Workflow`.\n",
    "\n",
    "### **Step 2: Define Tasks Using Real Data (Flights Data)**\n",
    "\n",
    "1.  Click **Add Task** and choose **Notebook Task**.\n",
    "2.  Set **Task Name**: `Extract_Flights_Data`.\n",
    "3.  Choose a **Cluster** and **Notebook Path**\n",
    "    (`/Users/etl/extract_flights_data`).\n",
    "4.  Click **Add Task** again to create a new task:\n",
    "    -   **Task Name**: `Transform_Flights_Data`\n",
    "    -   **Depends on**: `Extract_Flights_Data`\n",
    "    -   **Notebook Path**: `/Users/etl/transform_flights_data`\n",
    "5.  Add another task:\n",
    "    -   **Task Name**: `Load_Flights_to_Delta`\n",
    "    -   **Depends on**: `Transform_Flights_Data`\n",
    "    -   **Notebook Path**: `/Users/etl/load_flights_delta`\n",
    "\n",
    "### **Step 3: Run and Monitor the Workflow**\n",
    "\n",
    "1.  Click **Run Now**.\n",
    "2.  Navigate to the **Runs** tab and track execution.\n",
    "\n",
    "**Expected Outcome:** - The workflow **executes tasks sequentially**,\n",
    "ensuring **dependency management** for processing flights data.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Automating an ETL Pipeline using Databricks Workflows**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Build an **ETL pipeline** that extracts, transforms, and loads\n",
    "    **Bank Transactions Data** into **Delta Lake**.\n",
    "\n",
    "### **Step 1: Define an ETL Job with Tasks**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"name\": \"ETL_Pipeline\",\n",
    "  \"tasks\": [\n",
    "    {\"task_key\": \"ingest_bronze\", \"notebook_task\": {\"notebook_path\": \"./bronze_ingest_bank_data\"}},\n",
    "    {\"task_key\": \"transform_silver\", \"depends_on\": [{\"task_key\": \"ingest_bronze\"}], \"notebook_task\": {\"notebook_path\": \"./silver_transform_bank_data\"}},\n",
    "    {\"task_key\": \"aggregate_gold\", \"depends_on\": [{\"task_key\": \"transform_silver\"}], \"notebook_task\": {\"notebook_path\": \"./gold_aggregate_bank_data\"}}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Execute the ETL Workflow**\n",
    "\n",
    "``` python\n",
    "from databricks.sdk import JobsClient\n",
    "jobs_api = JobsClient()\n",
    "jobs_api.create(job_payload)\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - The workflow **automates an ETL pipeline**,\n",
    "storing **processed bank transaction data in Delta Lake**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Scheduling Workflows Using Triggers**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Automate workflow execution using **time-based and event-driven\n",
    "    triggers**.\n",
    "\n",
    "### **Step 1: Configure a Time-Based Trigger for Loan Foreclosure Data**\n",
    "\n",
    "1.  Open the **ETL_Pipeline** job in Databricks Workflows.\n",
    "2.  Click **Add Trigger** → Select **Scheduled**.\n",
    "3.  Choose **Cron Expression**: `0 0 */6 * * ?` (Runs every 6 hours).\n",
    "4.  Save and enable the schedule.\n",
    "\n",
    "### **Step 2: Configure an Event-Based Trigger for New Loan Data Arrival**\n",
    "\n",
    "1.  Open the **ETL_Pipeline** job.\n",
    "2.  Click **Add Trigger** → Select **File Arrival**.\n",
    "3.  Choose **Cloud Storage Location**:\n",
    "    `abfss://my-container@my-storage-account.dfs.core.windows.net/loan-data/`.\n",
    "4.  Enable the trigger.\n",
    "\n",
    "**Expected Outcome:** - **Loan data is processed every 6 hours** and\n",
    "**whenever new loan data arrives in S3**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Error Handling and Retry Mechanisms**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Implement **error handling** and **automatic retries** in Databricks\n",
    "    Workflows.\n",
    "\n",
    "### **Step 1: Configure Retry Logic**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"task\": {\n",
    "    \"max_retries\": 3,\n",
    "    \"min_retry_interval_millis\": 60000,\n",
    "    \"retry_on_timeout\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Alerts on Failure**\n",
    "\n",
    "1.  Open **ETL_Pipeline** job.\n",
    "2.  Click **Add Alert** → Select **Failure Condition**.\n",
    "3.  Choose **Notification Method** (Email, Slack, PagerDuty).\n",
    "4.  Save the alert.\n",
    "\n",
    "**Expected Outcome:** - **Tasks automatically retry up to 3 times**\n",
    "before failure. - **Alerts notify users on job failures.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Optimizing Workflow Performance and Cost**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Improve workflow efficiency by optimizing **cluster\n",
    "    configurations**.\n",
    "\n",
    "### **Step 1: Configure Cluster for Cost Efficiency**\n",
    "\n",
    "1.  Open **ETL_Pipeline** job.\n",
    "2.  Click **Edit Cluster** → Enable **Auto-Termination (5 min idle\n",
    "    timeout)**.\n",
    "3.  Choose **Spot Instances** for cost optimization.\n",
    "\n",
    "### **Step 2: Enable Delta Caching for Faster Execution**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Lower costs** due to **auto-termination and\n",
    "spot instances**. - **Faster execution** using **Delta Caching**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Creating and managing Databricks Workflows**. - **Building automated\n",
    "ETL and ML pipelines**. - **Using scheduling triggers for workflow\n",
    "automation**. - **Implementing error handling and monitoring\n",
    "workflows**. - **Optimizing execution time and cost efficiency**.\n",
    "\n",
    "These labs provide **real-world experience** in **building\n",
    "production-ready workflows** for **enterprise-scale data and AI\n",
    "workloads**, leveraging **Flights Data, Bank Transactions, and Loan\n",
    "Foreclosure Data**."
   ],
   "id": "8fff744f-fdbc-4b5a-a201-e516fb593fe2"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
