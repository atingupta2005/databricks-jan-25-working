{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "67393ab8",
   "metadata": {},
   "source": [
    "# **Building and Managing Workflows with Databricks Workflows - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** for building, managing, and optimizing **Databricks Workflows**. These labs will cover:\n",
    "- **Creating Databricks Jobs and Tasks**\n",
    "- **Defining task dependencies and execution order**\n",
    "- **Automating ETL and ML pipelines using Databricks Workflows**\n",
    "- **Scheduling workflows using triggers and event-based execution**\n",
    "- **Monitoring and handling workflow errors**\n",
    "- **Optimizing execution time and cost efficiency**\n",
    "- **Using sample data from Bank Transactions, Loan Foreclosure Data, and Flights Data**\n",
    "\n",
    "Each lab provides **real-world scenarios**, **detailed execution steps**, **sample data usage**, and **expected outcomes** to build **scalable and production-ready workflows**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Creating a Simple Databricks Workflow**\n",
    "### **Objective:**\n",
    "- Learn how to create a **Databricks Job** and define **multiple tasks**.\n",
    "\n",
    "### **Step 1: Create a New Job**\n",
    "1. Navigate to **Workflows > Jobs** in Databricks.\n",
    "2. Click **Create Job**.\n",
    "3. Name the job: `ETL_Pipeline_Workflow`.\n",
    "\n",
    "### **Step 2: Define Tasks Using Real Data (Flights Data)**\n",
    "1. Click **Add Task** and choose **Notebook Task**.\n",
    "2. Set **Task Name**: `Extract_Flights_Data`.\n",
    "3. Choose a **Cluster** and **Notebook Path** (`/Users/etl/extract_flights_data`).\n",
    "4. Click **Add Task** again to create a new task:\n",
    "   - **Task Name**: `Transform_Flights_Data`\n",
    "   - **Depends on**: `Extract_Flights_Data`\n",
    "   - **Notebook Path**: `/Users/etl/transform_flights_data`\n",
    "5. Add another task:\n",
    "   - **Task Name**: `Load_Flights_to_Delta`\n",
    "   - **Depends on**: `Transform_Flights_Data`\n",
    "   - **Notebook Path**: `/Users/etl/load_flights_delta`\n",
    "\n",
    "### **Step 3: Run and Monitor the Workflow**\n",
    "1. Click **Run Now**.\n",
    "2. Navigate to the **Runs** tab and track execution.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The workflow **executes tasks sequentially**, ensuring **dependency management** for processing flights data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Automating an ETL Pipeline using Databricks Workflows**\n",
    "### **Objective:**\n",
    "- Build an **ETL pipeline** that extracts, transforms, and loads **Bank Transactions Data** into **Delta Lake**.\n",
    "\n",
    "### **Step 1: Define an ETL Job with Tasks**\n",
    "```json\n",
    "{\n",
    "  \"name\": \"ETL_Pipeline\",\n",
    "  \"tasks\": [\n",
    "    {\"task_key\": \"ingest_bronze\", \"notebook_task\": {\"notebook_path\": \"./bronze_ingest_bank_data\"}},\n",
    "    {\"task_key\": \"transform_silver\", \"depends_on\": [{\"task_key\": \"ingest_bronze\"}], \"notebook_task\": {\"notebook_path\": \"./silver_transform_bank_data\"}},\n",
    "    {\"task_key\": \"aggregate_gold\", \"depends_on\": [{\"task_key\": \"transform_silver\"}], \"notebook_task\": {\"notebook_path\": \"./gold_aggregate_bank_data\"}}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Execute the ETL Workflow**\n",
    "```python\n",
    "from databricks.sdk import JobsClient\n",
    "jobs_api = JobsClient()\n",
    "jobs_api.create(job_payload)\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The workflow **automates an ETL pipeline**, storing **processed bank transaction data in Delta Lake**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Scheduling Workflows Using Triggers**\n",
    "### **Objective:**\n",
    "- Automate workflow execution using **time-based and event-driven triggers**.\n",
    "\n",
    "### **Step 1: Configure a Time-Based Trigger for Loan Foreclosure Data**\n",
    "1. Open the **ETL_Pipeline** job in Databricks Workflows.\n",
    "2. Click **Add Trigger** → Select **Scheduled**.\n",
    "3. Choose **Cron Expression**: `0 0 */6 * * ?` (Runs every 6 hours).\n",
    "4. Save and enable the schedule.\n",
    "\n",
    "### **Step 2: Configure an Event-Based Trigger for New Loan Data Arrival**\n",
    "1. Open the **ETL_Pipeline** job.\n",
    "2. Click **Add Trigger** → Select **File Arrival**.\n",
    "3. Choose **Cloud Storage Location**: `abfss://my-container@my-storage-account.dfs.core.windows.net/loan-data/`.\n",
    "4. Enable the trigger.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Loan data is processed every 6 hours** and **whenever new loan data arrives in S3**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Error Handling and Retry Mechanisms**\n",
    "### **Objective:**\n",
    "- Implement **error handling** and **automatic retries** in Databricks Workflows.\n",
    "\n",
    "### **Step 1: Configure Retry Logic**\n",
    "```json\n",
    "{\n",
    "  \"task\": {\n",
    "    \"max_retries\": 3,\n",
    "    \"min_retry_interval_millis\": 60000,\n",
    "    \"retry_on_timeout\": true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Alerts on Failure**\n",
    "1. Open **ETL_Pipeline** job.\n",
    "2. Click **Add Alert** → Select **Failure Condition**.\n",
    "3. Choose **Notification Method** (Email, Slack, PagerDuty).\n",
    "4. Save the alert.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Tasks automatically retry up to 3 times** before failure.\n",
    "- **Alerts notify users on job failures.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Optimizing Workflow Performance and Cost**\n",
    "### **Objective:**\n",
    "- Improve workflow efficiency by optimizing **cluster configurations**.\n",
    "\n",
    "### **Step 1: Configure Cluster for Cost Efficiency**\n",
    "1. Open **ETL_Pipeline** job.\n",
    "2. Click **Edit Cluster** → Enable **Auto-Termination (5 min idle timeout)**.\n",
    "3. Choose **Spot Instances** for cost optimization.\n",
    "\n",
    "### **Step 2: Enable Delta Caching for Faster Execution**\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Lower costs** due to **auto-termination and spot instances**.\n",
    "- **Faster execution** using **Delta Caching**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Creating and managing Databricks Workflows**.\n",
    "- **Building automated ETL and ML pipelines**.\n",
    "- **Using scheduling triggers for workflow automation**.\n",
    "- **Implementing error handling and monitoring workflows**.\n",
    "- **Optimizing execution time and cost efficiency**.\n",
    "\n",
    "These labs provide **real-world experience** in **building production-ready workflows** for **enterprise-scale data and AI workloads**, leveraging **Flights Data, Bank Transactions, and Loan Foreclosure Data**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}