{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bed486",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Advanced Transformation Techniques with PySpark - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This **comprehensive hands-on lab guide** provides **detailed, step-by-step instructions** for applying **advanced transformation techniques in PySpark** using **real-world datasets such as Bank Transactions, Loan Foreclosures, and Flight Data** (as used in previous sample notebooks). These labs are designed to ensure **minimal changes** are required for execution and cover:\n",
    "\n",
    "- **Lazy evaluation & DAG execution**\n",
    "- **Data preprocessing & schema evolution**\n",
    "- **Complex aggregations & window functions**\n",
    "- **Optimized joins & broadcast techniques**\n",
    "- **Handling complex nested data**\n",
    "- **Performance tuning in PySpark**\n",
    "\n",
    "Each lab provides **real-world datasets**, **in-depth explanations**, and **detailed code** to ensure **efficient and scalable big data processing**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Understanding Lazy Evaluation & DAG Execution**\n",
    "### **Objective:**\n",
    "- Learn how PySpark transformations execute lazily.\n",
    "- Visualize execution plans and DAG (Directed Acyclic Graph).\n",
    "\n",
    "### **Step 1: Load Bank Transactions Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e548fa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LazyEvaluationLab\").getOrCreate()\n",
    "\n",
    "# Load bank transaction dataset\n",
    "bank_df = spark.read.csv(\"/mnt/data/bank_transactions.csv\", header=True, inferSchema=True)\n",
    "bank_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c33b319a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Apply Transformations Without Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd4c28f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = bank_df.filter(bank_df.amount > 5000)\n",
    "selected_df = filtered_df.select(\"transaction_id\", \"amount\", \"account_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e49a7e6",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Trigger Execution & Inspect DAG**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce80a74",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "selected_df.show(10)\n",
    "print(selected_df.explain(True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0da7ae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- DAG should confirm execution only after an action is triggered.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Data Preprocessing & Schema Evolution**\n",
    "### **Objective:**\n",
    "- Handle missing values, duplicates, and schema changes.\n",
    "\n",
    "### **Step 1: Load Loan Foreclosure Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5943435",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_df = spark.read.parquet(\"/mnt/data/loan_foreclosure.parquet\")\n",
    "loan_df.printSchema()\n",
    "loan_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdc34a9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Handle Missing Values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "170bfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_cleaned = loan_df.na.fill({\"credit_score\": 650, \"income\": 50000})\n",
    "loan_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d733e7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Schema Evolution - Merging New Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a252a32c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_loans_df = spark.createDataFrame([(12345, \"NewLoan\", 750, 70000, \"Approved\")],\n",
    "                                      [\"loan_id\", \"loan_type\", \"credit_score\", \"income\", \"status\"])\n",
    "\n",
    "merged_loans = loan_df.unionByName(new_loans_df, allowMissingColumns=True)\n",
    "merged_loans.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05683b11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 3: Complex Aggregations & Window Functions**\n",
    "### **Objective:**\n",
    "- Use window functions for running totals, ranks, and trend analysis.\n",
    "\n",
    "### **Step 1: Load Flight Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32ba0d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "flight_df = spark.read.parquet(\"/mnt/data/flights.parquet\")\n",
    "flight_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c66f9d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Compute Total Flights per Airline**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75748a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "total_flights = flight_df.groupBy(\"airline\").agg(count(\"flight_id\").alias(\"total_flights\"))\n",
    "total_flights.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26cbcbb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Apply Window Functions to Rank Delays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9549e3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"airline\").orderBy(flight_df.delay.desc())\n",
    "flight_df = flight_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "flight_df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1fb1b19",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 4: Optimized Joins & Broadcast Techniques**\n",
    "### **Objective:**\n",
    "- Learn efficient join strategies and use broadcast joins.\n",
    "\n",
    "### **Step 1: Load Bank Customer & Transaction Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba88e2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "customers_df = spark.read.csv(\"/mnt/data/bank_customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"/mnt/data/bank_transactions.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b305cb0c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Perform a Join on Customer Transactions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2429c76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = customers_df.join(transactions_df, \"customer_id\", \"inner\")\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea17567",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Use Broadcast Join for Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3b62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "optimized_df = transactions_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
    "optimized_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45b1dcd",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 5: Handling Complex Nested Data**\n",
    "### **Objective:**\n",
    "- Process struct, array, and map-based data.\n",
    "\n",
    "### **Step 1: Load Nested Loan Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c8ae3ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_loan_df = spark.read.json(\"/mnt/data/nested_loan_data.json\")\n",
    "nested_loan_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562193c9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Extract Struct Fields**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "083206f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "nested_loan_df.select(\"loan_details.interest_rate\", \"loan_details.duration\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f34b2d8e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Flattening Arrays**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316ceff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_loans = nested_loan_df.withColumn(\"customer_loan\", explode(\"loan_history\"))\n",
    "exploded_loans.select(\"customer_id\", \"customer_loan.*\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6845515",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 6: Performance Tuning & Optimization**\n",
    "### **Objective:**\n",
    "- Optimize PySpark workloads using caching, partitioning, and execution plans.\n",
    "\n",
    "### **Step 1: Caching & Persisting Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093072b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_cleaned.cache()\n",
    "loan_cleaned.count()  # Triggers cache storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b721347a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Inspect Execution Plans**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181df5e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "loan_cleaned.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d17597c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Managing Partitions for Scalability**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b930aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_loans = loan_cleaned.repartition(\"loan_status\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b82d173",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **exhaustive hands-on labs**, you will have mastered:\n",
    "- **Lazy evaluation & DAG execution**\n",
    "- **Data preprocessing & schema evolution**\n",
    "- **Complex aggregations & window functions**\n",
    "- **Optimized joins & broadcast techniques**\n",
    "- **Handling complex nested data structures**\n",
    "- **Performance tuning for large-scale datasets**\n",
    "\n",
    "These labs provide **enterprise-grade real-world experience** in handling **banking, loan foreclosure, and flight data** using **PySpark's advanced transformations**, ensuring **optimized and scalable big data workflows**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
