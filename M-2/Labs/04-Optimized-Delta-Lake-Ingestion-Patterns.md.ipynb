{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Delta Lake Ingestion: Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab guide provides **detailed, step-by-step hands-on exercises** to\n",
    "optimize **Delta Lake ingestion patterns**. These labs ensure\n",
    "**high-performance, scalable, and cost-efficient data ingestion**\n",
    "workflows. Each lab is designed to work with **minimal changes**, using\n",
    "**variables and configurations** for flexibility.\n",
    "\n",
    "By completing these labs, you will gain experience in: 1. **Batch and\n",
    "Streaming Data Ingestion into Delta Lake** 2. **Optimizing Write\n",
    "Performance using Partitioning, Z-Ordering, and Auto-Optimized Writes**\n",
    "3. **Schema Evolution and Enforced Data Consistency** 4. **Managing\n",
    "Incremental Data and Change Data Capture (CDC)** 5. **Storage Management\n",
    "with Auto-Compaction, Vacuum, and Time Travel** 6. **Real-World Use\n",
    "Cases with Detailed Implementation**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Ingesting Batch and Streaming Data into Delta Lake**\n",
    "\n",
    "### **Step 1: Initialize Spark Session and Set Up Variables**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeIngestion\").getOrCreate()\n",
    "\n",
    "# Define storage paths\n",
    "base_path = \"/mnt/delta/\"\n",
    "raw_data_path = \"/mnt/raw_data/\"\n",
    "checkpoint_path = \"/mnt/checkpoints/\"\n",
    "\n",
    "# Define table names\n",
    "delta_table_name = \"user_events\"\n",
    "```\n",
    "\n",
    "### **Step 2: Batch Data Ingestion using Append Mode**\n",
    "\n",
    "``` python\n",
    "# Sample batch data\n",
    "data = [(\"user_1\", \"login\", \"2024-01-01\"), (\"user_2\", \"purchase\", \"2024-01-02\")]\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"event_type\", \"event_date\"])\n",
    "\n",
    "# Writing to Delta Lake in Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").save(f\"{base_path}{delta_table_name}\")\n",
    "```\n",
    "\n",
    "### **Step 3: Streaming Data Ingestion using Structured Streaming**\n",
    "\n",
    "``` python\n",
    "streaming_df = spark.readStream.format(\"json\").schema(\"user_id STRING, event_type STRING, event_date STRING\").load(raw_data_path)\n",
    "\n",
    "streaming_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}logs/\")\\\n",
    "    .start(f\"{base_path}logs\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Optimizing Write Performance with Partitioning and Z-Ordering**\n",
    "\n",
    "### **Step 1: Writing Data with Partitioning**\n",
    "\n",
    "``` python\n",
    "df.write.format(\"delta\").partitionBy(\"event_date\").save(f\"{base_path}partitioned_events\")\n",
    "```\n",
    "\n",
    "### **Step 2: Optimizing Reads using Z-Ordering**\n",
    "\n",
    "``` sql\n",
    "OPTIMIZE delta.`/mnt/delta/events` ZORDER BY (event_type);\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Schema Evolution and Enforced Data Consistency**\n",
    "\n",
    "### **Step 1: Enabling Schema Evolution**\n",
    "\n",
    "``` python\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{base_path}schema_evolution\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enforcing Schema on Write**\n",
    "\n",
    "``` sql\n",
    "ALTER TABLE delta_table ADD COLUMN new_column STRING;\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Managing Incremental Data and Change Data Capture (CDC)**\n",
    "\n",
    "### **Step 1: Merging Incremental Data Efficiently**\n",
    "\n",
    "``` sql\n",
    "MERGE INTO transactions AS target\n",
    "USING new_transactions AS source\n",
    "ON target.transaction_id = source.transaction_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.amount = source.amount, target.status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (transaction_id, amount, status, timestamp) VALUES (source.transaction_id, source.amount, source.status, source.timestamp);\n",
    "```\n",
    "\n",
    "### **Step 2: Using Change Data Capture (CDC) Queries**\n",
    "\n",
    "``` sql\n",
    "SELECT * FROM delta_table WHERE _change_type = 'update_postimage';\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Optimizing Query Performance and Storage Management**\n",
    "\n",
    "### **Step 1: Enabling Auto-Optimized Writes**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Enabling Automatic File Compaction**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 3: Retaining Historical Data using Time Travel**\n",
    "\n",
    "``` sql\n",
    "SELECT * FROM delta.`/mnt/delta/events` VERSION AS OF 10;\n",
    "```\n",
    "\n",
    "### **Step 4: Deleting Old Data with VACUUM**\n",
    "\n",
    "``` sql\n",
    "VACUUM delta.`/mnt/delta/events` RETAIN 7 HOURS;\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 6: Real-World Use Cases and Implementations**\n",
    "\n",
    "### **Use Case 1: Real-Time Security Log Ingestion and Processing**\n",
    "\n",
    "#### **Scenario:** A cybersecurity firm ingests **millions of log events per second** for **real-time threat detection**.\n",
    "\n",
    "#### **Implementation:**\n",
    "\n",
    "``` python\n",
    "log_df = spark.readStream.format(\"json\").load(f\"{raw_data_path}security_logs/\")\n",
    "\n",
    "log_df.writeStream.format(\"delta\")\\\n",
    "    .partitionBy(\"log_source\", \"event_date\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}security_logs/\")\\\n",
    "    .start(f\"{base_path}security_logs\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### **Use Case 2: Banking Transaction Upserts with CDC**\n",
    "\n",
    "#### **Scenario:** A financial institution **processes real-time transactions** and ensures **data consistency**.\n",
    "\n",
    "#### **Implementation:**\n",
    "\n",
    "``` sql\n",
    "MERGE INTO bank_transactions AS target\n",
    "USING new_transactions AS source\n",
    "ON target.transaction_id = source.transaction_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.amount = source.amount, target.status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (transaction_id, amount, status, timestamp) VALUES (source.transaction_id, source.amount, source.status, source.timestamp);\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "### **Use Case 3: ETL Pipeline for E-Commerce Data Processing**\n",
    "\n",
    "#### **Scenario:** An e-commerce company needs to process **customer actions in real-time** and **refresh product catalog daily**.\n",
    "\n",
    "#### **Implementation:**\n",
    "\n",
    "``` python\n",
    "# Process customer activity logs in real-time\n",
    "customer_logs_df = spark.readStream.format(\"json\").load(f\"{raw_data_path}ecommerce/logs/\")\n",
    "\n",
    "customer_logs_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}ecommerce/\")\\\n",
    "    .start(f\"{base_path}ecommerce_logs\")\n",
    "\n",
    "# Refresh product catalog daily\n",
    "daily_catalog_df = spark.read.format(\"json\").load(f\"{raw_data_path}ecommerce/product_catalog/\")\n",
    "\n",
    "daily_catalog_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{base_path}product_catalog\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these labs, you will **master optimized Delta Lake\n",
    "ingestion patterns** and build **scalable, high-performance, and\n",
    "cost-efficient** data pipelines in **Databricks**."
   ],
   "id": "eede5d23-e5f4-42fc-82cb-6b0107287c87"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
