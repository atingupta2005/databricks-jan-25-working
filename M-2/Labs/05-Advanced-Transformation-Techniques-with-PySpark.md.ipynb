{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Transformation Techniques with PySpark - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This **comprehensive hands-on lab guide** provides **detailed,\n",
    "step-by-step instructions** for applying **advanced transformation\n",
    "techniques in PySpark** using **real-world datasets such as Bank\n",
    "Transactions, Loan Foreclosures, and Flight Data** (as used in previous\n",
    "sample notebooks). These labs are designed to ensure **minimal changes**\n",
    "are required for execution and cover:\n",
    "\n",
    "-   **Lazy evaluation & DAG execution**\n",
    "-   **Data preprocessing & schema evolution**\n",
    "-   **Complex aggregations & window functions**\n",
    "-   **Optimized joins & broadcast techniques**\n",
    "-   **Handling complex nested data**\n",
    "-   **Performance tuning in PySpark**\n",
    "\n",
    "Each lab provides **real-world datasets**, **in-depth explanations**,\n",
    "and **detailed code** to ensure **efficient and scalable big data\n",
    "processing**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Understanding Lazy Evaluation & DAG Execution**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Learn how PySpark transformations execute lazily.\n",
    "-   Visualize execution plans and DAG (Directed Acyclic Graph).\n",
    "\n",
    "### **Step 1: Load Bank Transactions Data**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"LazyEvaluationLab\").getOrCreate()\n",
    "\n",
    "# Load bank transaction dataset\n",
    "bank_df = spark.read.csv(\"/mnt/data/bank_transactions.csv\", header=True, inferSchema=True)\n",
    "bank_df.show(5)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Transformations Without Execution**\n",
    "\n",
    "``` python\n",
    "filtered_df = bank_df.filter(bank_df.amount > 5000)\n",
    "selected_df = filtered_df.select(\"transaction_id\", \"amount\", \"account_type\")\n",
    "```\n",
    "\n",
    "### **Step 3: Trigger Execution & Inspect DAG**\n",
    "\n",
    "``` python\n",
    "selected_df.show(10)\n",
    "print(selected_df.explain(True))\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - DAG should confirm execution only after an\n",
    "action is triggered.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Data Preprocessing & Schema Evolution**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Handle missing values, duplicates, and schema changes.\n",
    "\n",
    "### **Step 1: Load Loan Foreclosure Data**\n",
    "\n",
    "``` python\n",
    "loan_df = spark.read.parquet(\"/mnt/data/loan_foreclosure.parquet\")\n",
    "loan_df.printSchema()\n",
    "loan_df.show(5)\n",
    "```\n",
    "\n",
    "### **Step 2: Handle Missing Values**\n",
    "\n",
    "``` python\n",
    "loan_cleaned = loan_df.na.fill({\"credit_score\": 650, \"income\": 50000})\n",
    "loan_cleaned.show(5)\n",
    "```\n",
    "\n",
    "### **Step 3: Schema Evolution - Merging New Data**\n",
    "\n",
    "``` python\n",
    "new_loans_df = spark.createDataFrame([(12345, \"NewLoan\", 750, 70000, \"Approved\")],\n",
    "                                      [\"loan_id\", \"loan_type\", \"credit_score\", \"income\", \"status\"])\n",
    "\n",
    "merged_loans = loan_df.unionByName(new_loans_df, allowMissingColumns=True)\n",
    "merged_loans.show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Complex Aggregations & Window Functions**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Use window functions for running totals, ranks, and trend analysis.\n",
    "\n",
    "### **Step 1: Load Flight Data**\n",
    "\n",
    "``` python\n",
    "flight_df = spark.read.parquet(\"/mnt/data/flights.parquet\")\n",
    "flight_df.show(5)\n",
    "```\n",
    "\n",
    "### **Step 2: Compute Total Flights per Airline**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import count\n",
    "\n",
    "total_flights = flight_df.groupBy(\"airline\").agg(count(\"flight_id\").alias(\"total_flights\"))\n",
    "total_flights.show()\n",
    "```\n",
    "\n",
    "### **Step 3: Apply Window Functions to Rank Delays**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark.sql.functions import rank\n",
    "\n",
    "window_spec = Window.partitionBy(\"airline\").orderBy(flight_df.delay.desc())\n",
    "flight_df = flight_df.withColumn(\"rank\", rank().over(window_spec))\n",
    "flight_df.show(10)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Optimized Joins & Broadcast Techniques**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Learn efficient join strategies and use broadcast joins.\n",
    "\n",
    "### **Step 1: Load Bank Customer & Transaction Data**\n",
    "\n",
    "``` python\n",
    "customers_df = spark.read.csv(\"/mnt/data/bank_customers.csv\", header=True, inferSchema=True)\n",
    "transactions_df = spark.read.csv(\"/mnt/data/bank_transactions.csv\", header=True, inferSchema=True)\n",
    "```\n",
    "\n",
    "### **Step 2: Perform a Join on Customer Transactions**\n",
    "\n",
    "``` python\n",
    "joined_df = customers_df.join(transactions_df, \"customer_id\", \"inner\")\n",
    "joined_df.show(5)\n",
    "```\n",
    "\n",
    "### **Step 3: Use Broadcast Join for Optimization**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "optimized_df = transactions_df.join(broadcast(customers_df), \"customer_id\", \"inner\")\n",
    "optimized_df.show(5)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Handling Complex Nested Data**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Process struct, array, and map-based data.\n",
    "\n",
    "### **Step 1: Load Nested Loan Data**\n",
    "\n",
    "``` python\n",
    "nested_loan_df = spark.read.json(\"/mnt/data/nested_loan_data.json\")\n",
    "nested_loan_df.show(truncate=False)\n",
    "```\n",
    "\n",
    "### **Step 2: Extract Struct Fields**\n",
    "\n",
    "``` python\n",
    "nested_loan_df.select(\"loan_details.interest_rate\", \"loan_details.duration\").show()\n",
    "```\n",
    "\n",
    "### **Step 3: Flattening Arrays**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import explode\n",
    "\n",
    "exploded_loans = nested_loan_df.withColumn(\"customer_loan\", explode(\"loan_history\"))\n",
    "exploded_loans.select(\"customer_id\", \"customer_loan.*\").show()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 6: Performance Tuning & Optimization**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Optimize PySpark workloads using caching, partitioning, and\n",
    "    execution plans.\n",
    "\n",
    "### **Step 1: Caching & Persisting Data**\n",
    "\n",
    "``` python\n",
    "loan_cleaned.cache()\n",
    "loan_cleaned.count()  # Triggers cache storage\n",
    "```\n",
    "\n",
    "### **Step 2: Inspect Execution Plans**\n",
    "\n",
    "``` python\n",
    "loan_cleaned.explain(True)\n",
    "```\n",
    "\n",
    "### **Step 3: Managing Partitions for Scalability**\n",
    "\n",
    "``` python\n",
    "optimized_loans = loan_cleaned.repartition(\"loan_status\")\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **exhaustive hands-on labs**, you will have\n",
    "mastered: - **Lazy evaluation & DAG execution** - **Data preprocessing &\n",
    "schema evolution** - **Complex aggregations & window functions** -\n",
    "**Optimized joins & broadcast techniques** - **Handling complex nested\n",
    "data structures** - **Performance tuning for large-scale datasets**\n",
    "\n",
    "These labs provide **enterprise-grade real-world experience** in\n",
    "handling **banking, loan foreclosure, and flight data** using\n",
    "**PySparkâ€™s advanced transformations**, ensuring **optimized and\n",
    "scalable big data workflows**."
   ],
   "id": "db80edbc-8a4b-4621-a051-4a61561216a1"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
