{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f87a3fa",
   "metadata": {},
   "source": [
    "# **Managing Schema Evolution in Delta Lake - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides **detailed, step-by-step exercises** to help you master **schema evolution techniques** in **Delta Lake**. These hands-on labs focus on real-world scenarios, using **Bank Transactions, Loan Foreclosures, and Flight Data** as referenced in previous sample notebooks. These labs ensure **minimal changes for execution** and cover:\n",
    "\n",
    "- **Schema enforcement and rejection of mismatched data**\n",
    "- **Appending data with evolving schemas**\n",
    "- **Merging schema changes dynamically**\n",
    "- **Handling nested structures and complex schema modifications**\n",
    "- **Implementing best practices for schema evolution in production**\n",
    "\n",
    "Each lab includes **detailed code examples, step-by-step execution guidance, and validation checks**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Understanding Schema Enforcement in Delta Lake**\n",
    "### **Objective:**\n",
    "- Learn how Delta Lake enforces schema consistency.\n",
    "- Prevent accidental schema modifications.\n",
    "\n",
    "### **Step 1: Create a Delta Table with a Fixed Schema**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"SchemaEnforcementLab\").getOrCreate()\n",
    "\n",
    "# Load bank transactions dataset\n",
    "bank_data_path = \"/mnt/data/bank_transactions.csv\"\n",
    "bank_df = spark.read.csv(bank_data_path, header=True, inferSchema=True)\n",
    "\n",
    "# Define schema\n",
    "df = bank_df.select(\"transaction_id\", \"customer_id\", \"amount\", \"transaction_date\")\n",
    "df.write.format(\"delta\").save(\"/mnt/delta/bank_transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Try Writing Data with a Different Schema**\n",
    "```python\n",
    "new_data = [(1001, 5002, 1500, \"2024-05-01\", \"Online Purchase\")]\n",
    "df_new = spark.createDataFrame(new_data, [\"transaction_id\", \"customer_id\", \"amount\", \"transaction_date\", \"transaction_type\"])\n",
    "\n",
    "df_new.write.format(\"delta\").mode(\"append\").save(\"/mnt/delta/bank_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The operation should fail due to schema mismatch.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Enabling Schema Evolution for Append Mode**\n",
    "### **Objective:**\n",
    "- Allow new columns to be added dynamically.\n",
    "\n",
    "### **Step 1: Enable Schema Evolution in Write Operation**\n",
    "```python\n",
    "df_new.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"/mnt/delta/bank_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The table should now include the `transaction_type` column.\n",
    "- Historical records should have `NULL` in the new column.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Handling Schema Evolution in Merge Operations**\n",
    "### **Objective:**\n",
    "- Manage schema evolution when merging updates.\n",
    "\n",
    "### **Step 1: Create an Initial Loan Foreclosure Table**\n",
    "```python\n",
    "loan_data_path = \"/mnt/data/loan_foreclosure.parquet\"\n",
    "loan_df = spark.read.parquet(loan_data_path)\n",
    "loan_df.write.format(\"delta\").save(\"/mnt/delta/loan_foreclosure\")\n",
    "```\n",
    "\n",
    "### **Step 2: Merge Data with New Columns**\n",
    "```python\n",
    "new_data = [(105, \"2024-06-15\", 12000, \"Pending Review\")]\n",
    "df_new = spark.createDataFrame(new_data, [\"loan_id\", \"foreclosure_date\", \"amount\", \"review_status\"])\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "loan_table = DeltaTable.forPath(spark, \"/mnt/delta/loan_foreclosure\")\n",
    "\n",
    "loan_table.alias(\"t\").merge(\n",
    "    df_new.alias(\"s\"), \"t.loan_id = s.loan_id\"\").whenNotMatchedInsertAll()\n",
    ".option(\"mergeSchema\", \"true\").execute()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The `review_status` column should be added.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Schema Evolution in Nested and Struct Data**\n",
    "### **Objective:**\n",
    "- Manage schema changes in nested data.\n",
    "\n",
    "### **Step 1: Create a Delta Table with Nested Columns**\n",
    "```python\n",
    "from pyspark.sql.functions import struct\n",
    "\n",
    "data = [(1, struct(\"John\", \"Doe\"), \"NY\", \"Regular Customer\")]\n",
    "df = spark.createDataFrame(data, [\"customer_id\", \"name\", \"city\", \"customer_category\"])\n",
    "df.write.format(\"delta\").save(\"/mnt/delta/customers_nested\")\n",
    "```\n",
    "\n",
    "### **Step 2: Append Data with a New Field in Struct**\n",
    "```python\n",
    "new_data = [(2, struct(\"Alice\", \"Smith\", \"VIP\"), \"LA\", \"High Net Worth\")]\n",
    "df_new = spark.createDataFrame(new_data, [\"customer_id\", \"name\", \"city\", \"customer_category\"])\n",
    "df_new.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"/mnt/delta/customers_nested\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The `name` struct should now include a `VIP` category.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Schema Evolution in Streaming Data**\n",
    "### **Objective:**\n",
    "- Handle schema drift in structured streaming.\n",
    "\n",
    "### **Step 1: Simulate a Streaming Data Source**\n",
    "```python\n",
    "flight_data_path = \"/mnt/data/flights.parquet\"\n",
    "streaming_df = spark.readStream.format(\"delta\").load(\"/mnt/delta/flights\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Schema Evolution for Streaming Writes**\n",
    "```python\n",
    "new_stream_data = [(207, \"2024-07-10\", \"Delayed\", \"Weather Issue\")]\n",
    "df_new = spark.createDataFrame(new_stream_data, [\"flight_id\", \"flight_date\", \"status\", \"delay_reason\"])\n",
    "\n",
    "df_new.writeStream.format(\"delta\").option(\"mergeSchema\", \"true\").outputMode(\"append\").start(\"/mnt/delta/flights\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The `delay_reason` column should be dynamically added without affecting streaming pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these hands-on labs, you have gained expertise in:\n",
    "- **Schema enforcement and preventing unintended schema changes.**\n",
    "- **Handling schema evolution dynamically using append and merge operations.**\n",
    "- **Managing schema modifications in nested and struct columns.**\n",
    "- **Ensuring seamless schema evolution in structured streaming workflows.**\n",
    "\n",
    "These labs provide **real-world experience** in schema evolution using **Delta Lake**, ensuring data pipelines remain **scalable, reliable, and flexible**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}