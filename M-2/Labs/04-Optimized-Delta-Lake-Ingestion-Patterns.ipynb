{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a59148e5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Delta Lake Ingestion: Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab guide provides **detailed, step-by-step hands-on exercises** to optimize **Delta Lake ingestion patterns**. These labs ensure **high-performance, scalable, and cost-efficient data ingestion** workflows. Each lab is designed to work with **minimal changes**, using **variables and configurations** for flexibility.\n",
    "\n",
    "By completing these labs, you will gain experience in:\n",
    "1. **Batch and Streaming Data Ingestion into Delta Lake**\n",
    "2. **Optimizing Write Performance using Partitioning, Z-Ordering, and Auto-Optimized Writes**\n",
    "3. **Schema Evolution and Enforced Data Consistency**\n",
    "4. **Managing Incremental Data and Change Data Capture (CDC)**\n",
    "5. **Storage Management with Auto-Compaction, Vacuum, and Time Travel**\n",
    "6. **Real-World Use Cases with Detailed Implementation**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Ingesting Batch and Streaming Data into Delta Lake**\n",
    "\n",
    "### **Step 1: Initialize Spark Session and Set Up Variables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7e72ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaLakeIngestion\").getOrCreate()\n",
    "\n",
    "# Define storage paths\n",
    "base_path = \"/mnt/delta/\"\n",
    "raw_data_path = \"/mnt/raw_data/\"\n",
    "checkpoint_path = \"/mnt/checkpoints/\"\n",
    "\n",
    "# Define table names\n",
    "delta_table_name = \"user_events\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c72f76e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Batch Data Ingestion using Append Mode**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff1d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample batch data\n",
    "data = [(\"user_1\", \"login\", \"2024-01-01\"), (\"user_2\", \"purchase\", \"2024-01-02\")]\n",
    "df = spark.createDataFrame(data, [\"user_id\", \"event_type\", \"event_date\"])\n",
    "\n",
    "# Writing to Delta Lake in Append mode\n",
    "df.write.format(\"delta\").mode(\"append\").save(f\"{base_path}{delta_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b767e59",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Streaming Data Ingestion using Structured Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f69b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "streaming_df = spark.readStream.format(\"json\").schema(\"user_id STRING, event_type STRING, event_date STRING\").load(raw_data_path)\n",
    "\n",
    "streaming_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}logs/\")\\\n",
    "    .start(f\"{base_path}logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab1284b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 2: Optimizing Write Performance with Partitioning and Z-Ordering**\n",
    "\n",
    "### **Step 1: Writing Data with Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f63c506",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").partitionBy(\"event_date\").save(f\"{base_path}partitioned_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af624ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Optimizing Reads using Z-Ordering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa968403",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "OPTIMIZE delta.`/mnt/delta/events` ZORDER BY (event_type);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac971e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 3: Schema Evolution and Enforced Data Consistency**\n",
    "\n",
    "### **Step 1: Enabling Schema Evolution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9254ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(f\"{base_path}schema_evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62009a29",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Enforcing Schema on Write**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5548cfa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE delta_table ADD COLUMN new_column STRING;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6a9887",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 4: Managing Incremental Data and Change Data Capture (CDC)**\n",
    "\n",
    "### **Step 1: Merging Incremental Data Efficiently**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07880849",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "MERGE INTO transactions AS target\n",
    "USING new_transactions AS source\n",
    "ON target.transaction_id = source.transaction_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.amount = source.amount, target.status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (transaction_id, amount, status, timestamp) VALUES (source.transaction_id, source.amount, source.status, source.timestamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e424d4b5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Using Change Data Capture (CDC) Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7337af36",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM delta_table WHERE _change_type = 'update_postimage';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e75c2d2b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 5: Optimizing Query Performance and Storage Management**\n",
    "\n",
    "### **Step 1: Enabling Auto-Optimized Writes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ef396f",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce031a5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Enabling Automatic File Compaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "686da299",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "702417cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Retaining Historical Data using Time Travel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM delta.`/mnt/delta/events` VERSION AS OF 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7120b88",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 4: Deleting Old Data with VACUUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fbceaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "VACUUM delta.`/mnt/delta/events` RETAIN 7 HOURS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4264d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 6: Real-World Use Cases and Implementations**\n",
    "\n",
    "### **Use Case 1: Real-Time Security Log Ingestion and Processing**\n",
    "\n",
    "#### **Scenario:** A cybersecurity firm ingests **millions of log events per second** for **real-time threat detection**.\n",
    "\n",
    "#### **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f45d970",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_df = spark.readStream.format(\"json\").load(f\"{raw_data_path}security_logs/\")\n",
    "\n",
    "log_df.writeStream.format(\"delta\")\\\n",
    "    .partitionBy(\"log_source\", \"event_date\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}security_logs/\")\\\n",
    "    .start(f\"{base_path}security_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c76f804",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### **Use Case 2: Banking Transaction Upserts with CDC**\n",
    "\n",
    "#### **Scenario:** A financial institution **processes real-time transactions** and ensures **data consistency**.\n",
    "\n",
    "#### **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415df4fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "MERGE INTO bank_transactions AS target\n",
    "USING new_transactions AS source\n",
    "ON target.transaction_id = source.transaction_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.amount = source.amount, target.status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (transaction_id, amount, status, timestamp) VALUES (source.transaction_id, source.amount, source.status, source.timestamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7894a016",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "### **Use Case 3: ETL Pipeline for E-Commerce Data Processing**\n",
    "\n",
    "#### **Scenario:** An e-commerce company needs to process **customer actions in real-time** and **refresh product catalog daily**.\n",
    "\n",
    "#### **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a3e459",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process customer activity logs in real-time\n",
    "customer_logs_df = spark.readStream.format(\"json\").load(f\"{raw_data_path}ecommerce/logs/\")\n",
    "\n",
    "customer_logs_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", f\"{checkpoint_path}ecommerce/\")\\\n",
    "    .start(f\"{base_path}ecommerce_logs\")\n",
    "\n",
    "# Refresh product catalog daily\n",
    "daily_catalog_df = spark.read.format(\"json\").load(f\"{raw_data_path}ecommerce/product_catalog/\")\n",
    "\n",
    "daily_catalog_df.write.format(\"delta\").mode(\"overwrite\").save(f\"{base_path}product_catalog\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79227a27",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these labs, you will **master optimized Delta Lake ingestion patterns** and build **scalable, high-performance, and cost-efficient** data pipelines in **Databricks**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
