{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ed9db974",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Optimized Delta Lake Ingestion Patterns**\n",
    "\n",
    "## **Introduction**\n",
    "Delta Lake is a powerful **open-source storage layer** that brings **ACID transactions, schema enforcement, and data reliability** to big data processing. However, achieving **high-performance ingestion** at scale requires **optimized patterns** that enhance **efficiency, reliability, and cost-effectiveness**.\n",
    "\n",
    "This document provides an **exhaustive guide** to **optimized Delta Lake ingestion patterns**, covering **concepts, real-world strategies, and advanced techniques** aligned with **real-world datasets from the provided sample notebooks**.\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Key Challenges in Data Ingestion**\n",
    "\n",
    "Before diving into **optimized ingestion patterns**, it is essential to understand **common challenges** faced during Delta Lake ingestion:\n",
    "\n",
    "- **Slow ingestion performance** due to high file sizes, small file issues, or inefficient partitioning.\n",
    "- **Schema evolution complexity** when new columns or data types are introduced.\n",
    "- **Duplicate and inconsistent data issues** leading to incorrect aggregations.\n",
    "- **Inefficient updates and deletes** affecting downstream analytics.\n",
    "- **Cost and resource inefficiency** caused by unnecessary writes and compactions.\n",
    "- **Lack of real-time ingestion strategies** for streaming-based use cases.\n",
    "- **Storage management issues**, including **high metadata load** and **file fragmentation**.\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Core Principles of Efficient Delta Lake Ingestion**\n",
    "\n",
    "To optimize **ingestion into Delta Lake**, consider the following principles:\n",
    "\n",
    "### **2.1 Efficient Data Write Strategies**\n",
    "- **Minimize Small File Creation:** Use batch writes and enable auto-compaction.\n",
    "- **Efficient Schema Evolution:** Use Delta Lakeâ€™s `mergeSchema` intelligently.\n",
    "- **Choose the Right Write Mode:** Append, Overwrite, or Merge.\n",
    "\n",
    "### **2.2 Data Partitioning & Distribution**\n",
    "- **Partition by High-Cardinality Columns:** Choose columns with a balanced distribution.\n",
    "- **Avoid Over-Partitioning:** Too many partitions can increase metadata overhead.\n",
    "- **Use Z-Ordering for Optimized Reads:** Helps co-locate related data.\n",
    "\n",
    "### **2.3 Optimize Data Layout for Query Performance**\n",
    "- **Cluster data based on query patterns** to reduce scanning costs.\n",
    "- **Enable Auto-Optimized Writes** to dynamically adjust data placement.\n",
    "- **Use Databricks Photon Engine** for faster ingestion processing.\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Best Practices for Optimized Delta Lake Ingestion**\n",
    "\n",
    "### **3.1 Choosing the Right Write Mode**\n",
    "| **Write Mode** | **Use Case** |\n",
    "|--------------|-------------|\n",
    "| **Append Mode** | High-throughput streaming & batch ingestion |\n",
    "| **Overwrite Mode** | Full data refresh in batch processing |\n",
    "| **Merge Mode (Upserts)** | Handling duplicates and updates efficiently |\n",
    "\n",
    "#### **Example: Append Mode for High-Speed Ingestion**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1880c491",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df.write.format(\"delta\").mode(\"append\").save(\"/mnt/delta/events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae27938e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Example: Merge Mode for Upsert Operations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27346295",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE INTO delta_table AS target\n",
    "USING new_data AS source\n",
    "ON target.id = source.id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.value = source.value\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (id, value) VALUES (source.id, source.value);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaae9d2",
   "metadata": {},
   "source": [
    "### **3.2 Partitioning for Performance Optimization**\n",
    "Partitioning improves query speed by reducing scanned data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9219d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df.write.format(\"delta\").partitionBy(\"event_date\").save(\"/mnt/delta/partitioned_events\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93297f4c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **3.3 Handling Small File Problems**\n",
    "- **Enable Auto-Optimized Writes**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af236155",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239839c9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- **Enable Automatic File Compaction**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736ec73a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "%%python\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b58063e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "- **Manually Optimize Data Layout**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61891772",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPTIMIZE delta.`/mnt/delta/events` ZORDER BY (event_type);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18be82e2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **4. Handling Schema Evolution and Data Consistency**\n",
    "\n",
    "### **4.1 Automatic Schema Evolution**\n",
    "Delta Lake supports **automatic schema evolution**, reducing ingestion failures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64bae3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df.write.format(\"delta\").option(\"mergeSchema\", \"true\").mode(\"append\").save(\"/mnt/delta/schema_evolution\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e3e6322",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **4.2 Enforcing Schema Validation**\n",
    "For strict control over schema changes, enforce column types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a341dca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER TABLE delta_table ADD COLUMN new_column STRING;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64860a11",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **5. Incremental Data Processing and Change Data Capture (CDC)**\n",
    "\n",
    "### **5.1 Streaming Data Ingestion with Structured Streaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cabd4f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "df = spark.readStream.format(\"delta\").load(\"/mnt/delta/source\")\n",
    "df.writeStream.format(\"delta\").option(\"checkpointLocation\", \"/mnt/delta/checkpoints\")\\\n",
    "    .start(\"/mnt/delta/processed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a392803",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **5.2 Change Data Capture (CDC) Using Delta Lake**\n",
    "CDC captures only **new and modified rows**, reducing ingestion costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7058dd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM delta_table WHERE _change_type = 'update_postimage';"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc4caff",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **6. Managing Data Retention, Cleanup & Historical Data**\n",
    "\n",
    "### **6.1 Retaining Historical Data Using Time Travel**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941e5338",
   "metadata": {},
   "outputs": [],
   "source": [
    "SELECT * FROM delta.`/mnt/delta/events` VERSION AS OF 10;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcabd667",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **6.2 Configuring Data Retention Policies**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7baf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALTER TABLE delta_table SET TBLPROPERTIES ('delta.logRetentionDuration' = '30 days');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "903378cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **6.3 Deleting Unused Data with VACUUM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa97a1f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "VACUUM delta.`/mnt/delta/events` RETAIN 7 HOURS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f1f477",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **7. Real-World Enterprise Use Cases (With Detailed Code Examples)**\n",
    "\n",
    "### **Use Case 1: Real-Time Log Ingestion for Security Monitoring**\n",
    "A security company **ingests millions of log events** per second and needs **low-latency analytics**.\n",
    "#### **Solution:**\n",
    "- **Append Mode Streaming with Auto-Optimized Writes**.\n",
    "- **Partitioning by log source and event date**.\n",
    "- **Z-Ordering logs for faster query execution**.\n",
    "\n",
    "#### **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6648864f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%python\n",
    "log_df = spark.readStream.format(\"json\").load(\"/mnt/raw/security_logs/\")\n",
    "\n",
    "log_df.writeStream.format(\"delta\")\\\n",
    "    .partitionBy(\"log_source\", \"event_date\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/checkpoints/security_logs/\")\\\n",
    "    .start(\"/mnt/delta/security_logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a90dab",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Use Case 2: Upserting Customer Transactions for Banking**\n",
    "A bank processes **real-time financial transactions** and must ensure **data consistency**.\n",
    "\n",
    "#### **Solution:**\n",
    "- **Merge Mode for efficient upserts**.\n",
    "- **Change Data Capture (CDC) to track modifications**.\n",
    "- **Time Travel for audit compliance**.\n",
    "\n",
    "#### **Implementation:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b932282a",
   "metadata": {},
   "outputs": [],
   "source": [
    "MERGE INTO transactions AS target\n",
    "USING new_transactions AS source\n",
    "ON target.transaction_id = source.transaction_id\n",
    "WHEN MATCHED THEN\n",
    "  UPDATE SET target.amount = source.amount, target.status = source.status\n",
    "WHEN NOT MATCHED THEN\n",
    "  INSERT (transaction_id, amount, status, timestamp) VALUES (source.transaction_id, source.amount, source.status, source.timestamp);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5b1b88e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "Optimized **Delta Lake ingestion** ensures **scalability, performance, and cost-efficiency**. This guide covered **best practices** for **write modes, partitioning, schema evolution, incremental ingestion, and data retention strategies**.\n",
    "\n",
    "By implementing these patterns, enterprises can achieve **high-performance, resilient, and cost-effective data pipelines** in **Databricks Delta Lake**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "sql",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
