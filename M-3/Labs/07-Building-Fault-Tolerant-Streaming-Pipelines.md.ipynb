{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Building Fault-Tolerant Streaming Pipelines - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab document provides **detailed, step-by-step exercises** to help\n",
    "you master **fault tolerance in streaming pipelines** using **Apache\n",
    "Spark Structured Streaming and Delta Lake**. These labs focus on\n",
    "real-world datasets such as **Bank Transactions, Loan Foreclosures, and\n",
    "Flight Data** as referenced in previous sample notebooks.\n",
    "\n",
    "These labs will cover: - **Ensuring recovery using Checkpointing &\n",
    "Write-Ahead Logging (WAL)** - **Guaranteeing exactly-once processing\n",
    "with idempotent writes** - **Handling late-arriving data using\n",
    "watermarks** - **Managing backpressure and autoscaling for high\n",
    "availability** - **Monitoring, debugging, and failure recovery in\n",
    "streaming pipelines**\n",
    "\n",
    "Each lab provides **detailed code examples, execution steps, and\n",
    "validation checks** to ensure practical and scalable solutions.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Implementing Checkpointing and WAL for Fault Tolerance**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Ensure **fault recovery** using **checkpointing and WAL**.\n",
    "-   Resume streaming jobs from the last successful state.\n",
    "\n",
    "### **Step 1: Set Up Streaming Data Source (Bank Transactions)**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FaultToleranceLab1\").getOrCreate()\n",
    "\n",
    "# Load bank transactions dataset\n",
    "bank_data_path = \"/mnt/data/bank_transactions.csv\"\n",
    "bank_schema = \"transaction_id INT, customer_id INT, amount FLOAT, transaction_date STRING\"\n",
    "bank_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(bank_schema).load(bank_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Configure Checkpointing and WAL**\n",
    "\n",
    "``` python\n",
    "query = bank_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - If the job crashes and restarts, it should\n",
    "**resume from the last committed checkpoint**. - **No duplicate\n",
    "records** should be written.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Ensuring Exactly-Once Processing with Idempotent Writes**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Prevent duplicate records when recovering from failures.\n",
    "-   Use **Delta Lakeâ€™s ACID transactions** to maintain consistency.\n",
    "\n",
    "### **Step 1: Define a Streaming Query with Idempotent Writes**\n",
    "\n",
    "``` python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/bank_transactions\")\n",
    "    deltaTable.alias(\"t\").merge(\n",
    "        microBatchDF.alias(\"s\"), \"t.transaction_id = s.transaction_id\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "query = bank_df.writeStream.format(\"delta\")\\\n",
    "    .foreachBatch(upsert_to_delta)\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Duplicate transactions are prevented**. -\n",
    "**Idempotent writes ensure consistent state across restarts**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Handling Late Data with Watermarks**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Process late-arriving transactions while ensuring correctness.\n",
    "-   Prevent outdated records from affecting aggregations.\n",
    "\n",
    "### **Step 1: Define a Watermark for Late Transactions**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "bank_df_with_watermark = bank_df.withWatermark(\"transaction_date\", \"10 minutes\")\n",
    "\n",
    "agg_query = bank_df_with_watermark.groupBy(\n",
    "    window(\"transaction_date\", \"10 minutes\"), \"customer_id\"\n",
    ").agg({\"amount\": \"sum\"})\n",
    "\n",
    "query = agg_query.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_aggregated\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Late transactions arriving **within 10 minutes**\n",
    "of their event-time are processed correctly. - Older transactions are\n",
    "**discarded**, avoiding incorrect computations.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Managing Backpressure & Autoscaling**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Prevent overload using **backpressure handling**.\n",
    "-   Enable **autoscaling** in Spark streaming jobs.\n",
    "\n",
    "### **Step 1: Configure Backpressure Settings**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.kafka.maxRatePerPartition\", \"100\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Autoscaling in Databricks**\n",
    "\n",
    "-   **Go to Databricks Cluster Settings**\n",
    "-   **Enable Autoscaling** and set min/max worker nodes.\n",
    "\n",
    "**Expected Outcome:** - Spark **adjusts micro-batch sizes dynamically**\n",
    "based on system load. - Autoscaling **adds/removes worker nodes**\n",
    "automatically during traffic spikes.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Monitoring & Debugging Streaming Failures**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Implement **real-time monitoring** and **failure recovery\n",
    "    mechanisms**.\n",
    "\n",
    "### **Step 1: Enable Streaming Metrics**\n",
    "\n",
    "``` python\n",
    "query = bank_df.writeStream.format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Logs for Debugging Failures**\n",
    "\n",
    "-   **Check Databricks UI \\> Streaming** for real-time monitoring.\n",
    "-   **View Spark Event Logs** to analyze failures.\n",
    "-   **Enable cluster-level logs** for deeper insights.\n",
    "\n",
    "**Expected Outcome:** - Streaming metrics display **processing rates,\n",
    "backpressure indicators, and failure logs**. - Logs capture **detailed\n",
    "stack traces** for debugging.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Implementing checkpointing and WAL for fault tolerance.** - **Ensuring\n",
    "exactly-once processing with idempotent writes.** - **Handling\n",
    "late-arriving data with watermarks.** - **Managing backpressure and\n",
    "enabling autoscaling for resilience.** - **Monitoring and debugging\n",
    "streaming failures for high availability.**\n",
    "\n",
    "These labs provide **real-world experience** in **building\n",
    "fault-tolerant streaming pipelines**, ensuring that enterprises can\n",
    "process real-time data **with high reliability and consistency**."
   ],
   "id": "6c58a599-26b9-483e-a050-bf343720746b"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
