{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Advanced Window Functions and Watermarking - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab document provides **detailed, step-by-step exercises** to help\n",
    "you master **window functions and watermarking in Apache Spark\n",
    "Structured Streaming**. The labs focus on real-world datasets such as\n",
    "**Bank Transactions, Loan Foreclosures, and Flight Data**, as referenced\n",
    "in previous sample notebooks. These labs ensure **minimal changes for\n",
    "execution** and follow best practices for **state management,\n",
    "efficiency, and correctness**.\n",
    "\n",
    "These labs will cover: - **Implementing Tumbling, Sliding, and Session\n",
    "Windows** - **Using Watermarks to Handle Late-Arriving Data\n",
    "Efficiently** - **Combining Windows and Watermarks for Reliable\n",
    "Aggregations** - **Optimizing Query Performance and State Management** -\n",
    "**Monitoring and Debugging Streaming Windows**\n",
    "\n",
    "Each lab provides **detailed code examples, execution steps, and\n",
    "validation checks** to ensure practical and scalable solutions with\n",
    "**real-world complexity**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Implementing Tumbling Windows for Transaction Aggregation**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Perform transaction count and sum over fixed time windows.\n",
    "-   Ensure state efficiency while handling high-throughput data.\n",
    "\n",
    "### **Step 1: Load Streaming Data (Bank Transactions)**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TumblingWindowLab\").getOrCreate()\n",
    "\n",
    "# Define source path for bank transactions\n",
    "bank_data_path = \"/mnt/data/bank_transactions.csv\"\n",
    "\n",
    "# Define schema for structured streaming\n",
    "bank_schema = \"\"\"\n",
    "    transaction_id INT,\n",
    "    customer_id INT,\n",
    "    amount FLOAT,\n",
    "    transaction_type STRING,\n",
    "    transaction_date TIMESTAMP\n",
    "\"\"\"\n",
    "\n",
    "# Read stream\n",
    "bank_df = spark.readStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(bank_schema)\\\n",
    "    .load(bank_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Tumbling Window Aggregation**\n",
    "\n",
    "``` python\n",
    "tumbling_window_df = bank_df.withWatermark(\"transaction_date\", \"10 minutes\")\\\n",
    "    .groupBy(window(\"transaction_date\", \"10 minutes\"), \"transaction_type\")\\\n",
    "    .agg({\"amount\": \"sum\", \"transaction_id\": \"count\"})\\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_amount\")\\\n",
    "    .withColumnRenamed(\"count(transaction_id)\", \"transaction_count\")\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Delta Lake**\n",
    "\n",
    "``` python\n",
    "query = tumbling_window_df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_transactions_windowed\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Transactions are grouped into **non-overlapping\n",
    "10-minute intervals**. - Each interval outputs **transaction count and\n",
    "total amount by transaction type.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Implementing Sliding Windows for Flight Delay Monitoring**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Calculate rolling flight delay averages using sliding windows.\n",
    "-   Capture overlapping time windows for real-time analytics.\n",
    "\n",
    "### **Step 1: Load Streaming Data (Flight Delays)**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "flight_data_path = \"/mnt/data/flights.parquet\"\n",
    "flight_schema = \"flight_id INT, airline STRING, delay INT, event_time TIMESTAMP\"\n",
    "\n",
    "flight_df = spark.readStream.format(\"parquet\").schema(flight_schema).load(flight_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Sliding Window Aggregation**\n",
    "\n",
    "``` python\n",
    "sliding_window_df = flight_df.withWatermark(\"event_time\", \"15 minutes\")\\\n",
    "    .groupBy(window(\"event_time\", \"15 minutes\", \"5 minutes\"), \"airline\")\\\n",
    "    .agg(avg(\"delay\").alias(\"average_delay\"))\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Console**\n",
    "\n",
    "``` python\n",
    "query = sliding_window_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Flight delays are averaged over **overlapping\n",
    "15-minute windows sliding every 5 minutes**. - The same flight may\n",
    "appear in multiple overlapping windows.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Using Session Windows for E-Commerce User Sessionization**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Dynamically track user sessions based on inactivity timeout.\n",
    "-   Optimize memory utilization by discarding stale session states.\n",
    "\n",
    "### **Step 1: Load Streaming Data (User Activity Logs)**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import session_window\n",
    "\n",
    "user_data_path = \"/mnt/data/user_activity.json\"\n",
    "user_schema = \"user_id STRING, action STRING, event_time TIMESTAMP\"\n",
    "\n",
    "user_df = spark.readStream.format(\"json\").schema(user_schema).load(user_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Session Window Aggregation**\n",
    "\n",
    "``` python\n",
    "session_window_df = user_df.withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "    .groupBy(session_window(\"event_time\", \"30 minutes\"), \"user_id\")\\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Delta Lake**\n",
    "\n",
    "``` python\n",
    "query = session_window_df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/user_session_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/user_sessions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - User sessions are **dynamically grouped based on\n",
    "30-minute inactivity.** - Sessionized data is stored efficiently using\n",
    "**Delta Lake for further analysis.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Using Tumbling, Sliding, and Session Windows for real-time\n",
    "aggregations**. - **Applying watermarks to manage late-arriving data\n",
    "efficiently**. - **Combining windows and watermarks for optimal\n",
    "streaming performance**. - **Ensuring state efficiency and data accuracy\n",
    "in real-time event processing**. - **Writing streaming output to Delta\n",
    "Lake for persistence and analytics**.\n",
    "\n",
    "These labs provide **real-world experience** in **window functions and\n",
    "watermarking**, ensuring scalable and reliable **structured streaming\n",
    "pipelines** that handle **high-volume, real-time data efficiently**."
   ],
   "id": "74814b7f-4aa7-4ba5-aa4e-6cbf06157096"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
