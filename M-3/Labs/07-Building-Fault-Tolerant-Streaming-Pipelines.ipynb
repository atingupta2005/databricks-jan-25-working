{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91716a2b",
   "metadata": {},
   "source": [
    "# **Building Fault-Tolerant Streaming Pipelines - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides **detailed, step-by-step exercises** to help you master **fault tolerance in streaming pipelines** using **Apache Spark Structured Streaming and Delta Lake**. These labs focus on real-world datasets such as **Bank Transactions, Loan Foreclosures, and Flight Data** as referenced in previous sample notebooks.\n",
    "\n",
    "These labs will cover:\n",
    "- **Ensuring recovery using Checkpointing & Write-Ahead Logging (WAL)**\n",
    "- **Guaranteeing exactly-once processing with idempotent writes**\n",
    "- **Handling late-arriving data using watermarks**\n",
    "- **Managing backpressure and autoscaling for high availability**\n",
    "- **Monitoring, debugging, and failure recovery in streaming pipelines**\n",
    "\n",
    "Each lab provides **detailed code examples, execution steps, and validation checks** to ensure practical and scalable solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Implementing Checkpointing and WAL for Fault Tolerance**\n",
    "### **Objective:**\n",
    "- Ensure **fault recovery** using **checkpointing and WAL**.\n",
    "- Resume streaming jobs from the last successful state.\n",
    "\n",
    "### **Step 1: Set Up Streaming Data Source (Bank Transactions)**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"FaultToleranceLab1\").getOrCreate()\n",
    "\n",
    "# Load bank transactions dataset\n",
    "bank_data_path = \"/mnt/data/bank_transactions.csv\"\n",
    "bank_schema = \"transaction_id INT, customer_id INT, amount FLOAT, transaction_date STRING\"\n",
    "bank_df = spark.readStream.format(\"csv\").option(\"header\", \"true\").schema(bank_schema).load(bank_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Configure Checkpointing and WAL**\n",
    "```python\n",
    "query = bank_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- If the job crashes and restarts, it should **resume from the last committed checkpoint**.\n",
    "- **No duplicate records** should be written.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Ensuring Exactly-Once Processing with Idempotent Writes**\n",
    "### **Objective:**\n",
    "- Prevent duplicate records when recovering from failures.\n",
    "- Use **Delta Lake's ACID transactions** to maintain consistency.\n",
    "\n",
    "### **Step 1: Define a Streaming Query with Idempotent Writes**\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "def upsert_to_delta(microBatchDF, batchId):\n",
    "    deltaTable = DeltaTable.forPath(spark, \"/mnt/delta/bank_transactions\")\n",
    "    deltaTable.alias(\"t\").merge(\n",
    "        microBatchDF.alias(\"s\"), \"t.transaction_id = s.transaction_id\"\n",
    "    ).whenMatchedUpdateAll().whenNotMatchedInsertAll().execute()\n",
    "\n",
    "query = bank_df.writeStream.format(\"delta\")\\\n",
    "    .foreachBatch(upsert_to_delta)\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Duplicate transactions are prevented**.\n",
    "- **Idempotent writes ensure consistent state across restarts**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Handling Late Data with Watermarks**\n",
    "### **Objective:**\n",
    "- Process late-arriving transactions while ensuring correctness.\n",
    "- Prevent outdated records from affecting aggregations.\n",
    "\n",
    "### **Step 1: Define a Watermark for Late Transactions**\n",
    "```python\n",
    "from pyspark.sql.functions import window\n",
    "\n",
    "bank_df_with_watermark = bank_df.withWatermark(\"transaction_date\", \"10 minutes\")\n",
    "\n",
    "agg_query = bank_df_with_watermark.groupBy(\n",
    "    window(\"transaction_date\", \"10 minutes\"), \"customer_id\"\n",
    ").agg({\"amount\": \"sum\"})\n",
    "\n",
    "query = agg_query.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_aggregated\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Late transactions arriving **within 10 minutes** of their event-time are processed correctly.\n",
    "- Older transactions are **discarded**, avoiding incorrect computations.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Managing Backpressure & Autoscaling**\n",
    "### **Objective:**\n",
    "- Prevent overload using **backpressure handling**.\n",
    "- Enable **autoscaling** in Spark streaming jobs.\n",
    "\n",
    "### **Step 1: Configure Backpressure Settings**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "spark.conf.set(\"spark.streaming.backpressure.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.streaming.kafka.maxRatePerPartition\", \"100\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Autoscaling in Databricks**\n",
    "- **Go to Databricks Cluster Settings**\n",
    "- **Enable Autoscaling** and set min/max worker nodes.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Spark **adjusts micro-batch sizes dynamically** based on system load.\n",
    "- Autoscaling **adds/removes worker nodes** automatically during traffic spikes.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Monitoring & Debugging Streaming Failures**\n",
    "### **Objective:**\n",
    "- Implement **real-time monitoring** and **failure recovery mechanisms**.\n",
    "\n",
    "### **Step 1: Enable Streaming Metrics**\n",
    "```python\n",
    "query = bank_df.writeStream.format(\"console\")\\\n",
    "    .trigger(processingTime=\"10 seconds\")\\\n",
    "    .option(\"truncate\", \"false\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Logs for Debugging Failures**\n",
    "- **Check Databricks UI > Streaming** for real-time monitoring.\n",
    "- **View Spark Event Logs** to analyze failures.\n",
    "- **Enable cluster-level logs** for deeper insights.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Streaming metrics display **processing rates, backpressure indicators, and failure logs**.\n",
    "- Logs capture **detailed stack traces** for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Implementing checkpointing and WAL for fault tolerance.**\n",
    "- **Ensuring exactly-once processing with idempotent writes.**\n",
    "- **Handling late-arriving data with watermarks.**\n",
    "- **Managing backpressure and enabling autoscaling for resilience.**\n",
    "- **Monitoring and debugging streaming failures for high availability.**\n",
    "\n",
    "These labs provide **real-world experience** in **building fault-tolerant streaming pipelines**, ensuring that enterprises can process real-time data **with high reliability and consistency**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}