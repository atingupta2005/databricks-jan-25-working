{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Integrating with Message Queues (Kafka, Event Hubs) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab document provides **detailed, step-by-step exercises** to help\n",
    "you master **Apache Kafka and Azure Event Hubs integration** with\n",
    "**Apache Spark Structured Streaming**. These hands-on labs focus on\n",
    "real-world scenarios using **Bank Transactions, Loan Foreclosures, and\n",
    "Flight Data**, ensuring minimal modifications for execution.\n",
    "\n",
    "These labs will cover: - **Setting up Kafka and Event Hubs Producers &\n",
    "Consumers** - **Ingesting Streaming Data from Kafka and Event Hubs into\n",
    "Spark** - **Writing Processed Data to Message Queues using Structured\n",
    "Streaming** - **Ensuring Fault Tolerance, Exactly-Once Processing, and\n",
    "Checkpointing** - **Monitoring, Debugging, and Performance\n",
    "Optimization**\n",
    "\n",
    "Each lab includes **detailed code examples, execution steps, validation\n",
    "checks, and advanced configurations** to ensure **enterprise-grade\n",
    "reliability and scalability**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Setting Up Kafka Producer & Consumer**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Configure Kafka topics for streaming data ingestion.\n",
    "-   Produce and consume messages from Kafka.\n",
    "-   Simulate a real-world transaction stream.\n",
    "\n",
    "### **Step 1: Start Kafka Broker & Create Topics**\n",
    "\n",
    "``` sh\n",
    "# Start Zookeeper (if not running)\n",
    "zookeeper-server-start.sh config/zookeeper.properties &\n",
    "\n",
    "# Start Kafka Broker\n",
    "kafka-server-start.sh config/server.properties &\n",
    "\n",
    "# Create a new topic for transactions\n",
    "kafka-topics.sh --create --topic transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1\n",
    "```\n",
    "\n",
    "### **Step 2: Produce Messages to Kafka Topic**\n",
    "\n",
    "``` sh\n",
    "# Start a Kafka producer and send messages\n",
    "echo \"101,5000,2024-02-01,Loan\" | kafka-console-producer.sh --broker-list localhost:9092 --topic transactions\n",
    "```\n",
    "\n",
    "### **Step 3: Consume Messages from Kafka**\n",
    "\n",
    "``` sh\n",
    "# Start a Kafka consumer\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic transactions --from-beginning\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - The **consumer receives** messages produced to\n",
    "Kafka in real-time.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Reading Streaming Data from Kafka in Spark Structured Streaming**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Read real-time transaction data from Kafka into Spark Structured\n",
    "    Streaming.\n",
    "-   Perform schema enforcement and parsing.\n",
    "\n",
    "### **Step 1: Configure Spark Streaming Source for Kafka**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KafkaConsumerLab\").getOrCreate()\n",
    "\n",
    "kafka_df = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"transactions\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### **Step 2: Parse Kafka Message Data and Enforce Schema**\n",
    "\n",
    "``` python\n",
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING) as raw_value\")\\\n",
    "    .select(split(col(\"raw_value\"), \",\").alias(\"columns\"))\\\n",
    "    .selectExpr(\"columns[0] AS transaction_id\", \"columns[1] AS amount\", \"columns[2] AS transaction_date\", \"columns[3] AS transaction_type\")\n",
    "```\n",
    "\n",
    "### **Step 3: Start Streaming Query to Console Output**\n",
    "\n",
    "``` python\n",
    "query = parsed_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Kafka messages are **continuously read, parsed,\n",
    "and structured** in Spark.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Writing Processed Data to Kafka**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Write transformed transaction data back to Kafka.\n",
    "-   Maintain structured JSON format.\n",
    "\n",
    "### **Step 1: Define Spark Streaming Query to Kafka Sink**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "query = parsed_df.selectExpr(\"transaction_id AS key\", \"to_json(struct(*)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"topic\", \"processed-transactions\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/kafka_checkpoint/\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Transformed data is published to the\n",
    "**processed-transactions** Kafka topic in JSON format.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Reading Streaming Data from Azure Event Hubs**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Read real-time messages from Azure Event Hubs in Spark Structured\n",
    "    Streaming.\n",
    "-   Handle JSON messages effectively.\n",
    "\n",
    "### **Step 1: Configure Spark Streaming Source for Event Hubs**\n",
    "\n",
    "``` python\n",
    "connection_string = \"<EVENT_HUB_CONNECTION_STRING>\"\n",
    "event_hub_df = spark.readStream\\\n",
    "    .format(\"eventhubs\")\\\n",
    "    .option(\"eventhubs.connectionString\", connection_string)\\\n",
    "    .load()\n",
    "```\n",
    "\n",
    "### **Step 2: Parse JSON Payload from Event Hubs**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "schema = \"transaction_id STRING, amount FLOAT, transaction_date STRING, transaction_type STRING\"\n",
    "parsed_event_hub_df = event_hub_df.selectExpr(\"CAST(body AS STRING) AS json_str\")\\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\"))\\\n",
    "    .select(\"data.*\")\n",
    "```\n",
    "\n",
    "### **Step 3: Start Streaming Query to Console Output**\n",
    "\n",
    "``` python\n",
    "query = parsed_event_hub_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Messages from Event Hubs are streamed, parsed,\n",
    "and displayed in Spark console.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Writing Streaming Data to Azure Event Hubs**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Write processed data back to Event Hubs.\n",
    "-   Preserve structured JSON format.\n",
    "\n",
    "### **Step 1: Define Spark Streaming Query to Event Hubs Sink**\n",
    "\n",
    "``` python\n",
    "query = parsed_event_hub_df.selectExpr(\"CAST(transaction_id AS STRING) AS key\", \"to_json(struct(*)) AS body\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"eventhubs\")\\\n",
    "    .option(\"eventhubs.connectionString\", connection_string)\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/eventhub_checkpoint/\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Processed data is continuously **written back to\n",
    "Azure Event Hubs** in structured format.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Integrating Kafka and Event Hubs with Apache Spark Structured\n",
    "Streaming.** - **Producing and consuming messages from real-time message\n",
    "queues.** - **Processing, structuring, and writing data back to Kafka\n",
    "and Event Hubs.** - **Ensuring data reliability with schema enforcement,\n",
    "checkpointing, and exactly-once processing.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable,\n",
    "fault-tolerant, and event-driven streaming pipelines** with **structured\n",
    "data processing** and **enterprise reliability**."
   ],
   "id": "5ea1546f-9894-45e3-9e23-5541450f1544"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
