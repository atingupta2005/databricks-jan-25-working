{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a4218ac",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Integrating with Message Queues (Kafka, Event Hubs) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides **detailed, step-by-step exercises** to help you master **Apache Kafka and Azure Event Hubs integration** with **Apache Spark Structured Streaming**. These hands-on labs focus on real-world scenarios using **Bank Transactions, Loan Foreclosures, and Flight Data**, ensuring minimal modifications for execution.\n",
    "\n",
    "These labs will cover:\n",
    "- **Setting up Kafka and Event Hubs Producers & Consumers**\n",
    "- **Ingesting Streaming Data from Kafka and Event Hubs into Spark**\n",
    "- **Writing Processed Data to Message Queues using Structured Streaming**\n",
    "- **Ensuring Fault Tolerance, Exactly-Once Processing, and Checkpointing**\n",
    "- **Monitoring, Debugging, and Performance Optimization**\n",
    "\n",
    "Each lab includes **detailed code examples, execution steps, validation checks, and advanced configurations** to ensure **enterprise-grade reliability and scalability**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Setting Up Kafka Producer & Consumer**\n",
    "### **Objective:**\n",
    "- Configure Kafka topics for streaming data ingestion.\n",
    "- Produce and consume messages from Kafka.\n",
    "- Simulate a real-world transaction stream.\n",
    "\n",
    "### **Step 1: Start Kafka Broker & Create Topics**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066c8f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Start Zookeeper (if not running)\n",
    "zookeeper-server-start.sh config/zookeeper.properties &\n",
    "\n",
    "# Start Kafka Broker\n",
    "kafka-server-start.sh config/server.properties &\n",
    "\n",
    "# Create a new topic for transactions\n",
    "kafka-topics.sh --create --topic transactions --bootstrap-server localhost:9092 --partitions 3 --replication-factor 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38cb1cb9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Produce Messages to Kafka Topic**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "464a58f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Start a Kafka producer and send messages\n",
    "echo \"101,5000,2024-02-01,Loan\" | kafka-console-producer.sh --broker-list localhost:9092 --topic transactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c81b7e85",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Consume Messages from Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5ecbdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "# Start a Kafka consumer\n",
    "kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic transactions --from-beginning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b04dc78",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- The **consumer receives** messages produced to Kafka in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Reading Streaming Data from Kafka in Spark Structured Streaming**\n",
    "### **Objective:**\n",
    "- Read real-time transaction data from Kafka into Spark Structured Streaming.\n",
    "- Perform schema enforcement and parsing.\n",
    "\n",
    "### **Step 1: Configure Spark Streaming Source for Kafka**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8337851c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, split\n",
    "\n",
    "spark = SparkSession.builder.appName(\"KafkaConsumerLab\").getOrCreate()\n",
    "\n",
    "kafka_df = spark.readStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"transactions\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bd53b1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Parse Kafka Message Data and Enforce Schema**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3257bd85",
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed_df = kafka_df.selectExpr(\"CAST(value AS STRING) as raw_value\")\\\n",
    "    .select(split(col(\"raw_value\"), \",\").alias(\"columns\"))\\\n",
    "    .selectExpr(\"columns[0] AS transaction_id\", \"columns[1] AS amount\", \"columns[2] AS transaction_date\", \"columns[3] AS transaction_type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b805ff15",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Start Streaming Query to Console Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d1d7f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae407eb7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Kafka messages are **continuously read, parsed, and structured** in Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Writing Processed Data to Kafka**\n",
    "### **Objective:**\n",
    "- Write transformed transaction data back to Kafka.\n",
    "- Maintain structured JSON format.\n",
    "\n",
    "### **Step 1: Define Spark Streaming Query to Kafka Sink**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7245f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_json, struct\n",
    "\n",
    "query = parsed_df.selectExpr(\"transaction_id AS key\", \"to_json(struct(*)) AS value\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"topic\", \"processed-transactions\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/kafka_checkpoint/\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2428b502",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Transformed data is published to the **processed-transactions** Kafka topic in JSON format.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Reading Streaming Data from Azure Event Hubs**\n",
    "### **Objective:**\n",
    "- Read real-time messages from Azure Event Hubs in Spark Structured Streaming.\n",
    "- Handle JSON messages effectively.\n",
    "\n",
    "### **Step 1: Configure Spark Streaming Source for Event Hubs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b5587d",
   "metadata": {},
   "outputs": [],
   "source": [
    "connection_string = \"<EVENT_HUB_CONNECTION_STRING>\"\n",
    "event_hub_df = spark.readStream\\\n",
    "    .format(\"eventhubs\")\\\n",
    "    .option(\"eventhubs.connectionString\", connection_string)\\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccaf4a41",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Parse JSON Payload from Event Hubs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03f5e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import from_json\n",
    "\n",
    "schema = \"transaction_id STRING, amount FLOAT, transaction_date STRING, transaction_type STRING\"\n",
    "parsed_event_hub_df = event_hub_df.selectExpr(\"CAST(body AS STRING) AS json_str\")\\\n",
    "    .select(from_json(col(\"json_str\"), schema).alias(\"data\"))\\\n",
    "    .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4baa5b3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Start Streaming Query to Console Output**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3dc032",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_event_hub_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e80073",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Messages from Event Hubs are streamed, parsed, and displayed in Spark console.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Writing Streaming Data to Azure Event Hubs**\n",
    "### **Objective:**\n",
    "- Write processed data back to Event Hubs.\n",
    "- Preserve structured JSON format.\n",
    "\n",
    "### **Step 1: Define Spark Streaming Query to Event Hubs Sink**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3311583",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = parsed_event_hub_df.selectExpr(\"CAST(transaction_id AS STRING) AS key\", \"to_json(struct(*)) AS body\")\\\n",
    "    .writeStream\\\n",
    "    .format(\"eventhubs\")\\\n",
    "    .option(\"eventhubs.connectionString\", connection_string)\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/eventhub_checkpoint/\")\\\n",
    "    .start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c2d79a0",
   "metadata": {},
   "source": [
    "**Expected Outcome:**\n",
    "- Processed data is continuously **written back to Azure Event Hubs** in structured format.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Integrating Kafka and Event Hubs with Apache Spark Structured Streaming.**\n",
    "- **Producing and consuming messages from real-time message queues.**\n",
    "- **Processing, structuring, and writing data back to Kafka and Event Hubs.**\n",
    "- **Ensuring data reliability with schema enforcement, checkpointing, and exactly-once processing.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable, fault-tolerant, and event-driven streaming pipelines** with **structured data processing** and **enterprise reliability**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
