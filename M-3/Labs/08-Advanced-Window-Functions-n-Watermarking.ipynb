{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ddc500d",
   "metadata": {},
   "source": [
    "# **Advanced Window Functions and Watermarking - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides **detailed, step-by-step exercises** to help you master **window functions and watermarking in Apache Spark Structured Streaming**. The labs focus on real-world datasets such as **Bank Transactions, Loan Foreclosures, and Flight Data**, as referenced in previous sample notebooks. These labs ensure **minimal changes for execution** and follow best practices for **state management, efficiency, and correctness**.\n",
    "\n",
    "These labs will cover:\n",
    "- **Implementing Tumbling, Sliding, and Session Windows**\n",
    "- **Using Watermarks to Handle Late-Arriving Data Efficiently**\n",
    "- **Combining Windows and Watermarks for Reliable Aggregations**\n",
    "- **Optimizing Query Performance and State Management**\n",
    "- **Monitoring and Debugging Streaming Windows**\n",
    "\n",
    "Each lab provides **detailed code examples, execution steps, and validation checks** to ensure practical and scalable solutions with **real-world complexity**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Implementing Tumbling Windows for Transaction Aggregation**\n",
    "### **Objective:**\n",
    "- Perform transaction count and sum over fixed time windows.\n",
    "- Ensure state efficiency while handling high-throughput data.\n",
    "\n",
    "### **Step 1: Load Streaming Data (Bank Transactions)**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TumblingWindowLab\").getOrCreate()\n",
    "\n",
    "# Define source path for bank transactions\n",
    "bank_data_path = \"/mnt/data/bank_transactions.csv\"\n",
    "\n",
    "# Define schema for structured streaming\n",
    "bank_schema = \"\"\"\n",
    "    transaction_id INT,\n",
    "    customer_id INT,\n",
    "    amount FLOAT,\n",
    "    transaction_type STRING,\n",
    "    transaction_date TIMESTAMP\n",
    "\"\"\"\n",
    "\n",
    "# Read stream\n",
    "bank_df = spark.readStream\\\n",
    "    .format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .schema(bank_schema)\\\n",
    "    .load(bank_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Tumbling Window Aggregation**\n",
    "```python\n",
    "tumbling_window_df = bank_df.withWatermark(\"transaction_date\", \"10 minutes\")\\\n",
    "    .groupBy(window(\"transaction_date\", \"10 minutes\"), \"transaction_type\")\\\n",
    "    .agg({\"amount\": \"sum\", \"transaction_id\": \"count\"})\\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_amount\")\\\n",
    "    .withColumnRenamed(\"count(transaction_id)\", \"transaction_count\")\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Delta Lake**\n",
    "```python\n",
    "query = tumbling_window_df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bank_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/bank_transactions_windowed\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Transactions are grouped into **non-overlapping 10-minute intervals**.\n",
    "- Each interval outputs **transaction count and total amount by transaction type.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Implementing Sliding Windows for Flight Delay Monitoring**\n",
    "### **Objective:**\n",
    "- Calculate rolling flight delay averages using sliding windows.\n",
    "- Capture overlapping time windows for real-time analytics.\n",
    "\n",
    "### **Step 1: Load Streaming Data (Flight Delays)**\n",
    "```python\n",
    "from pyspark.sql.functions import avg\n",
    "\n",
    "flight_data_path = \"/mnt/data/flights.parquet\"\n",
    "flight_schema = \"flight_id INT, airline STRING, delay INT, event_time TIMESTAMP\"\n",
    "\n",
    "flight_df = spark.readStream.format(\"parquet\").schema(flight_schema).load(flight_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Sliding Window Aggregation**\n",
    "```python\n",
    "sliding_window_df = flight_df.withWatermark(\"event_time\", \"15 minutes\")\\\n",
    "    .groupBy(window(\"event_time\", \"15 minutes\", \"5 minutes\"), \"airline\")\\\n",
    "    .agg(avg(\"delay\").alias(\"average_delay\"))\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Console**\n",
    "```python\n",
    "query = sliding_window_df.writeStream\\\n",
    "    .format(\"console\")\\\n",
    "    .outputMode(\"update\")\\\n",
    "    .start()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Flight delays are averaged over **overlapping 15-minute windows sliding every 5 minutes**.\n",
    "- The same flight may appear in multiple overlapping windows.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Using Session Windows for E-Commerce User Sessionization**\n",
    "### **Objective:**\n",
    "- Dynamically track user sessions based on inactivity timeout.\n",
    "- Optimize memory utilization by discarding stale session states.\n",
    "\n",
    "### **Step 1: Load Streaming Data (User Activity Logs)**\n",
    "```python\n",
    "from pyspark.sql.functions import session_window\n",
    "\n",
    "user_data_path = \"/mnt/data/user_activity.json\"\n",
    "user_schema = \"user_id STRING, action STRING, event_time TIMESTAMP\"\n",
    "\n",
    "user_df = spark.readStream.format(\"json\").schema(user_schema).load(user_data_path)\n",
    "```\n",
    "\n",
    "### **Step 2: Apply Session Window Aggregation**\n",
    "```python\n",
    "session_window_df = user_df.withWatermark(\"event_time\", \"30 minutes\")\\\n",
    "    .groupBy(session_window(\"event_time\", \"30 minutes\"), \"user_id\")\\\n",
    "    .count()\n",
    "```\n",
    "\n",
    "### **Step 3: Write Streaming Output to Delta Lake**\n",
    "```python\n",
    "query = session_window_df.writeStream\\\n",
    "    .format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/user_session_checkpoint\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .start(\"/mnt/delta/user_sessions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- User sessions are **dynamically grouped based on 30-minute inactivity.**\n",
    "- Sessionized data is stored efficiently using **Delta Lake for further analysis.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Using Tumbling, Sliding, and Session Windows for real-time aggregations**.\n",
    "- **Applying watermarks to manage late-arriving data efficiently**.\n",
    "- **Combining windows and watermarks for optimal streaming performance**.\n",
    "- **Ensuring state efficiency and data accuracy in real-time event processing**.\n",
    "- **Writing streaming output to Delta Lake for persistence and analytics**.\n",
    "\n",
    "These labs provide **real-world experience** in **window functions and watermarking**, ensuring scalable and reliable **structured streaming pipelines** that handle **high-volume, real-time data efficiently**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}