{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Implementing Delta Lake in Multi-Hop Architectures - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab document provides a **detailed, step-by-step guide** to\n",
    "implementing a **Multi-Hop Architecture (Bronze, Silver, Gold layers)**\n",
    "using **Delta Lake**. These hands-on exercises focus on **real-world\n",
    "scenarios** using datasets such as **Bank Transactions, Loan\n",
    "Foreclosures, and Flight Data** to ensure practical learning with\n",
    "minimal modifications.\n",
    "\n",
    "These labs cover: - **Setting up Delta Lake multi-hop pipelines** -\n",
    "**Ingesting data into the Bronze layer from Kafka and cloud storage** -\n",
    "**Transforming data in the Silver layer (Cleansing, Deduplication, and\n",
    "CDC)** - **Aggregating and curating data in the Gold layer** -\n",
    "**Optimizing performance using Z-Ordering, Compaction, and Schema\n",
    "Enforcement** - **Enabling Change Data Capture (CDC) and Time Travel**\n",
    "\n",
    "Each lab provides **detailed code, execution steps, and validation\n",
    "techniques** to build **scalable, enterprise-ready data pipelines**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Ingesting Data into the Bronze Layer**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Load raw, unprocessed data into the **Bronze layer** from streaming\n",
    "    and batch sources.\n",
    "\n",
    "### **Step 1: Load Streaming Data from Kafka into the Bronze Layer**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BronzeLayerIngestion\").getOrCreate()\n",
    "\n",
    "bronze_df = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"transactions\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\n",
    "\n",
    "bronze_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bronze_checkpoint\")\\\n",
    "    .start(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Load Batch Data from Cloud Storage**\n",
    "\n",
    "``` python\n",
    "bronze_batch_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"s3://my-bucket/raw-data/\")\n",
    "\n",
    "bronze_batch_df.write.format(\"delta\").mode(\"append\").save(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Raw event-driven data is ingested into Delta\n",
    "Lake in append-only format.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Processing and Cleaning Data in the Silver Layer**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Deduplicate, clean, and transform the data **incrementally**.\n",
    "\n",
    "### **Step 1: Load Bronze Data into Silver Layer**\n",
    "\n",
    "``` python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "bronze_table = DeltaTable.forPath(spark, \"/mnt/delta/bronze_transactions\")\n",
    "\n",
    "silver_df = bronze_table.toDF().dropDuplicates([\"transaction_id\"]).filter(\"amount IS NOT NULL\")\n",
    "\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/silver_transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Change Data Capture (CDC) for Incremental Processing**\n",
    "\n",
    "``` python\n",
    "cdc_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"readChangeFeed\", \"true\")\\\n",
    "    .table(\"silver_transactions\")\n",
    "\n",
    "query = cdc_df.writeStream.format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Silver layer contains cleaned, deduplicated\n",
    "data ready for business logic transformations.** - **Incremental changes\n",
    "are continuously processed using CDC.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Aggregating and Curating Data in the Gold Layer**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Transform and **aggregate** cleansed data into business-ready\n",
    "    tables.\n",
    "\n",
    "### **Step 1: Perform Aggregations and Store Data in the Gold Layer**\n",
    "\n",
    "``` python\n",
    "gold_df = spark.read.format(\"delta\").load(\"/mnt/delta/silver_transactions\")\\\n",
    "    .groupBy(\"customer_id\", \"transaction_date\")\\\n",
    "    .agg({\"amount\": \"sum\"})\\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_spent\")\n",
    "\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/gold_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Gold layer contains structured, aggregated\n",
    "data optimized for reporting and machine learning.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Enabling Time Travel for Data Versioning**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Query and restore previous data versions using **Time Travel**.\n",
    "\n",
    "### **Step 1: Query a Previous Version of the Silver Layer**\n",
    "\n",
    "``` sql\n",
    "SELECT * FROM silver_transactions VERSION AS OF 5;\n",
    "```\n",
    "\n",
    "### **Step 2: Restore a Table to a Previous Version**\n",
    "\n",
    "``` python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/silver_transactions\")\n",
    "delta_table.restoreToVersion(3)\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Historical data versions are accessible for\n",
    "compliance, auditing, and rollback.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Optimizing Performance with Compaction and Z-Ordering**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Improve **query performance** and **reduce storage costs**.\n",
    "\n",
    "### **Step 1: Compact Small Files Using OPTIMIZE**\n",
    "\n",
    "``` sql\n",
    "OPTIMIZE gold_transactions;\n",
    "```\n",
    "\n",
    "### **Step 2: Improve Query Performance Using Z-Ordering**\n",
    "\n",
    "``` sql\n",
    "OPTIMIZE gold_transactions ZORDER BY customer_id;\n",
    "```\n",
    "\n",
    "### **Step 3: Clean Up Old Data Versions Using VACUUM**\n",
    "\n",
    "``` sql\n",
    "VACUUM silver_transactions RETAIN 24 HOURS;\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Improved query efficiency and reduced storage\n",
    "fragmentation.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 6: Automating Data Ingestion Using Auto Loader**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Automate continuous ingestion from cloud storage.\n",
    "\n",
    "### **Step 1: Configure Auto Loader to Monitor a Cloud Storage Path**\n",
    "\n",
    "``` python\n",
    "raw_df = spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/delta/schema\")\\\n",
    "    .load(\"s3://my-bucket/raw-data/\")\n",
    "\n",
    "raw_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/autoloader_checkpoint\")\\\n",
    "    .start(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **New files are automatically detected and\n",
    "processed.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Building end-to-end Delta Lake pipelines using a multi-hop\n",
    "architecture.** - **Processing raw data incrementally using CDC and\n",
    "schema enforcement.** - **Optimizing Delta Lake tables for performance\n",
    "and cost efficiency.** - **Automating data ingestion with Auto\n",
    "Loader.** - **Ensuring data reliability with ACID transactions, time\n",
    "travel, and Z-Ordering.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable,\n",
    "fault-tolerant, and high-performance multi-hop architectures** that\n",
    "support **enterprise data lakes, analytics, and AI workloads**."
   ],
   "id": "b6e4c8bd-beab-4973-936c-d7ae17622fd2"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
