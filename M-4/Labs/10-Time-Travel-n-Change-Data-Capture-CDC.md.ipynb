{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Time Travel and Change Data Capture (CDC) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This lab document provides a **detailed, step-by-step guide** to\n",
    "implementing **Time Travel and Change Data Capture (CDC)** using\n",
    "**Apache Spark and Delta Lake**. These hands-on exercises will focus on\n",
    "**real-world scenarios**, utilizing datasets such as **Bank\n",
    "Transactions, Loan Foreclosures, and Flight Data** to ensure practical\n",
    "learning with minimal modifications.\n",
    "\n",
    "These labs cover: - **Implementing Time Travel to access historical data\n",
    "versions** - **Restoring previous data states using Delta Lake** -\n",
    "**Tracking and processing changes using CDC techniques** - **Using\n",
    "Structured Streaming for real-time CDC pipelines** - **Ensuring fault\n",
    "tolerance, exactly-once processing, and optimization**\n",
    "\n",
    "Each lab provides **detailed code, execution steps, and validation\n",
    "techniques** to build **scalable, enterprise-ready data pipelines**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Implementing Time Travel in Delta Lake**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Enable **data versioning** and retrieve historical records.\n",
    "-   Restore data to a previous state using **Time Travel**.\n",
    "\n",
    "### **Step 1: Create a Delta Table with Sample Data**\n",
    "\n",
    "``` python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TimeTravelLab\").getOrCreate()\n",
    "\n",
    "# Define sample transactions data\n",
    "data = [(101, 5000, \"2024-02-01\"), (102, 7000, \"2024-02-02\"), (103, 4500, \"2024-02-03\")]\n",
    "columns = [\"transaction_id\", \"amount\", \"transaction_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Perform Updates and Check Versions**\n",
    "\n",
    "``` python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/transactions\")\n",
    "\n",
    "# Update a record\n",
    "delta_table.update(\"transaction_id = 101\", {\"amount\": \"6000\"})\n",
    "\n",
    "# Show history\n",
    "delta_table.history().show()\n",
    "```\n",
    "\n",
    "### **Step 3: Query a Previous Version**\n",
    "\n",
    "``` python\n",
    "previous_version = 0  # Adjust based on the output of history()\n",
    "delta_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(\"/mnt/delta/transactions\")\n",
    "delta_df.show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - The previous transaction state should be\n",
    "retrieved using **Time Travel**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Restoring Data to a Previous Version**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Restore a table to a **specific historical version**.\n",
    "\n",
    "### **Step 1: Restore Table to a Previous Version**\n",
    "\n",
    "``` python\n",
    "restore_version = 1  # Choose an appropriate version from history()\n",
    "delta_table.restoreToVersion(restore_version)\n",
    "```\n",
    "\n",
    "### **Step 2: Verify the Restoration**\n",
    "\n",
    "``` python\n",
    "spark.read.format(\"delta\").load(\"/mnt/delta/transactions\").show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - The table should **roll back** to its previous\n",
    "state.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Implementing Change Data Capture (CDC) Using Merge**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Implement **upserts and deletes** using CDC.\n",
    "\n",
    "### **Step 1: Create Source Data with New Changes**\n",
    "\n",
    "``` python\n",
    "updated_data = [(101, 5500, \"2024-02-01\"), (104, 9000, \"2024-02-05\")]  # New transaction\n",
    "source_df = spark.createDataFrame(updated_data, columns)\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/updates\")\n",
    "```\n",
    "\n",
    "### **Step 2: Merge Updates into Target Table**\n",
    "\n",
    "``` python\n",
    "source_table = DeltaTable.forPath(spark, \"/mnt/delta/updates\")\n",
    "delta_table.alias(\"t\").merge(\n",
    "    source_table.alias(\"s\"), \"t.transaction_id = s.transaction_id\"\n",
    ").whenMatchedUpdate(set={\"t.amount\": \"s.amount\"})\n",
    " .whenNotMatchedInsert(values={\"transaction_id\": \"s.transaction_id\", \"amount\": \"s.amount\", \"transaction_date\": \"s.transaction_date\"})\n",
    " .execute()\n",
    "```\n",
    "\n",
    "### **Step 3: Verify the Changes**\n",
    "\n",
    "``` python\n",
    "delta_table.toDF().show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Existing transactions should be updated, and\n",
    "new transactions should be inserted**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Streaming CDC with Structured Streaming**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Capture and process real-time updates using **Streaming CDC**.\n",
    "\n",
    "### **Step 1: Enable Change Data Feed in Delta Lake**\n",
    "\n",
    "``` sql\n",
    "ALTER TABLE transactions SET TBLPROPERTIES ('delta.enableChangeDataFeed' = true);\n",
    "```\n",
    "\n",
    "### **Step 2: Read CDC Data as a Stream**\n",
    "\n",
    "``` python\n",
    "cdc_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"readChangeFeed\", \"true\")\\\n",
    "    .table(\"transactions\")\n",
    "\n",
    "query = cdc_df.writeStream.format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - New changes should **stream continuously**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Optimizing Time Travel and CDC Performance**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Optimize **query performance** and **manage storage overhead**.\n",
    "\n",
    "### **Step 1: Run Delta Optimization Commands**\n",
    "\n",
    "``` python\n",
    "# Optimize table for performance\n",
    "spark.sql(\"OPTIMIZE transactions\")\n",
    "\n",
    "# Clean old versions to reduce storage\n",
    "spark.sql(\"VACUUM transactions RETAIN 30 HOURS\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Improved **query performance and reduced storage\n",
    "overhead**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have gained expertise in: -\n",
    "**Querying and restoring historical data using Time Travel.** -\n",
    "**Implementing Change Data Capture (CDC) with structured updates.** -\n",
    "**Building streaming CDC pipelines for real-time analytics.** -\n",
    "**Optimizing data lake performance using Delta Lake commands.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable,\n",
    "fault-tolerant, and high-performance data pipelines** for\n",
    "**enterprise-grade architectures**."
   ],
   "id": "989a95d1-d22c-44d9-9ef6-4966a217e604"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
