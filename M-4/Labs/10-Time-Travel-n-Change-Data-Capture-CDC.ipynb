{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a9b2cb87",
   "metadata": {},
   "source": [
    "# **Time Travel and Change Data Capture (CDC) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides a **detailed, step-by-step guide** to implementing **Time Travel and Change Data Capture (CDC)** using **Apache Spark and Delta Lake**. These hands-on exercises will focus on **real-world scenarios**, utilizing datasets such as **Bank Transactions, Loan Foreclosures, and Flight Data** to ensure practical learning with minimal modifications.\n",
    "\n",
    "These labs cover:\n",
    "- **Implementing Time Travel to access historical data versions**\n",
    "- **Restoring previous data states using Delta Lake**\n",
    "- **Tracking and processing changes using CDC techniques**\n",
    "- **Using Structured Streaming for real-time CDC pipelines**\n",
    "- **Ensuring fault tolerance, exactly-once processing, and optimization**\n",
    "\n",
    "Each lab provides **detailed code, execution steps, and validation techniques** to build **scalable, enterprise-ready data pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Implementing Time Travel in Delta Lake**\n",
    "### **Objective:**\n",
    "- Enable **data versioning** and retrieve historical records.\n",
    "- Restore data to a previous state using **Time Travel**.\n",
    "\n",
    "### **Step 1: Create a Delta Table with Sample Data**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TimeTravelLab\").getOrCreate()\n",
    "\n",
    "# Define sample transactions data\n",
    "data = [(101, 5000, \"2024-02-01\"), (102, 7000, \"2024-02-02\"), (103, 4500, \"2024-02-03\")]\n",
    "columns = [\"transaction_id\", \"amount\", \"transaction_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Perform Updates and Check Versions**\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/transactions\")\n",
    "\n",
    "# Update a record\n",
    "delta_table.update(\"transaction_id = 101\", {\"amount\": \"6000\"})\n",
    "\n",
    "# Show history\n",
    "delta_table.history().show()\n",
    "```\n",
    "\n",
    "### **Step 3: Query a Previous Version**\n",
    "```python\n",
    "previous_version = 0  # Adjust based on the output of history()\n",
    "delta_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(\"/mnt/delta/transactions\")\n",
    "delta_df.show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The previous transaction state should be retrieved using **Time Travel**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Restoring Data to a Previous Version**\n",
    "### **Objective:**\n",
    "- Restore a table to a **specific historical version**.\n",
    "\n",
    "### **Step 1: Restore Table to a Previous Version**\n",
    "```python\n",
    "restore_version = 1  # Choose an appropriate version from history()\n",
    "delta_table.restoreToVersion(restore_version)\n",
    "```\n",
    "\n",
    "### **Step 2: Verify the Restoration**\n",
    "```python\n",
    "spark.read.format(\"delta\").load(\"/mnt/delta/transactions\").show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The table should **roll back** to its previous state.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Implementing Change Data Capture (CDC) Using Merge**\n",
    "### **Objective:**\n",
    "- Implement **upserts and deletes** using CDC.\n",
    "\n",
    "### **Step 1: Create Source Data with New Changes**\n",
    "```python\n",
    "updated_data = [(101, 5500, \"2024-02-01\"), (104, 9000, \"2024-02-05\")]  # New transaction\n",
    "source_df = spark.createDataFrame(updated_data, columns)\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/updates\")\n",
    "```\n",
    "\n",
    "### **Step 2: Merge Updates into Target Table**\n",
    "```python\n",
    "source_table = DeltaTable.forPath(spark, \"/mnt/delta/updates\")\n",
    "delta_table.alias(\"t\").merge(\n",
    "    source_table.alias(\"s\"), \"t.transaction_id = s.transaction_id\"\n",
    ").whenMatchedUpdate(set={\"t.amount\": \"s.amount\"})\n",
    " .whenNotMatchedInsert(values={\"transaction_id\": \"s.transaction_id\", \"amount\": \"s.amount\", \"transaction_date\": \"s.transaction_date\"})\n",
    " .execute()\n",
    "```\n",
    "\n",
    "### **Step 3: Verify the Changes**\n",
    "```python\n",
    "delta_table.toDF().show()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Existing transactions should be updated, and new transactions should be inserted**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Streaming CDC with Structured Streaming**\n",
    "### **Objective:**\n",
    "- Capture and process real-time updates using **Streaming CDC**.\n",
    "\n",
    "### **Step 1: Enable Change Data Feed in Delta Lake**\n",
    "```sql\n",
    "ALTER TABLE transactions SET TBLPROPERTIES ('delta.enableChangeDataFeed' = true);\n",
    "```\n",
    "\n",
    "### **Step 2: Read CDC Data as a Stream**\n",
    "```python\n",
    "cdc_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"readChangeFeed\", \"true\")\\\n",
    "    .table(\"transactions\")\n",
    "\n",
    "query = cdc_df.writeStream.format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- New changes should **stream continuously**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Optimizing Time Travel and CDC Performance**\n",
    "### **Objective:**\n",
    "- Optimize **query performance** and **manage storage overhead**.\n",
    "\n",
    "### **Step 1: Run Delta Optimization Commands**\n",
    "```python\n",
    "# Optimize table for performance\n",
    "spark.sql(\"OPTIMIZE transactions\")\n",
    "\n",
    "# Clean old versions to reduce storage\n",
    "spark.sql(\"VACUUM transactions RETAIN 30 HOURS\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Improved **query performance and reduced storage overhead**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Querying and restoring historical data using Time Travel.**\n",
    "- **Implementing Change Data Capture (CDC) with structured updates.**\n",
    "- **Building streaming CDC pipelines for real-time analytics.**\n",
    "- **Optimizing data lake performance using Delta Lake commands.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable, fault-tolerant, and high-performance data pipelines** for **enterprise-grade architectures**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}