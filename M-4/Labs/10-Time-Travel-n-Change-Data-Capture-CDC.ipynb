{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "545a5ede",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Time Travel and Change Data Capture (CDC) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides a **detailed, step-by-step guide** to implementing **Time Travel and Change Data Capture (CDC)** using **Apache Spark and Delta Lake**. These hands-on exercises will focus on **real-world scenarios**, utilizing datasets such as **Bank Transactions, Loan Foreclosures, and Flight Data** to ensure practical learning with minimal modifications.\n",
    "\n",
    "These labs cover:\n",
    "- **Implementing Time Travel to access historical data versions**\n",
    "- **Restoring previous data states using Delta Lake**\n",
    "- **Tracking and processing changes using CDC techniques**\n",
    "- **Using Structured Streaming for real-time CDC pipelines**\n",
    "- **Ensuring fault tolerance, exactly-once processing, and optimization**\n",
    "\n",
    "Each lab provides **detailed code, execution steps, and validation techniques** to build **scalable, enterprise-ready data pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Implementing Time Travel in Delta Lake**\n",
    "### **Objective:**\n",
    "- Enable **data versioning** and retrieve historical records.\n",
    "- Restore data to a previous state using **Time Travel**.\n",
    "\n",
    "### **Step 1: Create a Delta Table with Sample Data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e62903",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TimeTravelLab\").getOrCreate()\n",
    "\n",
    "# Define sample transactions data\n",
    "data = [(101, 5000, \"2024-02-01\"), (102, 7000, \"2024-02-02\"), (103, 4500, \"2024-02-03\")]\n",
    "columns = [\"transaction_id\", \"amount\", \"transaction_date\"]\n",
    "\n",
    "df = spark.createDataFrame(data, columns)\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8030315",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Perform Updates and Check Versions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b50498ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import DeltaTable\n",
    "\n",
    "# Load Delta table\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/transactions\")\n",
    "\n",
    "# Update a record\n",
    "delta_table.update(\"transaction_id = 101\", {\"amount\": \"6000\"})\n",
    "\n",
    "# Show history\n",
    "delta_table.history().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c292fc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Query a Previous Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "354e78d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_version = 0  # Adjust based on the output of history()\n",
    "delta_df = spark.read.format(\"delta\").option(\"versionAsOf\", previous_version).load(\"/mnt/delta/transactions\")\n",
    "delta_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1683d96",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- The previous transaction state should be retrieved using **Time Travel**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Restoring Data to a Previous Version**\n",
    "### **Objective:**\n",
    "- Restore a table to a **specific historical version**.\n",
    "\n",
    "### **Step 1: Restore Table to a Previous Version**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118ea50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "restore_version = 1  # Choose an appropriate version from history()\n",
    "delta_table.restoreToVersion(restore_version)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c294739",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Verify the Restoration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7fc585",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"delta\").load(\"/mnt/delta/transactions\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3a18d9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- The table should **roll back** to its previous state.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Implementing Change Data Capture (CDC) Using Merge**\n",
    "### **Objective:**\n",
    "- Implement **upserts and deletes** using CDC.\n",
    "\n",
    "### **Step 1: Create Source Data with New Changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d9c1119",
   "metadata": {},
   "outputs": [],
   "source": [
    "updated_data = [(101, 5500, \"2024-02-01\"), (104, 9000, \"2024-02-05\")]  # New transaction\n",
    "source_df = spark.createDataFrame(updated_data, columns)\n",
    "source_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/updates\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c3f42",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Merge Updates into Target Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecd5e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_table = DeltaTable.forPath(spark, \"/mnt/delta/updates\")\n",
    "delta_table.alias(\"t\").merge(\n",
    "    source_table.alias(\"s\"), \"t.transaction_id = s.transaction_id\"\n",
    ").whenMatchedUpdate(set={\"t.amount\": \"s.amount\"})\n",
    " .whenNotMatchedInsert(values={\"transaction_id\": \"s.transaction_id\", \"amount\": \"s.amount\", \"transaction_date\": \"s.transaction_date\"})\n",
    " .execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5061357",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Verify the Changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4013cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_table.toDF().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28e17fc8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- **Existing transactions should be updated, and new transactions should be inserted**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Streaming CDC with Structured Streaming**\n",
    "### **Objective:**\n",
    "- Capture and process real-time updates using **Streaming CDC**.\n",
    "\n",
    "### **Step 1: Enable Change Data Feed in Delta Lake**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592f71b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "ALTER TABLE transactions SET TBLPROPERTIES ('delta.enableChangeDataFeed' = true);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d94aa119",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Read CDC Data as a Stream**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dee5d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cdc_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"readChangeFeed\", \"true\")\\\n",
    "    .table(\"transactions\")\n",
    "\n",
    "query = cdc_df.writeStream.format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9626d678",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- New changes should **stream continuously**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Optimizing Time Travel and CDC Performance**\n",
    "### **Objective:**\n",
    "- Optimize **query performance** and **manage storage overhead**.\n",
    "\n",
    "### **Step 1: Run Delta Optimization Commands**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfffa0ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimize table for performance\n",
    "spark.sql(\"OPTIMIZE transactions\")\n",
    "\n",
    "# Clean old versions to reduce storage\n",
    "spark.sql(\"VACUUM transactions RETAIN 30 HOURS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121fb17",
   "metadata": {},
   "source": [
    "**Expected Outcome:**\n",
    "- Improved **query performance and reduced storage overhead**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Querying and restoring historical data using Time Travel.**\n",
    "- **Implementing Change Data Capture (CDC) with structured updates.**\n",
    "- **Building streaming CDC pipelines for real-time analytics.**\n",
    "- **Optimizing data lake performance using Delta Lake commands.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable, fault-tolerant, and high-performance data pipelines** for **enterprise-grade architectures**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
