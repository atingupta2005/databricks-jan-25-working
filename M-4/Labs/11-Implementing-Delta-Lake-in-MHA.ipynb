{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "da7eb56d",
   "metadata": {},
   "source": [
    "# **Implementing Delta Lake in Multi-Hop Architectures - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This lab document provides a **detailed, step-by-step guide** to implementing a **Multi-Hop Architecture (Bronze, Silver, Gold layers)** using **Delta Lake**. These hands-on exercises focus on **real-world scenarios** using datasets such as **Bank Transactions, Loan Foreclosures, and Flight Data** to ensure practical learning with minimal modifications.\n",
    "\n",
    "These labs cover:\n",
    "- **Setting up Delta Lake multi-hop pipelines**\n",
    "- **Ingesting data into the Bronze layer from Kafka and cloud storage**\n",
    "- **Transforming data in the Silver layer (Cleansing, Deduplication, and CDC)**\n",
    "- **Aggregating and curating data in the Gold layer**\n",
    "- **Optimizing performance using Z-Ordering, Compaction, and Schema Enforcement**\n",
    "- **Enabling Change Data Capture (CDC) and Time Travel**\n",
    "\n",
    "Each lab provides **detailed code, execution steps, and validation techniques** to build **scalable, enterprise-ready data pipelines**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Ingesting Data into the Bronze Layer**\n",
    "### **Objective:**\n",
    "- Load raw, unprocessed data into the **Bronze layer** from streaming and batch sources.\n",
    "\n",
    "### **Step 1: Load Streaming Data from Kafka into the Bronze Layer**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"BronzeLayerIngestion\").getOrCreate()\n",
    "\n",
    "bronze_df = spark.readStream.format(\"kafka\")\\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\")\\\n",
    "    .option(\"subscribe\", \"transactions\")\\\n",
    "    .option(\"startingOffsets\", \"earliest\")\\\n",
    "    .load()\n",
    "\n",
    "bronze_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/bronze_checkpoint\")\\\n",
    "    .start(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Load Batch Data from Cloud Storage**\n",
    "```python\n",
    "bronze_batch_df = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .load(\"s3://my-bucket/raw-data/\")\n",
    "\n",
    "bronze_batch_df.write.format(\"delta\").mode(\"append\").save(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Raw event-driven data is ingested into Delta Lake in append-only format.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Processing and Cleaning Data in the Silver Layer**\n",
    "### **Objective:**\n",
    "- Deduplicate, clean, and transform the data **incrementally**.\n",
    "\n",
    "### **Step 1: Load Bronze Data into Silver Layer**\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "bronze_table = DeltaTable.forPath(spark, \"/mnt/delta/bronze_transactions\")\n",
    "\n",
    "silver_df = bronze_table.toDF().dropDuplicates([\"transaction_id\"]).filter(\"amount IS NOT NULL\")\n",
    "\n",
    "silver_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/silver_transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Change Data Capture (CDC) for Incremental Processing**\n",
    "```python\n",
    "cdc_df = spark.readStream.format(\"delta\")\\\n",
    "    .option(\"readChangeFeed\", \"true\")\\\n",
    "    .table(\"silver_transactions\")\n",
    "\n",
    "query = cdc_df.writeStream.format(\"console\").start()\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Silver layer contains cleaned, deduplicated data ready for business logic transformations.**\n",
    "- **Incremental changes are continuously processed using CDC.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Aggregating and Curating Data in the Gold Layer**\n",
    "### **Objective:**\n",
    "- Transform and **aggregate** cleansed data into business-ready tables.\n",
    "\n",
    "### **Step 1: Perform Aggregations and Store Data in the Gold Layer**\n",
    "```python\n",
    "gold_df = spark.read.format(\"delta\").load(\"/mnt/delta/silver_transactions\")\\\n",
    "    .groupBy(\"customer_id\", \"transaction_date\")\\\n",
    "    .agg({\"amount\": \"sum\"})\\\n",
    "    .withColumnRenamed(\"sum(amount)\", \"total_spent\")\n",
    "\n",
    "gold_df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/delta/gold_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Gold layer contains structured, aggregated data optimized for reporting and machine learning.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Enabling Time Travel for Data Versioning**\n",
    "### **Objective:**\n",
    "- Query and restore previous data versions using **Time Travel**.\n",
    "\n",
    "### **Step 1: Query a Previous Version of the Silver Layer**\n",
    "```sql\n",
    "SELECT * FROM silver_transactions VERSION AS OF 5;\n",
    "```\n",
    "\n",
    "### **Step 2: Restore a Table to a Previous Version**\n",
    "```python\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "delta_table = DeltaTable.forPath(spark, \"/mnt/delta/silver_transactions\")\n",
    "delta_table.restoreToVersion(3)\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Historical data versions are accessible for compliance, auditing, and rollback.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Optimizing Performance with Compaction and Z-Ordering**\n",
    "### **Objective:**\n",
    "- Improve **query performance** and **reduce storage costs**.\n",
    "\n",
    "### **Step 1: Compact Small Files Using OPTIMIZE**\n",
    "```sql\n",
    "OPTIMIZE gold_transactions;\n",
    "```\n",
    "\n",
    "### **Step 2: Improve Query Performance Using Z-Ordering**\n",
    "```sql\n",
    "OPTIMIZE gold_transactions ZORDER BY customer_id;\n",
    "```\n",
    "\n",
    "### **Step 3: Clean Up Old Data Versions Using VACUUM**\n",
    "```sql\n",
    "VACUUM silver_transactions RETAIN 24 HOURS;\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **Improved query efficiency and reduced storage fragmentation.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 6: Automating Data Ingestion Using Auto Loader**\n",
    "### **Objective:**\n",
    "- Automate continuous ingestion from cloud storage.\n",
    "\n",
    "### **Step 1: Configure Auto Loader to Monitor a Cloud Storage Path**\n",
    "```python\n",
    "raw_df = spark.readStream.format(\"cloudFiles\")\\\n",
    "    .option(\"cloudFiles.format\", \"csv\")\\\n",
    "    .option(\"cloudFiles.schemaLocation\", \"/mnt/delta/schema\")\\\n",
    "    .load(\"s3://my-bucket/raw-data/\")\n",
    "\n",
    "raw_df.writeStream.format(\"delta\")\\\n",
    "    .option(\"checkpointLocation\", \"/mnt/delta/autoloader_checkpoint\")\\\n",
    "    .start(\"/mnt/delta/bronze_transactions\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **New files are automatically detected and processed.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have gained expertise in:\n",
    "- **Building end-to-end Delta Lake pipelines using a multi-hop architecture.**\n",
    "- **Processing raw data incrementally using CDC and schema enforcement.**\n",
    "- **Optimizing Delta Lake tables for performance and cost efficiency.**\n",
    "- **Automating data ingestion with Auto Loader.**\n",
    "- **Ensuring data reliability with ACID transactions, time travel, and Z-Ordering.**\n",
    "\n",
    "These labs provide **real-world experience** in building **scalable, fault-tolerant, and high-performance multi-hop architectures** that support **enterprise data lakes, analytics, and AI workloads**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}