{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ba48960",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Optimizing Storage and Compute Costs - Concepts**\n",
    "\n",
    "## **Introduction**\n",
    "Efficient cost management is a critical aspect of **data engineering, analytics, and machine learning workflows** in cloud environments. Organizations must balance **performance, scalability, and cost-efficiency** while storing and processing large-scale datasets.\n",
    "\n",
    "This document provides a **detailed guide** on optimizing **storage and compute costs** in cloud-based data platforms such as **Databricks, Azure, and AWS**. It covers:\n",
    "- **Understanding cloud storage and compute pricing models**\n",
    "- **Techniques to reduce storage costs (Delta Lake, data compression, lifecycle policies)**\n",
    "- **Optimizing compute resources (auto-scaling, job tuning, cluster management)**\n",
    "- **Best practices for cost monitoring and governance**\n",
    "\n",
    "---\n",
    "\n",
    "## **1. Understanding Cloud Storage and Compute Costs**\n",
    "### **1.1 Storage Pricing Models**\n",
    "Cloud storage costs vary depending on the **type of storage** and the **frequency of access**. Key pricing models include:\n",
    "\n",
    "| Storage Type         | Characteristics | Cost Factors |\n",
    "|----------------------|----------------|--------------|\n",
    "| **Hot Storage**      | Fast access, high availability | Higher cost per GB |\n",
    "| **Cool Storage**     | Less frequent access | Lower cost, higher retrieval time |\n",
    "| **Archive Storage**  | Long-term retention | Lowest cost, significant retrieval time |\n",
    "\n",
    "### **1.2 Compute Pricing Models**\n",
    "Compute resources are typically billed based on **usage duration, processing power, and instance type**.\n",
    "\n",
    "| Compute Type             | Cost Factor |\n",
    "|--------------------------|------------|\n",
    "| **On-Demand Instances**  | Pay-as-you-go, higher cost |\n",
    "| **Spot/Preemptible Instances** | Lower cost, can be interrupted |\n",
    "| **Reserved Instances**   | Long-term commitment, cost-effective |\n",
    "| **Serverless Computing** | Pay for execution time only |\n",
    "\n",
    "---\n",
    "\n",
    "## **2. Storage Optimization Techniques**\n",
    "### **2.1 Using Delta Lake for Efficient Storage**\n",
    "**Delta Lake** optimizes storage and reduces costs through:\n",
    "- **File compaction**: Reduces small file inefficiencies.\n",
    "- **Data skipping**: Reads only relevant data.\n",
    "- **Time travel**: Avoids unnecessary data duplication.\n",
    "\n",
    "#### **Example: Enabling Delta Lake Compaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aa3b423",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "deltaTable = DeltaTable.forPath(spark, \"abfss://datalake@storage.dfs.core.windows.net/sales\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b17628",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **2.2 Data Compression and Columnar Formats**\n",
    "Using **Parquet** and **ORC** formats can significantly reduce storage footprint while improving query performance.\n",
    "\n",
    "#### **Example: Writing Data in Parquet Format**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13964468",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").save(\"abfss://datalake@storage.dfs.core.windows.net/compressed-data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8092708c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **2.3 Storage Lifecycle Policies**\n",
    "Automatically **move data from hot to cold storage** based on access frequency.\n",
    "\n",
    "#### **Example: Configuring Azure Blob Storage Lifecycle Policy**\n",
    "```json\n",
    "{\n",
    "  \"rules\": [\n",
    "    {\n",
    "      \"name\": \"MoveOldDataToCoolTier\",\n",
    "      \"enabled\": true,\n",
    "      \"action\": { \"type\": \"MoveToCool\" },\n",
    "      \"conditions\": { \"daysSinceModificationGreaterThan\": 90 }\n",
    "    }\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3. Compute Cost Optimization**\n",
    "### **3.1 Auto-Scaling and Cluster Management**\n",
    "Enabling **auto-scaling** in Databricks dynamically adjusts cluster size based on workload demand.\n",
    "\n",
    "#### **Example: Enabling Auto-Scaling in Databricks**\n",
    "```json\n",
    "{\n",
    "  \"cluster_name\": \"Optimized Cluster\",\n",
    "  \"autoscale\": {\n",
    "    \"min_workers\": 2,\n",
    "    \"max_workers\": 8\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **3.2 Optimizing Spark Jobs**\n",
    "Techniques to improve Spark efficiency and reduce compute costs:\n",
    "- **Optimize Shuffle Operations**: Reduce unnecessary data movement.\n",
    "- **Broadcast Joins**: Use when smaller datasets are involved.\n",
    "- **Cache Intermediate Results**: Avoid recomputation.\n",
    "\n",
    "#### **Example: Using Broadcast Joins in PySpark**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6952cf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import broadcast\n",
    "df_small = broadcast(spark.read.format(\"delta\").load(\"/small-dataset\"))\n",
    "df_large = spark.read.format(\"delta\").load(\"/large-dataset\")\n",
    "joined_df = df_large.join(df_small, \"key\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf1ba9f",
   "metadata": {},
   "source": [
    "### **3.3 Serverless Computing for Cost Efficiency**\n",
    "Serverless architectures reduce costs by eliminating idle compute resources.\n",
    "\n",
    "#### **Example: Using Databricks Serverless SQL Warehouse**\n",
    "```json\n",
    "{\n",
    "  \"warehouse_type\": \"serverless\",\n",
    "  \"enable_auto_stop\": true\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4. Cost Monitoring and Governance**\n",
    "### **4.1 Tracking Storage and Compute Costs**\n",
    "- Use **Azure Cost Management**, **AWS Cost Explorer**, or **Databricks Cost Dashboard** to monitor spending.\n",
    "- Implement **alerts** to detect high-cost jobs or excessive storage usage.\n",
    "\n",
    "#### **Example: Setting Up Azure Cost Alerts**\n",
    "```json\n",
    "{\n",
    "  \"conditions\": {\n",
    "    \"metricName\": \"StorageUsedGB\",\n",
    "    \"threshold\": 5000\n",
    "  },\n",
    "  \"actionGroup\": \"SendEmailAlert\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **4.2 Cost Governance Best Practices**\n",
    "- **Set Quotas**: Limit maximum resource allocation to avoid unexpected costs.\n",
    "- **Implement Budget Alerts**: Notify teams when nearing budget limits.\n",
    "- **Use Tagging Strategies**: Classify cloud resources by project, team, or cost center.\n",
    "\n",
    "#### **Example: Applying Tags for Cost Allocation**\n",
    "```json\n",
    "{\n",
    "  \"resource\": \"databricks-cluster\",\n",
    "  \"tags\": {\n",
    "    \"department\": \"finance\",\n",
    "    \"project\": \"etl-optimization\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "Optimizing **storage and compute costs** ensures **scalability, performance, and financial efficiency**. By implementing strategies such as:\n",
    "- **Delta Lake and columnar storage formats** for cost-effective data retention.\n",
    "- **Auto-scaling and Spark job optimizations** to minimize compute costs.\n",
    "- **Monitoring tools and governance policies** to enforce cost control.\n",
    "\n",
    "Organizations can **maximize their cloud investment** while maintaining high performance for data and AI workloads.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
