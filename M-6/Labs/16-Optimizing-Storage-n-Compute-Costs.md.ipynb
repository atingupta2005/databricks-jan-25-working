{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Optimizing Storage and Compute Costs - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This hands-on lab document provides **detailed, step-by-step exercises**\n",
    "for optimizing **storage and compute costs** in **Databricks, Azure, and\n",
    "AWS environments**. These labs cover: - **Efficient data storage using\n",
    "Delta Lake, Parquet, and lifecycle management**. - **Optimizing compute\n",
    "usage through cluster tuning, auto-scaling, and serverless\n",
    "computing**. - **Monitoring and governing costs using cloud-native tools\n",
    "and alerts**.\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step\n",
    "instructions**, and **sample dataset usage** (Banks Data, Loan\n",
    "Foreclosure Data, Flights Data from previous notebooks) to ensure\n",
    "**scalability and reliability**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Implementing Delta Lake for Storage Optimization**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Reduce **storage costs and query time** by using **Delta Lake** for\n",
    "    efficient data storage.\n",
    "\n",
    "### **Step 1: Create a Delta Table**\n",
    "\n",
    "``` python\n",
    "from delta.tables import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaOptimization\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")\n",
    "\n",
    "# Convert to Delta Lake\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://datalake@storage.dfs.core.windows.net/delta/flights\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Delta File Compaction**\n",
    "\n",
    "``` python\n",
    "deltaTable = DeltaTable.forPath(spark, \"abfss://datalake@storage.dfs.core.windows.net/delta/flights\")\n",
    "deltaTable.optimize().executeCompaction()\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - Delta Lake **reduces small files**, improving\n",
    "query efficiency and reducing storage costs.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Using Parquet Format to Reduce Storage Costs**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Use **columnar storage formats** like **Parquet** to minimize\n",
    "    storage costs.\n",
    "\n",
    "### **Step 1: Convert CSV to Parquet**\n",
    "\n",
    "``` python\n",
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"abfss://datalake@storage.dfs.core.windows.net/parquet/flights\")\n",
    "```\n",
    "\n",
    "### **Step 2: Compare File Sizes**\n",
    "\n",
    "``` bash\n",
    "# List storage files to compare sizes\n",
    "az storage blob list --container-name datalake --account-name myaccount --query \"[].{Name:name, Size:properties.contentLength}\"\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **Parquet files are smaller than CSV**, reducing\n",
    "storage costs while maintaining fast query performance.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Implementing Auto-Scaling for Compute Optimization**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Automatically **adjust cluster size** based on workload demand to\n",
    "    reduce costs.\n",
    "\n",
    "### **Step 1: Configure Auto-Scaling in Databricks**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"cluster_name\": \"Optimized Cluster\",\n",
    "  \"autoscale\": {\n",
    "    \"min_workers\": 2,\n",
    "    \"max_workers\": 8\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Monitor Auto-Scaling Performance**\n",
    "\n",
    "-   Open **Databricks UI** → Navigate to **Clusters**.\n",
    "-   Observe the cluster **scaling up/down** based on workload.\n",
    "\n",
    "**Expected Outcome:** - Cluster **scales dynamically**, reducing compute\n",
    "costs when idle and handling peak loads efficiently.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Enabling Serverless Computing to Minimize Costs**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Run **serverless queries** on demand, paying only for execution\n",
    "    time.\n",
    "\n",
    "### **Step 1: Enable Serverless SQL Warehouse in Databricks**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"warehouse_type\": \"serverless\",\n",
    "  \"enable_auto_stop\": true\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Run a Serverless Query**\n",
    "\n",
    "``` sql\n",
    "SELECT COUNT(*) FROM delta.`abfss://datalake@storage.dfs.core.windows.net/delta/flights`;\n",
    "```\n",
    "\n",
    "**Expected Outcome:** - **No persistent compute resources**,\n",
    "significantly reducing idle compute costs.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Implementing Cost Monitoring and Alerts**\n",
    "\n",
    "### **Objective:**\n",
    "\n",
    "-   Set up **cost monitoring tools** and **alerts** to track and control\n",
    "    expenses.\n",
    "\n",
    "### **Step 1: Configure Azure Cost Alerts**\n",
    "\n",
    "``` json\n",
    "{\n",
    "  \"conditions\": {\n",
    "    \"metricName\": \"StorageUsedGB\",\n",
    "    \"threshold\": 5000\n",
    "  },\n",
    "  \"actionGroup\": \"SendEmailAlert\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Cost Reports in Azure Cost Management**\n",
    "\n",
    "1.  Open **Azure Portal** → Go to **Cost Management + Billing**.\n",
    "2.  Navigate to **Cost Analysis** and set up a **custom budget alert**.\n",
    "\n",
    "**Expected Outcome:** - Teams receive **alerts** when storage or compute\n",
    "usage exceeds predefined thresholds.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "By completing these **hands-on labs**, you have learned how to: -\n",
    "**Optimize storage using Delta Lake and Parquet formats**. - **Minimize\n",
    "compute costs with auto-scaling and serverless computing**. -\n",
    "**Implement real-time cost monitoring and governance policies**.\n",
    "\n",
    "These labs provide **real-world experience** in building\n",
    "**cost-effective and scalable data workflows** while maintaining\n",
    "performance and reliability."
   ],
   "id": "efa7e735-62d4-4c8c-84ad-a9c0c3811120"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
