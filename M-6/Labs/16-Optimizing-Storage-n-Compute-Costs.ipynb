{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e1e0464e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Optimizing Storage and Compute Costs - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** for optimizing **storage and compute costs** in **Databricks, Azure, and AWS environments**. These labs cover:\n",
    "- **Efficient data storage using Delta Lake, Parquet, and lifecycle management**.\n",
    "- **Optimizing compute usage through cluster tuning, auto-scaling, and serverless computing**.\n",
    "- **Monitoring and governing costs using cloud-native tools and alerts**.\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step instructions**, and **sample dataset usage** (Banks Data, Loan Foreclosure Data, Flights Data from previous notebooks) to ensure **scalability and reliability**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Implementing Delta Lake for Storage Optimization**\n",
    "### **Objective:**\n",
    "- Reduce **storage costs and query time** by using **Delta Lake** for efficient data storage.\n",
    "\n",
    "### **Step 1: Create a Delta Table**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8766da9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"DeltaOptimization\").getOrCreate()\n",
    "\n",
    "# Load Data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")\n",
    "\n",
    "# Convert to Delta Lake\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"abfss://datalake@storage.dfs.core.windows.net/delta/flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3265bc2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Enable Delta File Compaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9c298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaTable = DeltaTable.forPath(spark, \"abfss://datalake@storage.dfs.core.windows.net/delta/flights\")\n",
    "deltaTable.optimize().executeCompaction()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a54338f7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Delta Lake **reduces small files**, improving query efficiency and reducing storage costs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Using Parquet Format to Reduce Storage Costs**\n",
    "### **Objective:**\n",
    "- Use **columnar storage formats** like **Parquet** to minimize storage costs.\n",
    "\n",
    "### **Step 1: Convert CSV to Parquet**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f76d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.write.format(\"parquet\").mode(\"overwrite\").save(\"abfss://datalake@storage.dfs.core.windows.net/parquet/flights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0773e2cb",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Compare File Sizes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de52b4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "# List storage files to compare sizes\n",
    "az storage blob list --container-name datalake --account-name myaccount --query \"[].{Name:name, Size:properties.contentLength}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f0b7b7",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- **Parquet files are smaller than CSV**, reducing storage costs while maintaining fast query performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Implementing Auto-Scaling for Compute Optimization**\n",
    "### **Objective:**\n",
    "- Automatically **adjust cluster size** based on workload demand to reduce costs.\n",
    "\n",
    "### **Step 1: Configure Auto-Scaling in Databricks**\n",
    "```json\n",
    "{\n",
    "  \"cluster_name\": \"Optimized Cluster\",\n",
    "  \"autoscale\": {\n",
    "    \"min_workers\": 2,\n",
    "    \"max_workers\": 8\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Monitor Auto-Scaling Performance**\n",
    "- Open **Databricks UI** → Navigate to **Clusters**.\n",
    "- Observe the cluster **scaling up/down** based on workload.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Cluster **scales dynamically**, reducing compute costs when idle and handling peak loads efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Enabling Serverless Computing to Minimize Costs**\n",
    "### **Objective:**\n",
    "- Run **serverless queries** on demand, paying only for execution time.\n",
    "\n",
    "### **Step 1: Enable Serverless SQL Warehouse in Databricks**\n",
    "```json\n",
    "{\n",
    "  \"warehouse_type\": \"serverless\",\n",
    "  \"enable_auto_stop\": true\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Run a Serverless Query**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d637ced",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT COUNT(*) FROM delta.`abfss://datalake@storage.dfs.core.windows.net/delta/flights`;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb03c98",
   "metadata": {},
   "source": [
    "**Expected Outcome:**\n",
    "- **No persistent compute resources**, significantly reducing idle compute costs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Implementing Cost Monitoring and Alerts**\n",
    "### **Objective:**\n",
    "- Set up **cost monitoring tools** and **alerts** to track and control expenses.\n",
    "\n",
    "### **Step 1: Configure Azure Cost Alerts**\n",
    "```json\n",
    "{\n",
    "  \"conditions\": {\n",
    "    \"metricName\": \"StorageUsedGB\",\n",
    "    \"threshold\": 5000\n",
    "  },\n",
    "  \"actionGroup\": \"SendEmailAlert\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Cost Reports in Azure Cost Management**\n",
    "1. Open **Azure Portal** → Go to **Cost Management + Billing**.\n",
    "2. Navigate to **Cost Analysis** and set up a **custom budget alert**.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Teams receive **alerts** when storage or compute usage exceeds predefined thresholds.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have learned how to:\n",
    "- **Optimize storage using Delta Lake and Parquet formats**.\n",
    "- **Minimize compute costs with auto-scaling and serverless computing**.\n",
    "- **Implement real-time cost monitoring and governance policies**.\n",
    "\n",
    "These labs provide **real-world experience** in building **cost-effective and scalable data workflows** while maintaining performance and reliability.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
