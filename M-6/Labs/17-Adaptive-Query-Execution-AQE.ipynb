{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8b34dc0",
   "metadata": {},
   "source": [
    "# **Adaptive Query Execution (AQE) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** to implement and optimize **Adaptive Query Execution (AQE)** in Apache Spark and **Databricks**. These labs will cover:\n",
    "- **Enabling AQE and key configurations**\n",
    "- **Optimizing shuffle partitions dynamically**\n",
    "- **Implementing adaptive join strategies**\n",
    "- **Handling data skew effectively**\n",
    "- **Monitoring and troubleshooting AQE performance**\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step instructions**, and **sample dataset usage** (Banks Data, Loan Foreclosure Data, Flights Data from previous notebooks) to ensure **efficient query execution**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Enabling AQE and Configurations**\n",
    "### **Objective:**\n",
    "- Enable and verify **Adaptive Query Execution (AQE)** in **Databricks**.\n",
    "\n",
    "### **Step 1: Enable AQE in Databricks**\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AQE_Optimization\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "```\n",
    "\n",
    "### **Step 2: Verify AQE Configuration**\n",
    "```python\n",
    "print(\"AQE Enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- AQE is successfully **enabled** and **ready for execution optimizations**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Optimizing Shuffle Partitions with AQE**\n",
    "### **Objective:**\n",
    "- Use **AQE to dynamically adjust shuffle partitions** based on data size.\n",
    "\n",
    "### **Step 1: Load Sample Data** (Flights Data)\n",
    "```python\n",
    "df = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")\n",
    "```\n",
    "\n",
    "### **Step 2: Enable Dynamic Shuffle Partitioning**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\")\n",
    "```\n",
    "\n",
    "### **Step 3: Execute and Observe Changes**\n",
    "```python\n",
    "result = df.groupBy(\"flight_id\").count().collect()\n",
    "print(\"Query Execution Completed Successfully\")\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- Spark **dynamically adjusts** shuffle partitions to **optimize performance** and **reduce costs**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Implementing Adaptive Join Strategies**\n",
    "### **Objective:**\n",
    "- Dynamically **switch join types** to optimize execution.\n",
    "\n",
    "### **Step 1: Enable Adaptive Joins**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.join.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### **Step 2: Load Sample Datasets (Banks and Loan Data)**\n",
    "```python\n",
    "df_banks = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/banks_data\")\n",
    "df_loans = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/loan_foreclosure\")\n",
    "```\n",
    "\n",
    "### **Step 3: Perform Join and Observe AQE Behavior**\n",
    "```python\n",
    "joined_df = df_loans.join(df_banks, \"bank_id\")\n",
    "joined_df.show(5)\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- **AQE dynamically switches** from **shuffle join** to **broadcast join** when applicable, reducing shuffle overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Handling Data Skew with AQE**\n",
    "### **Objective:**\n",
    "- Optimize queries by **handling data skew dynamically**.\n",
    "\n",
    "### **Step 1: Enable Skew Join Optimization**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "```\n",
    "\n",
    "### **Step 2: Load Skewed Data (Loan Foreclosure Data)**\n",
    "```python\n",
    "df_loans_skewed = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/loan_foreclosure_skewed\")\n",
    "```\n",
    "\n",
    "### **Step 3: Execute Query and Monitor Execution Plan**\n",
    "```python\n",
    "from pyspark.sql.functions import col\n",
    "result = df_loans_skewed.groupBy(\"loan_status\").count()\n",
    "result.explain(True)\n",
    "```\n",
    "\n",
    "**Expected Outcome:**\n",
    "- AQE **automatically splits skewed partitions**, ensuring **better parallelism and query efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Monitoring AQE Execution Plans**\n",
    "### **Objective:**\n",
    "- Understand **how AQE modifies query execution plans**.\n",
    "\n",
    "### **Step 1: Enable AQE Execution Plan Logging**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.logLevel\", \"INFO\")\n",
    "```\n",
    "\n",
    "### **Step 2: Execute a Complex Query with AQE**\n",
    "```python\n",
    "df_complex = df_loans_skewed.groupBy(\"loan_purpose\").agg({\"loan_amount\": \"avg\"})\n",
    "df_complex.explain(True)\n",
    "```\n",
    "\n",
    "### **Step 3: Compare Execution Plan Changes**\n",
    "- Observe **shuffle partition coalescing**.\n",
    "- Identify **adaptive join strategy selection**.\n",
    "- Check for **skew join optimizations**.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The execution plan **shows AQE optimizations**, proving its effectiveness in runtime tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have learned how to:\n",
    "- **Enable and configure AQE** in **Databricks**.\n",
    "- **Optimize shuffle partitions dynamically** to improve query performance.\n",
    "- **Leverage adaptive join strategies** to reduce shuffle overhead.\n",
    "- **Handle data skew automatically** to prevent slow-running queries.\n",
    "- **Monitor and analyze AQE execution plans** for performance tuning.\n",
    "\n",
    "These labs provide **real-world experience** in leveraging **Adaptive Query Execution (AQE) for Spark query optimization**, making **data processing faster, more efficient, and cost-effective**.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}