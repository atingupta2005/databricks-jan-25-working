{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a2bc195",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Adaptive Query Execution (AQE) - Hands-on Labs**\n",
    "\n",
    "## **Introduction**\n",
    "This hands-on lab document provides **detailed, step-by-step exercises** to implement and optimize **Adaptive Query Execution (AQE)** in Apache Spark and **Databricks**. These labs will cover:\n",
    "- **Enabling AQE and key configurations**\n",
    "- **Optimizing shuffle partitions dynamically**\n",
    "- **Implementing adaptive join strategies**\n",
    "- **Handling data skew effectively**\n",
    "- **Monitoring and troubleshooting AQE performance**\n",
    "\n",
    "Each lab includes **real-world examples**, **step-by-step instructions**, and **sample dataset usage** (Banks Data, Loan Foreclosure Data, Flights Data from previous notebooks) to ensure **efficient query execution**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Enabling AQE and Configurations**\n",
    "### **Objective:**\n",
    "- Enable and verify **Adaptive Query Execution (AQE)** in **Databricks**.\n",
    "\n",
    "### **Step 1: Enable AQE in Databricks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0c4b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AQE_Optimization\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8878b2e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Verify AQE Configuration**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b972990a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"AQE Enabled:\", spark.conf.get(\"spark.sql.adaptive.enabled\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62df9d02",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- AQE is successfully **enabled** and **ready for execution optimizations**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Optimizing Shuffle Partitions with AQE**\n",
    "### **Objective:**\n",
    "- Use **AQE to dynamically adjust shuffle partitions** based on data size.\n",
    "\n",
    "### **Step 1: Load Sample Data** (Flights Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f84a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/flights_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "745e2338",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Enable Dynamic Shuffle Partitioning**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90777da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.minPartitionSize\", \"64MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403e7b8b",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Execute and Observe Changes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c3deb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = df.groupBy(\"flight_id\").count().collect()\n",
    "print(\"Query Execution Completed Successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35bc73a1",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- Spark **dynamically adjusts** shuffle partitions to **optimize performance** and **reduce costs**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Implementing Adaptive Join Strategies**\n",
    "### **Objective:**\n",
    "- Dynamically **switch join types** to optimize execution.\n",
    "\n",
    "### **Step 1: Enable Adaptive Joins**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9bc8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.join.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066c29be",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Load Sample Datasets (Banks and Loan Data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d763d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_banks = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/banks_data\")\n",
    "df_loans = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/loan_foreclosure\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d15da",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Perform Join and Observe AQE Behavior**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f364251",
   "metadata": {},
   "outputs": [],
   "source": [
    "joined_df = df_loans.join(df_banks, \"bank_id\")\n",
    "joined_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd29afc",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- **AQE dynamically switches** from **shuffle join** to **broadcast join** when applicable, reducing shuffle overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Handling Data Skew with AQE**\n",
    "### **Objective:**\n",
    "- Optimize queries by **handling data skew dynamically**.\n",
    "\n",
    "### **Step 1: Enable Skew Join Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bc9f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9338da0f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Load Skewed Data (Loan Foreclosure Data)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eab14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_loans_skewed = spark.read.format(\"delta\").load(\"abfss://datalake@storage.dfs.core.windows.net/loan_foreclosure_skewed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1284d7e4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Execute Query and Monitor Execution Plan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef06c078",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col\n",
    "result = df_loans_skewed.groupBy(\"loan_status\").count()\n",
    "result.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5047f9ae",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "**Expected Outcome:**\n",
    "- AQE **automatically splits skewed partitions**, ensuring **better parallelism and query efficiency**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Monitoring AQE Execution Plans**\n",
    "### **Objective:**\n",
    "- Understand **how AQE modifies query execution plans**.\n",
    "\n",
    "### **Step 1: Enable AQE Execution Plan Logging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33867618",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.logLevel\", \"INFO\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f8bddd3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Execute a Complex Query with AQE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acf9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_complex = df_loans_skewed.groupBy(\"loan_purpose\").agg({\"loan_amount\": \"avg\"})\n",
    "df_complex.explain(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c71ae7b",
   "metadata": {},
   "source": [
    "### **Step 3: Compare Execution Plan Changes**\n",
    "- Observe **shuffle partition coalescing**.\n",
    "- Identify **adaptive join strategy selection**.\n",
    "- Check for **skew join optimizations**.\n",
    "\n",
    "**Expected Outcome:**\n",
    "- The execution plan **shows AQE optimizations**, proving its effectiveness in runtime tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "By completing these **hands-on labs**, you have learned how to:\n",
    "- **Enable and configure AQE** in **Databricks**.\n",
    "- **Optimize shuffle partitions dynamically** to improve query performance.\n",
    "- **Leverage adaptive join strategies** to reduce shuffle overhead.\n",
    "- **Handle data skew automatically** to prevent slow-running queries.\n",
    "- **Monitor and analyze AQE execution plans** for performance tuning.\n",
    "\n",
    "These labs provide **real-world experience** in leveraging **Adaptive Query Execution (AQE) for Spark query optimization**, making **data processing faster, more efficient, and cost-effective**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
