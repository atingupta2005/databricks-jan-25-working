{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Managing Shuffles and Broadcast Joins - Concepts**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Apache Spark is designed to process large-scale data efficiently.\n",
    "However, inefficient shuffling and poor join strategies can lead to\n",
    "performance bottlenecks. **Managing shuffles and broadcast joins\n",
    "effectively** is crucial for optimizing **query execution speed,\n",
    "reducing memory usage, and minimizing network overhead**.\n",
    "\n",
    "This document provides a **detailed guide** on managing **shuffles and\n",
    "broadcast joins** in **Databricks and cloud-based Spark environments**.\n",
    "It covers: - **Understanding shuffling in Spark** and its impact on\n",
    "performance - **Techniques to optimize shuffling using partitioning and\n",
    "caching** - **Understanding broadcast joins and when to use them** -\n",
    "**Best practices for managing large-scale joins efficiently**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **1. Understanding Shuffling in Spark**\n",
    "\n",
    "### **1.1 What is Shuffling?**\n",
    "\n",
    "Shuffling in Spark occurs when data is **redistributed across nodes**\n",
    "due to operations such as **groupBy, join, and repartition**. It\n",
    "involves: - **Data movement across the cluster**. - **Read/write\n",
    "operations to disk and network**. - **Increased memory and execution\n",
    "time**.\n",
    "\n",
    "### **1.2 Common Causes of Shuffling**\n",
    "\n",
    "| Cause                         | Description                                                |\n",
    "|-------------------------|------------------------------------------------|\n",
    "| **groupBy / reduceBy**        | Redistributes data based on a key to perform aggregations. |\n",
    "| **Joins on large datasets**   | Requires data movement if keys are not co-located.         |\n",
    "| **Repartitioning / coalesce** | Adjusts partition numbers, triggering data movement.       |\n",
    "| **Distinct operations**       | Requires deduplication across partitions.                  |\n",
    "\n",
    "### **1.3 Problems Caused by Excessive Shuffling**\n",
    "\n",
    "-   **Increased query execution time** due to high I/O operations.\n",
    "-   **Memory spills leading to OutOfMemory (OOM) errors**.\n",
    "-   **Network congestion slowing down distributed processing**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **2. Optimizing Shuffling in Spark**\n",
    "\n",
    "### **2.1 Using Proper Partitioning Strategies**\n",
    "\n",
    "-   **Hash Partitioning**: Distributes data based on hash functions,\n",
    "    useful for joins and aggregations.\n",
    "-   **Range Partitioning**: Splits data based on a sorting order,\n",
    "    beneficial for ordered operations.\n",
    "-   **Bucket Partitioning**: Pre-partitions tables to reduce shuffle\n",
    "    during joins.\n",
    "\n",
    "#### **Example: Hash Partitioning for Efficient Joins**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"200\")\n",
    "df = df.repartition(\"customer_id\")\n",
    "```\n",
    "\n",
    "### **2.2 Using Coalesce vs.Â Repartition**\n",
    "\n",
    "-   **`repartition(n)`** increases partitions but triggers **full\n",
    "    shuffle**.\n",
    "-   **`coalesce(n)`** reduces partitions without triggering full\n",
    "    shuffle, better for reducing data movement.\n",
    "\n",
    "#### **Example: Using Coalesce to Optimize Data Movement**\n",
    "\n",
    "``` python\n",
    "df = df.coalesce(10)  # Reduces shuffle cost while merging partitions\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **3. Understanding Broadcast Joins**\n",
    "\n",
    "### **3.1 What is a Broadcast Join?**\n",
    "\n",
    "A **broadcast join** occurs when **one dataset is small enough** to be\n",
    "sent to all nodes in the cluster, avoiding shuffling the large dataset.\n",
    "\n",
    "### **3.2 When to Use Broadcast Joins**\n",
    "\n",
    "| Scenario                             | Benefit                                                     |\n",
    "|--------------------------------------|----------------------------------|\n",
    "| Small lookup tables                  | Avoids shuffling large datasets                             |\n",
    "| Star-schema joins                    | Improves performance by sending small dimensions to workers |\n",
    "| One dataset is significantly smaller | Minimizes network movement                                  |\n",
    "\n",
    "### **3.3 Enabling Broadcast Joins in Spark**\n",
    "\n",
    "By default, Spark **automatically** chooses broadcast joins if a dataset\n",
    "is **smaller than 10MB**. - **Manually enabling broadcast join** for\n",
    "larger datasets:\n",
    "\n",
    "``` python\n",
    "from pyspark.sql.functions import broadcast\n",
    "df_joined = df_large.join(broadcast(df_small), \"customer_id\")\n",
    "```\n",
    "\n",
    "-   **Increasing the broadcast threshold**:\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"50MB\")\n",
    "```\n",
    "\n",
    "**Advantages:** - **Eliminates shuffle for the smaller dataset**. -\n",
    "**Improves query execution speed significantly**.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **4. Best Practices for Managing Shuffles and Joins**\n",
    "\n",
    "### **4.1 Best Practices for Reducing Shuffles**\n",
    "\n",
    "-   **Minimize the number of shuffle operations** by caching frequently\n",
    "    used datasets.\n",
    "-   **Use partitioning techniques** to colocate data in the same\n",
    "    partitions.\n",
    "-   **Avoid unnecessary repartitioning** unless required for performance\n",
    "    tuning.\n",
    "-   **Use proper aggregation methods (reduceByKey instead of\n",
    "    groupByKey).**\n",
    "\n",
    "### **4.2 Best Practices for Broadcast Joins**\n",
    "\n",
    "-   **Broadcast only small datasets** to avoid memory pressure.\n",
    "-   **Monitor and tune the autoBroadcastJoinThreshold** based on cluster\n",
    "    capacity.\n",
    "-   **Use `EXPLAIN` to check if Spark is performing a broadcast join**.\n",
    "\n",
    "#### **Example: Checking Query Plan for Broadcast Joins**\n",
    "\n",
    "``` python\n",
    "df_joined.explain(True)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "Effectively managing **shuffles and broadcast joins** can **drastically\n",
    "improve Spark job performance** by minimizing data movement and\n",
    "optimizing query execution.\n",
    "\n",
    "Key takeaways: - **Shuffling increases computation cost** and should be\n",
    "minimized using **partitioning strategies**. - **Broadcast joins\n",
    "eliminate unnecessary shuffling**, making them ideal for **small lookup\n",
    "tables**. - **Fine-tuning partition sizes, using coalesce, and\n",
    "leveraging caching** can further **optimize performance**.\n",
    "\n",
    "By applying these best practices, organizations can **optimize data\n",
    "engineering workloads, reduce processing time, and lower infrastructure\n",
    "costs** in **Databricks and other Spark-based environments**."
   ],
   "id": "b94e2659-12c1-44f4-b471-e3853eb2ee3f"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
