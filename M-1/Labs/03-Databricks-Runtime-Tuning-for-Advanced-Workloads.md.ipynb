{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Databricks Runtime: Tuning for Advanced Workloads - Lab Guide**\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "This **comprehensive hands-on lab guide** covers advanced performance\n",
    "tuning techniques for **Databricks Runtime**. These labs will help you\n",
    "optimize **cluster configurations, query execution, storage management,\n",
    "and cost efficiency** while ensuring **scalability and high\n",
    "performance**.\n",
    "\n",
    "By completing these labs, you will learn: 1. **Configuring optimized\n",
    "clusters** for various workloads. 2. **Query tuning strategies**\n",
    "including **Adaptive Query Execution (AQE), caching, Z-Ordering, and\n",
    "bucketing**. 3. **Optimizing data storage and access** using **Delta\n",
    "Lake, Auto-Optimized Writes, and file compaction**. 4. **Efficient\n",
    "resource management** for **batch processing and streaming workloads**.\n",
    "5. **Implementing cost-saving techniques** using **Photon Engine, Spot\n",
    "Instances, and Auto-Termination**. 6. **Monitoring and debugging\n",
    "performance bottlenecks** in **Databricks Spark UI**.\n",
    "\n",
    "**Dataset Reference:** The sample datasets and workloads used in these\n",
    "labs are based on **real-world enterprise datasets** similar to those\n",
    "found in **previous sample notebooks** you provided.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Optimizing Cluster Configuration for High Performance**\n",
    "\n",
    "### **Step 1: Create an Optimized Cluster Using Databricks UI**\n",
    "\n",
    "1.  Navigate to **Compute → Create Cluster**.\n",
    "2.  Configure the cluster as follows:\n",
    "    -   **Databricks Runtime:** `11.3.x-photon`\n",
    "    -   **Cluster Mode:** Standard\n",
    "    -   **Worker Type:** `Standard_DS3_v2` (balanced performance)\n",
    "    -   **Auto-scaling:** Enabled (**min 3, max 10 workers**)\n",
    "    -   **Auto-termination:** 30 minutes idle time\n",
    "3.  Click **Create Cluster**.\n",
    "\n",
    "### **Step 2: Create Cluster Using API**\n",
    "\n",
    "``` python\n",
    "import requests\n",
    "TOKEN = \"<DATABRICKS_ACCESS_TOKEN>\"\n",
    "DATABRICKS_URL = \"https://<databricks-instance>.cloud.databricks.com/api/2.0/clusters/create\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "cluster_config = {\n",
    "  \"cluster_name\": \"Optimized Performance Cluster\",\n",
    "  \"spark_version\": \"11.3.x-photon\",\n",
    "  \"node_type_id\": \"Standard_DS3_v2\",\n",
    "  \"autoscale\": {\"min_workers\": 3, \"max_workers\": 10},\n",
    "  \"autotermination_minutes\": 30\n",
    "}\n",
    "\n",
    "response = requests.post(DATABRICKS_URL, headers=headers, json=cluster_config)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Query Optimization and Adaptive Query Execution (AQE)**\n",
    "\n",
    "### **Step 1: Enabling Adaptive Query Execution**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Optimizing Joins Using AQE**\n",
    "\n",
    "#### **Broadcast Joins (For Small Tables)**\n",
    "\n",
    "``` sql\n",
    "SELECT /*+ BROADCAST(small_table) */ * \n",
    "FROM large_table \n",
    "JOIN small_table ON large_table.id = small_table.id;\n",
    "```\n",
    "\n",
    "#### **Sort-Merge Joins (For Large Datasets)**\n",
    "\n",
    "``` sql\n",
    "SELECT * FROM sales s \n",
    "JOIN customers c \n",
    "ON s.customer_id = c.customer_id;\n",
    "```\n",
    "\n",
    "### **Step 3: Configuring Shuffle Partitions for Performance**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Storage Optimization with Delta Lake**\n",
    "\n",
    "### **Step 1: Enabling Delta Caching**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Optimizing Delta Tables for Faster Queries**\n",
    "\n",
    "``` sql\n",
    "OPTIMIZE transactions ZORDER BY (customer_id);\n",
    "```\n",
    "\n",
    "### **Step 3: Partitioning Large Tables for Better Performance**\n",
    "\n",
    "``` sql\n",
    "CREATE TABLE sales_data \n",
    "USING DELTA \n",
    "PARTITIONED BY (region)\n",
    "AS SELECT * FROM raw_sales_data;\n",
    "```\n",
    "\n",
    "### **Step 4: Using Bucketing to Improve Query Performance**\n",
    "\n",
    "``` sql\n",
    "CREATE TABLE customer_orders \n",
    "USING DELTA \n",
    "CLUSTERED BY (customer_id) INTO 50 BUCKETS;\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Workload Execution Optimization**\n",
    "\n",
    "### **Step 1: Enabling Auto-Optimized Writes**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Enabling Auto-Compaction**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 3: Optimizing Streaming Workloads**\n",
    "\n",
    "``` python\n",
    "df.writeStream.trigger(processingTime=\"1 minute\").start()\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Cost Optimization Strategies**\n",
    "\n",
    "### **Step 1: Using Photon Engine for High-Performance Queries**\n",
    "\n",
    "``` sql\n",
    "SELECT customer_id, SUM(amount) \n",
    "FROM transactions \n",
    "GROUP BY customer_id\n",
    "ORDER BY SUM(amount) DESC;\n",
    "```\n",
    "\n",
    "### **Step 2: Using Spot Instances to Reduce Costs**\n",
    "\n",
    "``` json\n",
    "\"aws_attributes\": {\n",
    "  \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 3: Scheduling Auto-Termination for Idle Clusters**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.databricks.cluster.autotermination.enabled\", True)\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 6: Monitoring & Debugging Performance Issues**\n",
    "\n",
    "### **Step 1: Using Databricks Spark UI for Performance Analysis**\n",
    "\n",
    "1.  Navigate to **Clusters → Spark UI**.\n",
    "2.  Identify **query execution bottlenecks** using **SQL Query Execution\n",
    "    Plans**.\n",
    "\n",
    "### **Step 2: Enabling Logging & Profiling for Spark Jobs**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.eventLog.enabled\", True)\n",
    "spark.conf.set(\"spark.history.fs.logDirectory\", \"dbfs:/logs\")\n",
    "```\n",
    "\n",
    "### **Step 3: Analyzing Cluster Utilization Metrics**\n",
    "\n",
    "1.  Navigate to **Clusters → Metrics**.\n",
    "2.  Identify underutilized resources and adjust configurations\n",
    "    accordingly.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This **comprehensive lab guide** provides real-world hands-on experience\n",
    "in: - **Optimizing cluster configurations** for performance and cost\n",
    "efficiency. - **Fine-tuning queries using AQE, partitioning, bucketing,\n",
    "and Z-Ordering.** - **Enhancing storage performance with Delta Lake\n",
    "optimizations.** - **Managing batch and streaming workloads for maximum\n",
    "efficiency.** - **Reducing costs with Photon, Spot Instances, and\n",
    "Auto-Termination.** - **Monitoring Spark jobs and identifying\n",
    "performance bottlenecks.**\n",
    "\n",
    "By mastering these **advanced Databricks tuning techniques**, you will\n",
    "ensure **high-performance, cost-efficient, and scalable** data\n",
    "processing pipelines for enterprise workloads."
   ],
   "id": "b16f521b-ee49-4c18-8359-9cbc8c0deeb9"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
