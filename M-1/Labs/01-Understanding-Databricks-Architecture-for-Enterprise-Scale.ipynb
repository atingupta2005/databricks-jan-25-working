{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "919a7ec9",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# Lab Guide: Understanding Databricks Architecture for Enterprise Scale (Azure)\n",
    "\n",
    "## **Objective**\n",
    "This hands-on lab will focus on **real-world, production-ready implementations** of Databricks on **Azure**. You will:\n",
    "- Configure an **Azure Databricks workspace**.\n",
    "- Set up **Azure Data Lake Storage Gen2 (ADLS)** and manage secure access.\n",
    "- Build **real-world ETL workflows** using **Delta Lake**.\n",
    "- Automate data pipelines with **Azure Databricks Jobs**.\n",
    "- Optimize performance using **adaptive query execution (AQE) and Photon Engine**.\n",
    "- Implement **real-world** use cases based on previously provided sample datasets.\n",
    "\n",
    "## **Prerequisites**\n",
    "- **Azure Subscription** with permissions to create resources.\n",
    "- **Azure Databricks workspace** and **Azure Storage Account** with ADLS Gen2.\n",
    "- **Service Principal** for secure authentication.\n",
    "- **Familiarity with Apache Spark, Python, and SQL.**\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Configuring Azure Databricks for Secure Enterprise Data Processing**\n",
    "\n",
    "### **Step 1: Create an Azure Databricks Workspace**\n",
    "1. Navigate to **Azure Portal** → **Create a Resource** → Search `Azure Databricks`.\n",
    "2. Click **Create** and configure:\n",
    "   - **Subscription**: Select an existing subscription.\n",
    "   - **Resource Group**: Create a new one or use an existing one.\n",
    "   - **Workspace Name**: `enterprise-databricks`\n",
    "   - **Region**: Select the closest data region.\n",
    "   - **Pricing Tier**: **Premium** (for RBAC, security, and governance).\n",
    "3. Click **Review + Create**, then deploy the workspace.\n",
    "\n",
    "### **Step 2: Deploy a High-Performance Databricks Cluster**\n",
    "1. Navigate to **Compute** → **Create Cluster**.\n",
    "2. Configure:\n",
    "   - **Cluster Mode**: Standard\n",
    "   - **Databricks Runtime**: **Photon-Enabled ML Runtime**\n",
    "   - **Worker Type**: `Standard_D8ds_v4`\n",
    "   - **Auto-scaling**: Min **3**, Max **12** workers\n",
    "   - **Enable Adaptive Query Execution (AQE)**\n",
    "3. Click **Create Cluster** and wait for it to initialize.\n",
    "\n",
    "### **Step 3: Attach a Notebook to the Cluster**\n",
    "1. Go to **Workspace** → **Create** → **Notebook**.\n",
    "2. Choose **Python** as the default language.\n",
    "3. Attach the notebook to your **Databricks cluster**.\n",
    "4. Run the following command:\n",
    "   ```python\n",
    "   from pyspark.sql import SparkSession\n",
    "   spark = SparkSession.builder.appName(\"Enterprise_Azure_Databricks\").getOrCreate()\n",
    "   print(spark.version)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Secure Data Access with Azure Data Lake (ADLS Gen2)**\n",
    "\n",
    "### **Step 1: Create an Azure Storage Account and Container**\n",
    "1. Open **Azure Portal** → **Storage Accounts** → **Create**.\n",
    "2. Configure:\n",
    "   - **Storage Account Name**: `databricks-adls-gen2`\n",
    "   - **Enable Hierarchical Namespace**: **Yes**\n",
    "   - **Enable Secure Transfer**: **Yes**\n",
    "3. Click **Create**.\n",
    "4. Navigate to **Containers** → **Create New** → Name: `enterprise-data`.\n",
    "\n",
    "### **Step 2: Authenticate Using a Service Principal**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b45286",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": \"<Application_Client_ID>\",\n",
    "           \"fs.azure.account.oauth2.client.secret\": \"<Client_Secret>\",\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<Tenant_ID>/oauth2/token\"}\n",
    "\n",
    "# Mount ADLS Storage\n",
    "dbutils.fs.mount(source=\"abfss://enterprise-data@databricks-adls-gen2.dfs.core.windows.net/\", \n",
    "                 mount_point=\"/mnt/adls_enterprise\", \n",
    "                 extra_configs=configs)\n",
    "\n",
    "print(dbutils.fs.ls(\"/mnt/adls_enterprise\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3396483c",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Replace placeholders with your actual values.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Real-World ETL Processing with Delta Lake**\n",
    "\n",
    "### **Step 1: Create and Optimize Delta Tables**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0ef2d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data from ADLS\n",
    "adls_path = \"/mnt/adls_enterprise/raw_transactions.csv\"\n",
    "df = spark.read.csv(adls_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert to Delta format\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/adls_enterprise/delta/transactions\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec4ee68e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Optimize Performance Using Z-Ordering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b79b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "OPTIMIZE delta.`/mnt/adls_enterprise/delta/transactions` ZORDER BY (transaction_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0fb050f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 4: Automating Enterprise Data Pipelines**\n",
    "\n",
    "### **Step 1: Create an Azure Databricks Job**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba4e919",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "job_config = {\n",
    "    \"name\": \"Daily_Transaction_Processing\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"load_delta_table\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/ETL/transaction_processing\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"<Cluster_ID>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "dbutils.notebook.run(\"/Workspace/ETL/transaction_processing\", 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1c6c1e",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "Replace `<Cluster_ID>` with your cluster ID.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Advanced Performance Optimization**\n",
    "\n",
    "### **Use Adaptive Query Execution (AQE)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4d7ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ef4cd2",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Leverage Photon Engine for Faster Query Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb795ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT customer_id, SUM(amount) AS total_spent\n",
    "FROM delta.`/mnt/adls_enterprise/delta/transactions`\n",
    "GROUP BY customer_id\n",
    "ORDER BY total_spent DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c22c7a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "This lab provided a **real-world, enterprise-grade approach** to deploying **Azure Databricks**, securely integrating with **Azure Data Lake**, implementing **ETL pipelines with Delta Lake**, automating workflows, and optimizing performance using **AQE and Photon Engine**.\n",
    "\n",
    "By following this guide, enterprises can build **highly scalable, secure, and production-ready** data architectures on **Azure Databricks**.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
