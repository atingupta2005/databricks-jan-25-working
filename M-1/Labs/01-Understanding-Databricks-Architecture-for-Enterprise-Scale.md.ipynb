{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab Guide: Understanding Databricks Architecture for Enterprise Scale (Azure)\n",
    "\n",
    "## **Objective**\n",
    "\n",
    "This hands-on lab will focus on **real-world, production-ready\n",
    "implementations** of Databricks on **Azure**. You will: - Configure an\n",
    "**Azure Databricks workspace**. - Set up **Azure Data Lake Storage Gen2\n",
    "(ADLS)** and manage secure access. - Build **real-world ETL workflows**\n",
    "using **Delta Lake**. - Automate data pipelines with **Azure Databricks\n",
    "Jobs**. - Optimize performance using **adaptive query execution (AQE)\n",
    "and Photon Engine**. - Implement **real-world** use cases based on\n",
    "previously provided sample datasets.\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "-   **Azure Subscription** with permissions to create resources.\n",
    "-   **Azure Databricks workspace** and **Azure Storage Account** with\n",
    "    ADLS Gen2.\n",
    "-   **Service Principal** for secure authentication.\n",
    "-   **Familiarity with Apache Spark, Python, and SQL.**\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 1: Configuring Azure Databricks for Secure Enterprise Data Processing**\n",
    "\n",
    "### **Step 1: Create an Azure Databricks Workspace**\n",
    "\n",
    "1.  Navigate to **Azure Portal** → **Create a Resource** → Search\n",
    "    `Azure Databricks`.\n",
    "2.  Click **Create** and configure:\n",
    "    -   **Subscription**: Select an existing subscription.\n",
    "    -   **Resource Group**: Create a new one or use an existing one.\n",
    "    -   **Workspace Name**: `enterprise-databricks`\n",
    "    -   **Region**: Select the closest data region.\n",
    "    -   **Pricing Tier**: **Premium** (for RBAC, security, and\n",
    "        governance).\n",
    "3.  Click **Review + Create**, then deploy the workspace.\n",
    "\n",
    "### **Step 2: Deploy a High-Performance Databricks Cluster**\n",
    "\n",
    "1.  Navigate to **Compute** → **Create Cluster**.\n",
    "2.  Configure:\n",
    "    -   **Cluster Mode**: Standard\n",
    "    -   **Databricks Runtime**: **Photon-Enabled ML Runtime**\n",
    "    -   **Worker Type**: `Standard_D8ds_v4`\n",
    "    -   **Auto-scaling**: Min **3**, Max **12** workers\n",
    "    -   **Enable Adaptive Query Execution (AQE)**\n",
    "3.  Click **Create Cluster** and wait for it to initialize.\n",
    "\n",
    "### **Step 3: Attach a Notebook to the Cluster**\n",
    "\n",
    "1.  Go to **Workspace** → **Create** → **Notebook**.\n",
    "\n",
    "2.  Choose **Python** as the default language.\n",
    "\n",
    "3.  Attach the notebook to your **Databricks cluster**.\n",
    "\n",
    "4.  Run the following command:\n",
    "\n",
    "    ``` python\n",
    "    from pyspark.sql import SparkSession\n",
    "    spark = SparkSession.builder.appName(\"Enterprise_Azure_Databricks\").getOrCreate()\n",
    "    print(spark.version)\n",
    "    ```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 2: Secure Data Access with Azure Data Lake (ADLS Gen2)**\n",
    "\n",
    "### **Step 1: Create an Azure Storage Account and Container**\n",
    "\n",
    "1.  Open **Azure Portal** → **Storage Accounts** → **Create**.\n",
    "2.  Configure:\n",
    "    -   **Storage Account Name**: `databricks-adls-gen2`\n",
    "    -   **Enable Hierarchical Namespace**: **Yes**\n",
    "    -   **Enable Secure Transfer**: **Yes**\n",
    "3.  Click **Create**.\n",
    "4.  Navigate to **Containers** → **Create New** → Name:\n",
    "    `enterprise-data`.\n",
    "\n",
    "### **Step 2: Authenticate Using a Service Principal**\n",
    "\n",
    "``` python\n",
    "configs = {\"fs.azure.account.auth.type\": \"OAuth\",\n",
    "           \"fs.azure.account.oauth.provider.type\": \"org.apache.hadoop.fs.azurebfs.oauth2.ClientCredsTokenProvider\",\n",
    "           \"fs.azure.account.oauth2.client.id\": \"<Application_Client_ID>\",\n",
    "           \"fs.azure.account.oauth2.client.secret\": \"<Client_Secret>\",\n",
    "           \"fs.azure.account.oauth2.client.endpoint\": \"https://login.microsoftonline.com/<Tenant_ID>/oauth2/token\"}\n",
    "\n",
    "# Mount ADLS Storage\n",
    "dbutils.fs.mount(source=\"abfss://enterprise-data@databricks-adls-gen2.dfs.core.windows.net/\", \n",
    "                 mount_point=\"/mnt/adls_enterprise\", \n",
    "                 extra_configs=configs)\n",
    "\n",
    "print(dbutils.fs.ls(\"/mnt/adls_enterprise\"))\n",
    "```\n",
    "\n",
    "Replace placeholders with your actual values.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 3: Real-World ETL Processing with Delta Lake**\n",
    "\n",
    "### **Step 1: Create and Optimize Delta Tables**\n",
    "\n",
    "``` python\n",
    "# Load sample data from ADLS\n",
    "adls_path = \"/mnt/adls_enterprise/raw_transactions.csv\"\n",
    "df = spark.read.csv(adls_path, header=True, inferSchema=True)\n",
    "\n",
    "# Convert to Delta format\n",
    "df.write.format(\"delta\").mode(\"overwrite\").save(\"/mnt/adls_enterprise/delta/transactions\")\n",
    "```\n",
    "\n",
    "### **Step 2: Optimize Performance Using Z-Ordering**\n",
    "\n",
    "``` sql\n",
    "OPTIMIZE delta.`/mnt/adls_enterprise/delta/transactions` ZORDER BY (transaction_id);\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 4: Automating Enterprise Data Pipelines**\n",
    "\n",
    "### **Step 1: Create an Azure Databricks Job**\n",
    "\n",
    "``` python\n",
    "job_config = {\n",
    "    \"name\": \"Daily_Transaction_Processing\",\n",
    "    \"tasks\": [\n",
    "        {\n",
    "            \"task_key\": \"load_delta_table\",\n",
    "            \"notebook_task\": {\n",
    "                \"notebook_path\": \"/Workspace/ETL/transaction_processing\"\n",
    "            },\n",
    "            \"existing_cluster_id\": \"<Cluster_ID>\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "dbutils.notebook.run(\"/Workspace/ETL/transaction_processing\", 0)\n",
    "```\n",
    "\n",
    "Replace `<Cluster_ID>` with your cluster ID.\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Lab 5: Advanced Performance Optimization**\n",
    "\n",
    "### **Use Adaptive Query Execution (AQE)**\n",
    "\n",
    "``` python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", True)\n",
    "spark.conf.set(\"spark.sql.adaptive.localShuffleReader.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Leverage Photon Engine for Faster Query Execution**\n",
    "\n",
    "``` sql\n",
    "SELECT customer_id, SUM(amount) AS total_spent\n",
    "FROM delta.`/mnt/adls_enterprise/delta/transactions`\n",
    "GROUP BY customer_id\n",
    "ORDER BY total_spent DESC;\n",
    "```\n",
    "\n",
    "------------------------------------------------------------------------\n",
    "\n",
    "## **Conclusion**\n",
    "\n",
    "This lab provided a **real-world, enterprise-grade approach** to\n",
    "deploying **Azure Databricks**, securely integrating with **Azure Data\n",
    "Lake**, implementing **ETL pipelines with Delta Lake**, automating\n",
    "workflows, and optimizing performance using **AQE and Photon Engine**.\n",
    "\n",
    "By following this guide, enterprises can build **highly scalable,\n",
    "secure, and production-ready** data architectures on **Azure\n",
    "Databricks**."
   ],
   "id": "43947a00-95c5-4619-a5a7-90a275b592c1"
  }
 ],
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {}
}
