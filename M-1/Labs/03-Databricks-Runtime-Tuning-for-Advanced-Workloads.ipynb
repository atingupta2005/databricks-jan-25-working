{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "43f06610",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "# **Databricks Runtime: Tuning for Advanced Workloads - Lab Guide**\n",
    "\n",
    "## **Introduction**\n",
    "This **comprehensive hands-on lab guide** covers advanced performance tuning techniques for **Databricks Runtime**. These labs will help you optimize **cluster configurations, query execution, storage management, and cost efficiency** while ensuring **scalability and high performance**.\n",
    "\n",
    "By completing these labs, you will learn:\n",
    "1. **Configuring optimized clusters** for various workloads.\n",
    "2. **Query tuning strategies** including **Adaptive Query Execution (AQE), caching, Z-Ordering, and bucketing**.\n",
    "3. **Optimizing data storage and access** using **Delta Lake, Auto-Optimized Writes, and file compaction**.\n",
    "4. **Efficient resource management** for **batch processing and streaming workloads**.\n",
    "5. **Implementing cost-saving techniques** using **Photon Engine, Spot Instances, and Auto-Termination**.\n",
    "6. **Monitoring and debugging performance bottlenecks** in **Databricks Spark UI**.\n",
    "\n",
    "**Dataset Reference:** The sample datasets and workloads used in these labs are based on **real-world enterprise datasets** similar to those found in **previous sample notebooks** you provided.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Optimizing Cluster Configuration for High Performance**\n",
    "\n",
    "### **Step 1: Create an Optimized Cluster Using Databricks UI**\n",
    "1. Navigate to **Compute → Create Cluster**.\n",
    "2. Configure the cluster as follows:\n",
    "   - **Databricks Runtime:** `11.3.x-photon`\n",
    "   - **Cluster Mode:** Standard\n",
    "   - **Worker Type:** `Standard_DS3_v2` (balanced performance)\n",
    "   - **Auto-scaling:** Enabled (**min 3, max 10 workers**)\n",
    "   - **Auto-termination:** 30 minutes idle time\n",
    "3. Click **Create Cluster**.\n",
    "\n",
    "### **Step 2: Create Cluster Using API**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bebf1e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "TOKEN = \"<DATABRICKS_ACCESS_TOKEN>\"\n",
    "DATABRICKS_URL = \"https://<databricks-instance>.cloud.databricks.com/api/2.0/clusters/create\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "cluster_config = {\n",
    "  \"cluster_name\": \"Optimized Performance Cluster\",\n",
    "  \"spark_version\": \"11.3.x-photon\",\n",
    "  \"node_type_id\": \"Standard_DS3_v2\",\n",
    "  \"autoscale\": {\"min_workers\": 3, \"max_workers\": 10},\n",
    "  \"autotermination_minutes\": 30\n",
    "}\n",
    "\n",
    "response = requests.post(DATABRICKS_URL, headers=headers, json=cluster_config)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cab8cd4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 2: Query Optimization and Adaptive Query Execution (AQE)**\n",
    "\n",
    "### **Step 1: Enabling Adaptive Query Execution**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6873d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136e3053",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Optimizing Joins Using AQE**\n",
    "#### **Broadcast Joins (For Small Tables)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eadf5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT /*+ BROADCAST(small_table) */ * \n",
    "FROM large_table \n",
    "JOIN small_table ON large_table.id = small_table.id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956f1449",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "#### **Sort-Merge Joins (For Large Datasets)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ababa42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT * FROM sales s \n",
    "JOIN customers c \n",
    "ON s.customer_id = c.customer_id;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cb203d3",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Configuring Shuffle Partitions for Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2de99b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cdef740",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 3: Storage Optimization with Delta Lake**\n",
    "\n",
    "### **Step 1: Enabling Delta Caching**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaad84e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add869c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Optimizing Delta Tables for Faster Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08da826c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "OPTIMIZE transactions ZORDER BY (customer_id);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97411aa5",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Partitioning Large Tables for Better Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a43123",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE sales_data \n",
    "USING DELTA \n",
    "PARTITIONED BY (region)\n",
    "AS SELECT * FROM raw_sales_data;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef63563a",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 4: Using Bucketing to Improve Query Performance**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5df971d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "CREATE TABLE customer_orders \n",
    "USING DELTA \n",
    "CLUSTERED BY (customer_id) INTO 50 BUCKETS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36d7328f",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 4: Workload Execution Optimization**\n",
    "\n",
    "### **Step 1: Enabling Auto-Optimized Writes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65788ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f18f3b20",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Enabling Auto-Compaction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a2d331",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4011fc8",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 3: Optimizing Streaming Workloads**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae0aeb6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.writeStream.trigger(processingTime=\"1 minute\").start()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf62bc2d",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 5: Cost Optimization Strategies**\n",
    "\n",
    "### **Step 1: Using Photon Engine for High-Performance Queries**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a775f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sql\n",
    "SELECT customer_id, SUM(amount) \n",
    "FROM transactions \n",
    "GROUP BY customer_id\n",
    "ORDER BY SUM(amount) DESC;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8da5ad",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "### **Step 2: Using Spot Instances to Reduce Costs**\n",
    "```json\n",
    "\"aws_attributes\": {\n",
    "  \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 3: Scheduling Auto-Termination for Idle Clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e663da4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.cluster.autotermination.enabled\", True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9d1a7ba",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "source": [
    "---\n",
    "\n",
    "## **Lab 6: Monitoring & Debugging Performance Issues**\n",
    "\n",
    "### **Step 1: Using Databricks Spark UI for Performance Analysis**\n",
    "1. Navigate to **Clusters → Spark UI**.\n",
    "2. Identify **query execution bottlenecks** using **SQL Query Execution Plans**.\n",
    "\n",
    "### **Step 2: Enabling Logging & Profiling for Spark Jobs**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e225bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.eventLog.enabled\", True)\n",
    "spark.conf.set(\"spark.history.fs.logDirectory\", \"dbfs:/logs\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b09210e",
   "metadata": {},
   "source": [
    "### **Step 3: Analyzing Cluster Utilization Metrics**\n",
    "1. Navigate to **Clusters → Metrics**.\n",
    "2. Identify underutilized resources and adjust configurations accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "This **comprehensive lab guide** provides real-world hands-on experience in:\n",
    "- **Optimizing cluster configurations** for performance and cost efficiency.\n",
    "- **Fine-tuning queries using AQE, partitioning, bucketing, and Z-Ordering.**\n",
    "- **Enhancing storage performance with Delta Lake optimizations.**\n",
    "- **Managing batch and streaming workloads for maximum efficiency.**\n",
    "- **Reducing costs with Photon, Spot Instances, and Auto-Termination.**\n",
    "- **Monitoring Spark jobs and identifying performance bottlenecks.**\n",
    "\n",
    "By mastering these **advanced Databricks tuning techniques**, you will ensure **high-performance, cost-efficient, and scalable** data processing pipelines for enterprise workloads.\n"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
