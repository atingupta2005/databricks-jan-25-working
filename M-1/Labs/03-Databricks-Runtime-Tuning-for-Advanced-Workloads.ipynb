{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c754af1",
   "metadata": {},
   "source": [
    "# **Databricks Runtime: Tuning for Advanced Workloads - Lab Guide**\n",
    "\n",
    "## **Introduction**\n",
    "This **comprehensive hands-on lab guide** covers advanced performance tuning techniques for **Databricks Runtime**. These labs will help you optimize **cluster configurations, query execution, storage management, and cost efficiency** while ensuring **scalability and high performance**.\n",
    "\n",
    "By completing these labs, you will learn:\n",
    "1. **Configuring optimized clusters** for various workloads.\n",
    "2. **Query tuning strategies** including **Adaptive Query Execution (AQE), caching, Z-Ordering, and bucketing**.\n",
    "3. **Optimizing data storage and access** using **Delta Lake, Auto-Optimized Writes, and file compaction**.\n",
    "4. **Efficient resource management** for **batch processing and streaming workloads**.\n",
    "5. **Implementing cost-saving techniques** using **Photon Engine, Spot Instances, and Auto-Termination**.\n",
    "6. **Monitoring and debugging performance bottlenecks** in **Databricks Spark UI**.\n",
    "\n",
    "**Dataset Reference:** The sample datasets and workloads used in these labs are based on **real-world enterprise datasets** similar to those found in **previous sample notebooks** you provided.\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 1: Optimizing Cluster Configuration for High Performance**\n",
    "\n",
    "### **Step 1: Create an Optimized Cluster Using Databricks UI**\n",
    "1. Navigate to **Compute → Create Cluster**.\n",
    "2. Configure the cluster as follows:\n",
    "   - **Databricks Runtime:** `11.3.x-photon`\n",
    "   - **Cluster Mode:** Standard\n",
    "   - **Worker Type:** `Standard_DS3_v2` (balanced performance)\n",
    "   - **Auto-scaling:** Enabled (**min 3, max 10 workers**)\n",
    "   - **Auto-termination:** 30 minutes idle time\n",
    "3. Click **Create Cluster**.\n",
    "\n",
    "### **Step 2: Create Cluster Using API**\n",
    "```python\n",
    "import requests\n",
    "TOKEN = \"<DATABRICKS_ACCESS_TOKEN>\"\n",
    "DATABRICKS_URL = \"https://<databricks-instance>.cloud.databricks.com/api/2.0/clusters/create\"\n",
    "headers = {\"Authorization\": f\"Bearer {TOKEN}\", \"Content-Type\": \"application/json\"}\n",
    "\n",
    "cluster_config = {\n",
    "  \"cluster_name\": \"Optimized Performance Cluster\",\n",
    "  \"spark_version\": \"11.3.x-photon\",\n",
    "  \"node_type_id\": \"Standard_DS3_v2\",\n",
    "  \"autoscale\": {\"min_workers\": 3, \"max_workers\": 10},\n",
    "  \"autotermination_minutes\": 30\n",
    "}\n",
    "\n",
    "response = requests.post(DATABRICKS_URL, headers=headers, json=cluster_config)\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 2: Query Optimization and Adaptive Query Execution (AQE)**\n",
    "\n",
    "### **Step 1: Enabling Adaptive Query Execution**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Optimizing Joins Using AQE**\n",
    "#### **Broadcast Joins (For Small Tables)**\n",
    "```sql\n",
    "SELECT /*+ BROADCAST(small_table) */ * \n",
    "FROM large_table \n",
    "JOIN small_table ON large_table.id = small_table.id;\n",
    "```\n",
    "\n",
    "#### **Sort-Merge Joins (For Large Datasets)**\n",
    "```sql\n",
    "SELECT * FROM sales s \n",
    "JOIN customers c \n",
    "ON s.customer_id = c.customer_id;\n",
    "```\n",
    "\n",
    "### **Step 3: Configuring Shuffle Partitions for Performance**\n",
    "```python\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 200)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 3: Storage Optimization with Delta Lake**\n",
    "\n",
    "### **Step 1: Enabling Delta Caching**\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.io.cache.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Optimizing Delta Tables for Faster Queries**\n",
    "```sql\n",
    "OPTIMIZE transactions ZORDER BY (customer_id);\n",
    "```\n",
    "\n",
    "### **Step 3: Partitioning Large Tables for Better Performance**\n",
    "```sql\n",
    "CREATE TABLE sales_data \n",
    "USING DELTA \n",
    "PARTITIONED BY (region)\n",
    "AS SELECT * FROM raw_sales_data;\n",
    "```\n",
    "\n",
    "### **Step 4: Using Bucketing to Improve Query Performance**\n",
    "```sql\n",
    "CREATE TABLE customer_orders \n",
    "USING DELTA \n",
    "CLUSTERED BY (customer_id) INTO 50 BUCKETS;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 4: Workload Execution Optimization**\n",
    "\n",
    "### **Step 1: Enabling Auto-Optimized Writes**\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.delta.optimizeWrite.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 2: Enabling Auto-Compaction**\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.delta.autoCompact.enabled\", True)\n",
    "```\n",
    "\n",
    "### **Step 3: Optimizing Streaming Workloads**\n",
    "```python\n",
    "df.writeStream.trigger(processingTime=\"1 minute\").start()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 5: Cost Optimization Strategies**\n",
    "\n",
    "### **Step 1: Using Photon Engine for High-Performance Queries**\n",
    "```sql\n",
    "SELECT customer_id, SUM(amount) \n",
    "FROM transactions \n",
    "GROUP BY customer_id\n",
    "ORDER BY SUM(amount) DESC;\n",
    "```\n",
    "\n",
    "### **Step 2: Using Spot Instances to Reduce Costs**\n",
    "```json\n",
    "\"aws_attributes\": {\n",
    "  \"availability\": \"SPOT_WITH_FALLBACK\"\n",
    "}\n",
    "```\n",
    "\n",
    "### **Step 3: Scheduling Auto-Termination for Idle Clusters**\n",
    "```python\n",
    "spark.conf.set(\"spark.databricks.cluster.autotermination.enabled\", True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lab 6: Monitoring & Debugging Performance Issues**\n",
    "\n",
    "### **Step 1: Using Databricks Spark UI for Performance Analysis**\n",
    "1. Navigate to **Clusters → Spark UI**.\n",
    "2. Identify **query execution bottlenecks** using **SQL Query Execution Plans**.\n",
    "\n",
    "### **Step 2: Enabling Logging & Profiling for Spark Jobs**\n",
    "```python\n",
    "spark.conf.set(\"spark.eventLog.enabled\", True)\n",
    "spark.conf.set(\"spark.history.fs.logDirectory\", \"dbfs:/logs\")\n",
    "```\n",
    "\n",
    "### **Step 3: Analyzing Cluster Utilization Metrics**\n",
    "1. Navigate to **Clusters → Metrics**.\n",
    "2. Identify underutilized resources and adjust configurations accordingly.\n",
    "\n",
    "---\n",
    "\n",
    "## **Conclusion**\n",
    "This **comprehensive lab guide** provides real-world hands-on experience in:\n",
    "- **Optimizing cluster configurations** for performance and cost efficiency.\n",
    "- **Fine-tuning queries using AQE, partitioning, bucketing, and Z-Ordering.**\n",
    "- **Enhancing storage performance with Delta Lake optimizations.**\n",
    "- **Managing batch and streaming workloads for maximum efficiency.**\n",
    "- **Reducing costs with Photon, Spot Instances, and Auto-Termination.**\n",
    "- **Monitoring Spark jobs and identifying performance bottlenecks.**\n",
    "\n",
    "By mastering these **advanced Databricks tuning techniques**, you will ensure **high-performance, cost-efficient, and scalable** data processing pipelines for enterprise workloads.\n",
    "\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}